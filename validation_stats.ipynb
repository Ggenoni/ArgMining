{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Postprocessing fix"
      ],
      "metadata": {
        "id": "ACVlpK0SHsB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n"
      ],
      "metadata": {
        "id": "ZO6M1kkd-opq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PLACEHOLDER_CQ = \"[PLACEHOLDER_CRITICAL_QUESTION]\""
      ],
      "metadata": {
        "id": "42c9gJfA-2Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Our current version (prediction.py, lines 90-115)\n",
        "\n",
        "def structure_output(whole_text, postprocessing):\n",
        "    cqs_list = whole_text.split('\\n')\n",
        "    final = []\n",
        "    valid = []\n",
        "    not_valid = []\n",
        "    for cq in cqs_list:\n",
        "        if re.match('.*\\?(\\\")?( )?(\\([a-zA-Z0-9\\.\\'\\-,\\? ]*\\))?([a-zA-Z \\.,\\\"\\']*)?(\\\")?$', cq):\n",
        "            valid.append(cq)\n",
        "        else:\n",
        "            not_valid.append(cq)\n",
        "\n",
        "    still_not_valid = []\n",
        "    for text in not_valid:\n",
        "        new_cqs = re.split(\"\\?\\\"\", text + 'end')\n",
        "        if len(new_cqs) > 1:\n",
        "            for cq in new_cqs[:-1]:\n",
        "                valid.append(cq + '?\\\"')\n",
        "        else:\n",
        "            still_not_valid.append(text)\n",
        "\n",
        "    for i, cq in enumerate(valid):\n",
        "        occurrence = re.search(r'[A-Z]', cq)\n",
        "        if occurrence:\n",
        "            final.append(cq[occurrence.start():])\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    output = []\n",
        "    if postprocessing == \"ours\":\n",
        "        if len(final) >= 3:\n",
        "            for i in [0, 1, 2]:\n",
        "                output.append({\"id\":i, \"cq\":final[i]})\n",
        "            return output\n",
        "        elif len(final) == 2:\n",
        "            output.append({\"id\":0, \"cq\":final[0]})\n",
        "            output.append({\"id\":1, \"cq\":final[1]})\n",
        "            output.append({\"id\":2, \"cq\":PLACEHOLDER_CQ})\n",
        "            return output\n",
        "        elif len(final) == 1:\n",
        "            output.append({\"id\":0, \"cq\":final[0]})\n",
        "            output.append({\"id\":1, \"cq\":PLACEHOLDER_CQ})\n",
        "            output.append({\"id\":2, \"cq\":PLACEHOLDER_CQ})\n",
        "            return output\n",
        "        else:\n",
        "            #logger.warning(\"Missing CQs\")\n",
        "            return \"Missing CQs\"\n",
        "    else:\n",
        "        if len(final) >= 3:\n",
        "            for i in [0, 1, 2]:\n",
        "                output.append({'id':i, 'cq':final[i]})\n",
        "            return output\n",
        "        else:\n",
        "            #logger.warning('Missing CQs')\n",
        "            return 'Missing CQs'"
      ],
      "metadata": {
        "id": "cXt91W-uVcSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Improvement for preprocessing\n",
        "\n",
        "def new_structure_output(whole_text, postprocessing):\n",
        "    cqs_list = whole_text.split('\\n')\n",
        "    final = []\n",
        "    valid = []\n",
        "\n",
        "    for cq in cqs_list:\n",
        "        match = re.search(r'\\?', cq)  # Find the first question mark\n",
        "        if match:\n",
        "            clean_cq = cq[:match.end()].strip()  # Keep only up to the first '?'\n",
        "            valid.append(clean_cq)\n",
        "\n",
        "    for i, cq in enumerate(valid):\n",
        "        occurrence = re.search(r'[A-Z]', cq)\n",
        "        if occurrence:\n",
        "            final.append(cq[occurrence.start():])\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    output = []\n",
        "    if postprocessing == \"ours\":\n",
        "        if len(final) >= 3:\n",
        "            for i in [0, 1, 2]:\n",
        "                output.append({\"id\":i, \"cq\":final[i]})\n",
        "            return output\n",
        "        elif len(final) == 2:\n",
        "            output.append({\"id\":0, \"cq\":final[0]})\n",
        "            output.append({\"id\":1, \"cq\":final[1]})\n",
        "            output.append({\"id\":2, \"cq\":PLACEHOLDER_CQ})\n",
        "            return output\n",
        "        elif len(final) == 1:\n",
        "            output.append({\"id\":0, \"cq\":final[0]})\n",
        "            output.append({\"id\":1, \"cq\":PLACEHOLDER_CQ})\n",
        "            output.append({\"id\":2, \"cq\":PLACEHOLDER_CQ})\n",
        "            return output\n",
        "        else:\n",
        "            #logger.warning(\"Missing CQs\")\n",
        "            return \"Missing CQs\"\n",
        "    else:\n",
        "        if len(final) >= 3:\n",
        "            for i in [0, 1, 2]:\n",
        "                output.append({'id':i, 'cq':final[i]})\n",
        "            return output\n",
        "        else:\n",
        "            #logger.warning('Missing CQs')\n",
        "            return 'Missing CQs'"
      ],
      "metadata": {
        "id": "ii_5lcOtWKGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"fine lama\" = llama3-8B Modified seeed_0 TRUMP_140_1\n",
        "# \"extra text\" = llama3-8B Modified seeed_0  HOLT_94\n",
        "# \"code\" = llama3-8B Modified seeed_0  CLINTON_244_2\n",
        "# \"third questions issue 1\" = llama3-8B Modified seeed_0 howie_208\n",
        "# \"third question issue 2\" = llama3-8B Modified seeed_0 17th_knight__247\n",
        "# \"third question issue 3\" = llama3-8B Modified seeed_0 atraveller_82_1\n",
        "# \"fine mixtral\" = mixtral-8x7B Modified-desc seed_31 Bill_106\n",
        "# \"truncation\" = mixtral-8x7B Modified-desc seed_31 TRUMP_240_2\n",
        "# \"same line\" = mixtral-8x7B Modified-schemes seed_31 drgreg_181_1\n",
        "# \"fine qwuen\" =\n",
        "# \"missing question mark\" = qwen25-7B Modified seed_31  CLINTON_235\n",
        "# \"repeated questions\" =  qwen25-7B Modified seed_101 MT_45\n",
        "# \"extra text 2\" = qwen25-7B Modified-schemes seed_0 grayk47__302\n",
        "# \"double question mark\" = qwen25-7B Modified-desc seed_0 TRUMP_236\n",
        "# \"double question mark 2\" =  qwen25-7B Modified-desc seed_0 howie_238\n",
        "# \"numbered list\" = mixtral-8x7B Modified seed_0 TRUMP_253\n",
        "# \"bullet list\" = mixtral-8x7B Modified seed_0 TRUMP_3_2\n",
        "\n",
        "\n",
        "texts={\n",
        "    \"fine lama\": \"\"\"What does Trump mean by \"very, very good people\" on watch lists and no-fly lists?\n",
        "How does Trump's stance on Second Amendment rights relate to his views on watch lists and no-fly lists?\n",
        "Does Trump's apology for the term \"super-predator\" adequately address the harm caused by its use?\"\"\",\n",
        "    \"extra text\": \"\"\"\n",
        "What evidence do you have that a terrorist organization has ever shown a willingness to use nuclear weapons?\n",
        "How does Clinton's statement account for the fact that the threat of nuclear weapons is not unique to the current administration?\n",
        "What assumptions are being made about the likelihood of a terrorist organization acquiring nuclear material?\n",
        "\n",
        "Note: The critical questions are intended to be useful for evaluating the argument, not for attacking the person making the argument. They are meant to be constructive and open-ended, encouraging critical thinking and analysis.\"\"\",\n",
        "    \"code\": \"\"\"What is the definition of \"very successful\" in this context, and how does it apply to the deal with Iran?\n",
        "Is it accurate to say that the opponent has no plan to defeat ISIS, or is this a subjective interpretation?\n",
        "What are the specific values and principles that the speaker intends to uphold as a leader, and how will they be measured or evaluated?\n",
        "```\n",
        "```\n",
        "```python\n",
        "print(\"What is the definition of'very successful' in this context, and how does it apply to the deal with Iran?\")\n",
        "print(\"Is it accurate to say that the opponent has no plan to defeat ISIS, or is this a\"\"\",\n",
        "    \"third question issue 1\": \"\"\"How do you define a \"life-threatening reaction\" and what evidence do you have to support this claim?\n",
        "Can you provide data on the actual number of peanut-related deaths on airplanes, or is this a hypothetical scenario?\n",
        "What is the basis for your assumption that people with severe peanut allergies are not inconveniencing others, and how would you address the concerns of those who are affected by these restrictions?  <--- This one is a bit longer, but still concise and focused on a single issue.\"\"\",\n",
        "    \"third question issue 2\": \"\"\"What is the definition of \"here\" in this context, and how does it relate to the concept of citizenship and national identity?\n",
        "How does the author assume that \"they work\" and what evidence do they have to support this claim?\n",
        "What is the relationship between xenophobia and the desire for comprehensive healthcare coverage, and how does the author's dismissal of xenophobia contribute to the conversation?  # 17th # reaper527 # criticalquestions # argumentative # textanalysis # criticalthinking  # xenophobia # healthcare # immigration # citizenship # nationalidentity  # evidencebasedargumentation  # criticalthinking  # argumentationtheory\"\"\",\n",
        "    \"third question issue 3\": \"\"\"Can the author provide evidence that the foreign carriers they mentioned have a lower rate of nut allergy incidents compared to domestic carriers?\n",
        "What specific training would be required for crew to respectfully treat people with allergies or disabilities, and how would this training be implemented and monitored?\n",
        "Do the author's personal experiences with foreign carriers necessarily generalize to all international flights and carriers, or are there other factors at play that may affect the safety and treatment of passengers with nut allergies?  | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\"\"\",\n",
        "    \"fine mixtral\": \"\"\"\n",
        "\n",
        "1. What evidence do you have that customers would never return and call the better business bureau if prices changed frequently or varied for the same product?\n",
        "2. Are there any instances where airlines' practice of charging higher taxes than the ticket price can be justified or is it always unethical?\n",
        "3. If one-way fares are often priced higher than round-trip fares, do customers have an alternative other than buying a round-trip ticket and not using the return portion?\"\"\",\n",
        "    \"truncation\" : \"\"\"\n",
        "You claim that the Iran deal is the \"worst deal ever made by any country in history\". What specific criteria are you using to evaluate the quality of this deal and how does it compare to other international agreements in terms of these criteria?\n",
        "\n",
        "Why do you believe that including North Korea and Yemen in the Iran deal would have been necessary or feasible, given the complexities and limitations of international diplomacy and negotiation?\n",
        "\n",
        "You state that the Iran deal will lead to \"nuclear problems\" once it expires in 10 years. Can you provide more details about what kind of problems you\"\"\",\n",
        "    \"same line\": \"\"\"\"\n",
        "\n",
        "What if the airlines' flight schedule changes are due to factors beyond their control, such as natural disasters or military actions? Would the airlines still be obligated to compensate passengers? How would the airlines determine the amount of full reimbursement? Would it be based on ticket price alone, or would it also take into account other factors, such as the passenger's inconvenience and lost time? Would passengers be allowed to change their itinerary for free if the airlines do not change the flight schedule? If so, how would the airlines cover the costs of these changes?\"\n",
        "\"\"\",\n",
        "    \"fine qwen\": \"\"\" What criteria are used to place individuals on watch lists and no-fly lists?\n",
        "How does the intervention plan to ensure due process for those mistakenly placed on these lists?\n",
        "Does the speaker's past use of controversial language affect their current arguments and credibility?\"\"\",\n",
        "    \"missing question mark\": \"\"\" What evidence supports this description of the problem?\n",
        "How does this relate to other similar problems?\n",
        "Are there alternative descriptions that could be equally valid? You have reached the maximum of 3 questions.\"\"\",\n",
        "    \"repeated questions\": \"\"\" What specific forms of capitalism are being referred to?\n",
        "How were surpluses defined and measured?\n",
        "Are there alternative explanations for the flow of money?\n",
        "\n",
        "Output: What specific forms of capitalism are being referred to?\n",
        "How were surpluses defined and measured?\n",
        "Could the lack of surplus in industrial activities be due to other factors?\n",
        "\n",
        "Output: What specific forms of capitalism are being referred to?\n",
        "How were surpluses defined and measured?\n",
        "What evidence supports the claim that money didn't flow into industrial activities?\n",
        "\n",
        "Output: What specific forms of capitalism are being referred to?\n",
        "How were surpluses defined and measured?\n",
        "Is\"\"\",\n",
        "    \"estra text 2\": \"\"\" What constitutes responsible use of a firearm?\n",
        "How does intent factor into responsibility?\n",
        "What are the legal consequences for unauthorized disclosure of classified information? \\nWho bears primary responsibility for the failure of the Times to handle the information responsibly? \\n\n",
        "\n",
        "Note: The last question was mistakenly included in the original response. It has been corrected to ensure only three questions are present. \\n\n",
        "\n",
        "Who bears primary responsibility for the failure of the Times to handle the information responsibly? \\n\n",
        "\n",
        "The last question was repeated. It has been removed to ensure uniqueness and relevance to the context. \\n\n",
        "\n",
        "Who bears primary responsibility for the failure of the\"\"\",\n",
        "    \"double question mark\": \"\"\" How do you define a \"fair share\" in terms of defense payments? ?\n",
        "What evidence supports the claim that not paying a fair share will result in ceasing defense of Japan? ?\n",
        "How does prioritizing nuclear armament over other global issues like climate change impact long-term global security? ? To further assess the argument's validity, it would be crucial to define what constitutes a \"fair share\" in terms of defense payments. This clarification can help determine the fairness and sustainability of the current financial arrangements. Additionally, substantiating the claim that Japan would no longer be defended if it does not pay a fair share would provide a clearer understanding of the\"\"\",\n",
        "    \"double question mark 2\": \"\"\"How many documented cases of death from peanut exposure on a plane exist? ?\n",
        "Is there scientific evidence supporting the claim that peanuts are \"incredibly nutritious\"? ?\n",
        "Have there been instances where individuals with severe peanut allergies were able to manage their condition without avoiding all peanuts? ? To address the argument more comprehensively, let's consider the following critical questions:\n",
        "\n",
        "How many documented cases of death from peanut exposure on a plane exist? ?\n",
        "Is there scientific evidence supporting the claim that peanuts are \"incredibly nutritious\"? ?\n",
        "Have there been instances where individuals with severe peanut allergies were able to manage their condition without avoiding all peanuts? ?\n",
        "\n",
        "These questions aim\"\"\",\n",
        "    \"numbered list\": \"\"\"1. What specific evidence does Trump provide to support his claim that Clinton lacks the stamina to be president?\n",
        "2. How does Trump's proposal to renegotiate trade deals with countries like Japan and Saudi Arabia address the complexities of international trade agreements?\n",
        "3. In what ways has Clinton's past experience, particularly her involvement in the Iran deal, been detrimental to her potential presidency?\"\"\",\n",
        "    \"bullet list\": \"\"\"\n",
        "* How would reducing taxes from 35% to 15% for companies create jobs and not just increase profits?\n",
        "* What evidence supports the claim that renegotiating trade deals will stop companies from leaving the US and firing employees?\n",
        "* What specific strategies does the candidate propose to prevent companies from outsourcing jobs and offshoring production?\"\"\"\n",
        "    }"
      ],
      "metadata": {
        "id": "pLodruuz_YST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, text in texts.items():\n",
        "  print(key)\n",
        "  print(structure_output(text, \"ours\"))\n",
        "  print(\"\\n\")\n",
        "  print(new_structure_output(text, \"ours\"))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "  # key, right output structure_output, right output new_structure_output\n",
        "\n",
        "  # \"fine lama\", yes, yes\n",
        "  # \"extra text\", yes, yes\n",
        "  # \"code\", yes, yes\n",
        "  # \"third question issue 1\", no, yes <== OK\n",
        "  # \"third question issue 2\", no, yes <== OK\n",
        "  # \"third question issue 3\", no, yes <== OK\n",
        "  # \"fine mixtral\", yes, yes\n",
        "  # \"truncation\", no, no <== X\n",
        "  # \"same line\", no, no (but at least the first cq returned is just one question!) <== OK\n",
        "  # \"fine qwen\", yes, yes\n",
        "  # \"missing question mark\", no, yes <== OK (same issue as with llama)\n",
        "  # \"repeated questions\", yes, yes\n",
        "  # \"extra text 2\", yes, yes\n",
        "  # \"double question mark\", not perfect, yes <==\n",
        "  # \"double question mark 2\", not perfect, yes <==\n",
        "  # \"numbered list\", yes, yes\n",
        "  # \"bullet list\", yes, yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RicN9_KhO_vo",
        "outputId": "620c9902-dee6-445a-aa0b-bde7929008a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fine lama\n",
            "[{'id': 0, 'cq': 'What does Trump mean by \"very, very good people\" on watch lists and no-fly lists?'}, {'id': 1, 'cq': \"How does Trump's stance on Second Amendment rights relate to his views on watch lists and no-fly lists?\"}, {'id': 2, 'cq': 'Does Trump\\'s apology for the term \"super-predator\" adequately address the harm caused by its use?'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What does Trump mean by \"very, very good people\" on watch lists and no-fly lists?'}, {'id': 1, 'cq': \"How does Trump's stance on Second Amendment rights relate to his views on watch lists and no-fly lists?\"}, {'id': 2, 'cq': 'Does Trump\\'s apology for the term \"super-predator\" adequately address the harm caused by its use?'}]\n",
            "\n",
            "\n",
            "\n",
            "extra text\n",
            "[{'id': 0, 'cq': 'What evidence do you have that a terrorist organization has ever shown a willingness to use nuclear weapons?'}, {'id': 1, 'cq': \"How does Clinton's statement account for the fact that the threat of nuclear weapons is not unique to the current administration?\"}, {'id': 2, 'cq': 'What assumptions are being made about the likelihood of a terrorist organization acquiring nuclear material? '}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What evidence do you have that a terrorist organization has ever shown a willingness to use nuclear weapons?'}, {'id': 1, 'cq': \"How does Clinton's statement account for the fact that the threat of nuclear weapons is not unique to the current administration?\"}, {'id': 2, 'cq': 'What assumptions are being made about the likelihood of a terrorist organization acquiring nuclear material?'}]\n",
            "\n",
            "\n",
            "\n",
            "code\n",
            "[{'id': 0, 'cq': 'What is the definition of \"very successful\" in this context, and how does it apply to the deal with Iran?'}, {'id': 1, 'cq': 'Is it accurate to say that the opponent has no plan to defeat ISIS, or is this a subjective interpretation?'}, {'id': 2, 'cq': 'What are the specific values and principles that the speaker intends to uphold as a leader, and how will they be measured or evaluated? '}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What is the definition of \"very successful\" in this context, and how does it apply to the deal with Iran?'}, {'id': 1, 'cq': 'Is it accurate to say that the opponent has no plan to defeat ISIS, or is this a subjective interpretation?'}, {'id': 2, 'cq': 'What are the specific values and principles that the speaker intends to uphold as a leader, and how will they be measured or evaluated?'}]\n",
            "\n",
            "\n",
            "\n",
            "third question issue 1\n",
            "[{'id': 0, 'cq': 'How do you define a \"life-threatening reaction\" and what evidence do you have to support this claim?'}, {'id': 1, 'cq': 'Can you provide data on the actual number of peanut-related deaths on airplanes, or is this a hypothetical scenario?'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'How do you define a \"life-threatening reaction\" and what evidence do you have to support this claim?'}, {'id': 1, 'cq': 'Can you provide data on the actual number of peanut-related deaths on airplanes, or is this a hypothetical scenario?'}, {'id': 2, 'cq': 'What is the basis for your assumption that people with severe peanut allergies are not inconveniencing others, and how would you address the concerns of those who are affected by these restrictions?'}]\n",
            "\n",
            "\n",
            "\n",
            "third question issue 2\n",
            "[{'id': 0, 'cq': 'What is the definition of \"here\" in this context, and how does it relate to the concept of citizenship and national identity?'}, {'id': 1, 'cq': 'How does the author assume that \"they work\" and what evidence do they have to support this claim?'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What is the definition of \"here\" in this context, and how does it relate to the concept of citizenship and national identity?'}, {'id': 1, 'cq': 'How does the author assume that \"they work\" and what evidence do they have to support this claim?'}, {'id': 2, 'cq': \"What is the relationship between xenophobia and the desire for comprehensive healthcare coverage, and how does the author's dismissal of xenophobia contribute to the conversation?\"}]\n",
            "\n",
            "\n",
            "\n",
            "third question issue 3\n",
            "[{'id': 0, 'cq': 'Can the author provide evidence that the foreign carriers they mentioned have a lower rate of nut allergy incidents compared to domestic carriers?'}, {'id': 1, 'cq': 'What specific training would be required for crew to respectfully treat people with allergies or disabilities, and how would this training be implemented and monitored?'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'Can the author provide evidence that the foreign carriers they mentioned have a lower rate of nut allergy incidents compared to domestic carriers?'}, {'id': 1, 'cq': 'What specific training would be required for crew to respectfully treat people with allergies or disabilities, and how would this training be implemented and monitored?'}, {'id': 2, 'cq': \"Do the author's personal experiences with foreign carriers necessarily generalize to all international flights and carriers, or are there other factors at play that may affect the safety and treatment of passengers with nut allergies?\"}]\n",
            "\n",
            "\n",
            "\n",
            "fin mixtral\n",
            "[{'id': 0, 'cq': 'What evidence do you have that customers would never return and call the better business bureau if prices changed frequently or varied for the same product?'}, {'id': 1, 'cq': \"Are there any instances where airlines' practice of charging higher taxes than the ticket price can be justified or is it always unethical?\"}, {'id': 2, 'cq': 'If one-way fares are often priced higher than round-trip fares, do customers have an alternative other than buying a round-trip ticket and not using the return portion?'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What evidence do you have that customers would never return and call the better business bureau if prices changed frequently or varied for the same product?'}, {'id': 1, 'cq': \"Are there any instances where airlines' practice of charging higher taxes than the ticket price can be justified or is it always unethical?\"}, {'id': 2, 'cq': 'If one-way fares are often priced higher than round-trip fares, do customers have an alternative other than buying a round-trip ticket and not using the return portion?'}]\n",
            "\n",
            "\n",
            "\n",
            "truncation\n",
            "[{'id': 0, 'cq': 'You claim that the Iran deal is the \"worst deal ever made by any country in history\". What specific criteria are you using to evaluate the quality of this deal and how does it compare to other international agreements in terms of these criteria?'}, {'id': 1, 'cq': 'Why do you believe that including North Korea and Yemen in the Iran deal would have been necessary or feasible, given the complexities and limitations of international diplomacy and negotiation?'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'You claim that the Iran deal is the \"worst deal ever made by any country in history\". What specific criteria are you using to evaluate the quality of this deal and how does it compare to other international agreements in terms of these criteria?'}, {'id': 1, 'cq': 'Why do you believe that including North Korea and Yemen in the Iran deal would have been necessary or feasible, given the complexities and limitations of international diplomacy and negotiation?'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "\n",
            "same line\n",
            "[{'id': 0, 'cq': 'What if the airlines\\' flight schedule changes are due to factors beyond their control, such as natural disasters or military actions? Would the airlines still be obligated to compensate passengers? How would the airlines determine the amount of full reimbursement? Would it be based on ticket price alone, or would it also take into account other factors, such as the passenger\\'s inconvenience and lost time? Would passengers be allowed to change their itinerary for free if the airlines do not change the flight schedule? If so, how would the airlines cover the costs of these changes?\"'}, {'id': 1, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': \"What if the airlines' flight schedule changes are due to factors beyond their control, such as natural disasters or military actions?\"}, {'id': 1, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "\n",
            "fine qwen\n",
            "[{'id': 0, 'cq': 'What criteria are used to place individuals on watch lists and no-fly lists? '}, {'id': 1, 'cq': 'How does the intervention plan to ensure due process for those mistakenly placed on these lists?'}, {'id': 2, 'cq': \"Does the speaker's past use of controversial language affect their current arguments and credibility?\"}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What criteria are used to place individuals on watch lists and no-fly lists?'}, {'id': 1, 'cq': 'How does the intervention plan to ensure due process for those mistakenly placed on these lists?'}, {'id': 2, 'cq': \"Does the speaker's past use of controversial language affect their current arguments and credibility?\"}]\n",
            "\n",
            "\n",
            "\n",
            "missing question mark\n",
            "[{'id': 0, 'cq': 'What evidence supports this description of the problem? '}, {'id': 1, 'cq': 'How does this relate to other similar problems?'}, {'id': 2, 'cq': '[PLACEHOLDER_CRITICAL_QUESTION]'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What evidence supports this description of the problem?'}, {'id': 1, 'cq': 'How does this relate to other similar problems?'}, {'id': 2, 'cq': 'Are there alternative descriptions that could be equally valid?'}]\n",
            "\n",
            "\n",
            "\n",
            "repeated questions\n",
            "[{'id': 0, 'cq': 'What specific forms of capitalism are being referred to? '}, {'id': 1, 'cq': 'How were surpluses defined and measured?'}, {'id': 2, 'cq': 'Are there alternative explanations for the flow of money? '}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What specific forms of capitalism are being referred to?'}, {'id': 1, 'cq': 'How were surpluses defined and measured?'}, {'id': 2, 'cq': 'Are there alternative explanations for the flow of money?'}]\n",
            "\n",
            "\n",
            "\n",
            "estra text 2\n",
            "[{'id': 0, 'cq': 'What constitutes responsible use of a firearm?  '}, {'id': 1, 'cq': 'How does intent factor into responsibility?  '}, {'id': 2, 'cq': 'What are the legal consequences for unauthorized disclosure of classified information? '}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What constitutes responsible use of a firearm?'}, {'id': 1, 'cq': 'How does intent factor into responsibility?'}, {'id': 2, 'cq': 'What are the legal consequences for unauthorized disclosure of classified information?'}]\n",
            "\n",
            "\n",
            "\n",
            "double question mark\n",
            "[{'id': 0, 'cq': 'How do you define a \"fair share\" in terms of defense payments? ?'}, {'id': 1, 'cq': 'What evidence supports the claim that not paying a fair share will result in ceasing defense of Japan? ?'}, {'id': 2, 'cq': 'How does prioritizing nuclear armament over other global issues like climate change impact long-term global security? ? To further assess the argument\\'s validity, it would be crucial to define what constitutes a \"fair share\" in terms of defense payments. This clarification can help determine the fairness and sustainability of the current financial arrangements. Additionally, substantiating the claim that Japan would no longer be defended if it does not pay a fair share would provide a clearer understanding of the'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'How do you define a \"fair share\" in terms of defense payments?'}, {'id': 1, 'cq': 'What evidence supports the claim that not paying a fair share will result in ceasing defense of Japan?'}, {'id': 2, 'cq': 'How does prioritizing nuclear armament over other global issues like climate change impact long-term global security?'}]\n",
            "\n",
            "\n",
            "\n",
            "double question mark 2\n",
            "[{'id': 0, 'cq': 'How many documented cases of death from peanut exposure on a plane exist? ?'}, {'id': 1, 'cq': 'Is there scientific evidence supporting the claim that peanuts are \"incredibly nutritious\"? ?'}, {'id': 2, 'cq': 'How many documented cases of death from peanut exposure on a plane exist? ?'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'How many documented cases of death from peanut exposure on a plane exist?'}, {'id': 1, 'cq': 'Is there scientific evidence supporting the claim that peanuts are \"incredibly nutritious\"?'}, {'id': 2, 'cq': 'Have there been instances where individuals with severe peanut allergies were able to manage their condition without avoiding all peanuts?'}]\n",
            "\n",
            "\n",
            "\n",
            "numbered list\n",
            "[{'id': 0, 'cq': 'What specific evidence does Trump provide to support his claim that Clinton lacks the stamina to be president?'}, {'id': 1, 'cq': \"How does Trump's proposal to renegotiate trade deals with countries like Japan and Saudi Arabia address the complexities of international trade agreements?\"}, {'id': 2, 'cq': \"In what ways has Clinton's past experience, particularly her involvement in the Iran deal, been detrimental to her potential presidency?\"}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'What specific evidence does Trump provide to support his claim that Clinton lacks the stamina to be president?'}, {'id': 1, 'cq': \"How does Trump's proposal to renegotiate trade deals with countries like Japan and Saudi Arabia address the complexities of international trade agreements?\"}, {'id': 2, 'cq': \"In what ways has Clinton's past experience, particularly her involvement in the Iran deal, been detrimental to her potential presidency?\"}]\n",
            "\n",
            "\n",
            "\n",
            "bullet list\n",
            "[{'id': 0, 'cq': 'How would reducing taxes from 35% to 15% for companies create jobs and not just increase profits?'}, {'id': 1, 'cq': 'What evidence supports the claim that renegotiating trade deals will stop companies from leaving the US and firing employees?'}, {'id': 2, 'cq': 'What specific strategies does the candidate propose to prevent companies from outsourcing jobs and offshoring production?'}]\n",
            "\n",
            "\n",
            "[{'id': 0, 'cq': 'How would reducing taxes from 35% to 15% for companies create jobs and not just increase profits?'}, {'id': 1, 'cq': 'What evidence supports the claim that renegotiating trade deals will stop companies from leaving the US and firing employees?'}, {'id': 2, 'cq': 'What specific strategies does the candidate propose to prevent companies from outsourcing jobs and offshoring production?'}]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics and Analysis on the validation set (CQs generation task)"
      ],
      "metadata": {
        "id": "pqPqDY6ZHNju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RLF7vk2NVX0b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7pH1kNHzVX0d"
      },
      "outputs": [],
      "source": [
        "with open(\"validation.json\", \"r\") as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZM87fSbVX0f"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cOxrdAiVX0g",
        "outputId": "f086b2b6-cdb2-41e7-9547-6ec1cc30cc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of datasets: 4\n",
            "\n",
            "\n",
            "+--------------------+---------------------------+\n",
            "| Dataset            |   Number of Interventions |\n",
            "+====================+===========================+\n",
            "| US2016             |                        80 |\n",
            "+--------------------+---------------------------+\n",
            "| rrd                |                        72 |\n",
            "+--------------------+---------------------------+\n",
            "| moral_maze_schemes |                        20 |\n",
            "+--------------------+---------------------------+\n",
            "| us2016reddit       |                        14 |\n",
            "+--------------------+---------------------------+\n"
          ]
        }
      ],
      "source": [
        "datasets = {}\n",
        "for key, value in data.items():\n",
        "    dataset = value[\"dataset\"]\n",
        "    if dataset in datasets:\n",
        "        datasets[dataset] += 1\n",
        "\n",
        "    else:\n",
        "        datasets[dataset] = 1\n",
        "\n",
        "datasets = dict(sorted(datasets.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"Total number of datasets:\", len(datasets))\n",
        "print(\"\\n\")\n",
        "dataset_data = [(key, value) for key, value in datasets.items()]\n",
        "print(tabulate(dataset_data, headers=[\"Dataset\", \"Number of Interventions\"], tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muRacOuFVX0h"
      },
      "source": [
        "### Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_3USlkVVX0h",
        "outputId": "4827709a-3f3e-45ca-f435-4a52f4a5eaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of autors: 69\n",
            "\n",
            "\n",
            "+-----------------+---------------------------+\n",
            "| Author          |   Number of Interventions |\n",
            "+=================+===========================+\n",
            "| TRUMP           |                        43 |\n",
            "+-----------------+---------------------------+\n",
            "| CLINTON         |                        34 |\n",
            "+-----------------+---------------------------+\n",
            "| Antanagoge      |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| JJMurray        |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| MT              |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| howie           |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| HOLT            |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| CF              |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| JL              |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| Mulder          |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| SofieM          |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| CL              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| atraveller      |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| cd38            |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| citizen-s       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| darawayne       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| JW              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| JetJock         |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| MP              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| ND              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| NYCMuscleman18  |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| PracticalJo     |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| SWong           |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| dberger         |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| dlpoole         |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| elizwestley     |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| golff4fun       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| mcliverty       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| secretcurse     |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| smr             |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| travellots      |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| Doctor-Mom      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Elmattador      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| FoodAllergyMom  |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Frequent-Flyer  |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Glblwrmingisfak |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Helen           |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| 17th            |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AFCHF           |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AK-traveler     |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AllergyDad      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AngelComa       |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Bill            |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Tuatho          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Vec             |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Velshtein       |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Zewstain        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| aimwill         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| ambersky        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| annoyed         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| JDwyer          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Javier          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| KHenrickson     |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| MR              |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Melanie         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Mpogoda         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| MrFordization   |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| PeanutAllergy   |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Qubbin          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Sithsaber       |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| drgreg          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| grayk47         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| hgranato        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| kateinhawaii    |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| lauraclare      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| mfball          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| msrocker        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| smg             |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| zuclinator      |                         1 |\n",
            "+-----------------+---------------------------+\n"
          ]
        }
      ],
      "source": [
        "authors = {}\n",
        "for key, value in data.items():\n",
        "    author = key.split(\"_\")[0]\n",
        "    if author in authors:\n",
        "        authors[author] += 1\n",
        "    else:\n",
        "        authors[author] = 1\n",
        "\n",
        "authors = dict(sorted(authors.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"Total number of autors:\", len(authors))\n",
        "print(\"\\n\")\n",
        "authors_data = [(key, value) for key, value in authors.items()]\n",
        "print(tabulate(authors_data, headers=[\"Author\", \"Number of Interventions\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkf7VjX3VX0h"
      },
      "source": [
        "### Argumentation schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1llQs_EVX0i",
        "outputId": "94cf6e08-0d34-4c2a-c916-50f7b69e725c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of schemes: 28\n",
            "\n",
            "\n",
            "+-------------------------+-------------------------+\n",
            "| Argumentation scheme    |   Number of Occurrences |\n",
            "+=========================+=========================+\n",
            "| ERPracticalReasoning    |                      97 |\n",
            "+-------------------------+-------------------------+\n",
            "| Example                 |                      91 |\n",
            "+-------------------------+-------------------------+\n",
            "| ERExample               |                      84 |\n",
            "+-------------------------+-------------------------+\n",
            "| CauseToEffect           |                      55 |\n",
            "+-------------------------+-------------------------+\n",
            "| PracticalReasoning      |                      38 |\n",
            "+-------------------------+-------------------------+\n",
            "| Consequences            |                      36 |\n",
            "+-------------------------+-------------------------+\n",
            "| VerbalClassification    |                      25 |\n",
            "+-------------------------+-------------------------+\n",
            "| Sign                    |                      24 |\n",
            "+-------------------------+-------------------------+\n",
            "| CircumstantialAdHominem |                      22 |\n",
            "+-------------------------+-------------------------+\n",
            "| GenericAdHominem        |                      15 |\n",
            "+-------------------------+-------------------------+\n",
            "| Analogy                 |                      11 |\n",
            "+-------------------------+-------------------------+\n",
            "| Values                  |                      10 |\n",
            "+-------------------------+-------------------------+\n",
            "| PositionToKnow          |                      10 |\n",
            "+-------------------------+-------------------------+\n",
            "| PopularOpinion          |                       8 |\n",
            "+-------------------------+-------------------------+\n",
            "| FearAppeal              |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| DangerAppeal            |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| Ad hominem              |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| ERAdHominem             |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| ExpertOpinion           |                       6 |\n",
            "+-------------------------+-------------------------+\n",
            "| Alternatives            |                       6 |\n",
            "+-------------------------+-------------------------+\n",
            "| PopularPractice         |                       6 |\n",
            "+-------------------------+-------------------------+\n",
            "| Bias                    |                       4 |\n",
            "+-------------------------+-------------------------+\n",
            "| ERExpertOpinion         |                       4 |\n",
            "+-------------------------+-------------------------+\n",
            "| ArgumentFromAuthority   |                       4 |\n",
            "+-------------------------+-------------------------+\n",
            "| DirectAdHominem         |                       2 |\n",
            "+-------------------------+-------------------------+\n",
            "| PositiveConsequences    |                       1 |\n",
            "+-------------------------+-------------------------+\n",
            "| NegativeConsequences    |                       1 |\n",
            "+-------------------------+-------------------------+\n",
            "| SignFromOtherEvents     |                       1 |\n",
            "+-------------------------+-------------------------+\n"
          ]
        }
      ],
      "source": [
        "schemes = {}\n",
        "for key, value in data.items():\n",
        "    scheme = value[\"schemes\"]\n",
        "    for sch in scheme:\n",
        "        if sch in schemes:\n",
        "            schemes[sch] += 1\n",
        "        else:\n",
        "            schemes[sch] = 1\n",
        "\n",
        "schemes = dict(sorted(schemes.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"Total number of schemes:\", len(schemes))\n",
        "print(\"\\n\")\n",
        "schemes_data = [(key, value) for key, value in schemes.items()]\n",
        "print(tabulate(schemes_data, headers=[\"Argumentation scheme\", \"Number of Occurrences\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eERTtasKVX0i"
      },
      "source": [
        "### Critical questions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cq_stats(data):\n",
        "    cq_stats = {\n",
        "        \"overall\": {\n",
        "            \"count\": 0,\n",
        "            \"labels_count\": defaultdict(int),\n",
        "            \"percentage_useful\": 0.0\n",
        "        },\n",
        "        \"theoretical\": {\n",
        "            \"count\": 0,\n",
        "            \"labels_count\": defaultdict(int),\n",
        "            \"percentage_useful\": 0.0\n",
        "        },\n",
        "        \"llm_generated\": {\n",
        "            \"count\": 0,\n",
        "            \"labels_count\": defaultdict(int),\n",
        "            \"percentage_useful\": 0.0\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    theoretical_cqs = {}\n",
        "    llm_cqs = {}\n",
        "\n",
        "    for value in data.values():\n",
        "        for cq in value.get(\"cqs\", []):\n",
        "            cq_stats[\"overall\"][\"count\"] += 1\n",
        "            cq_id = cq.get(\"id\")\n",
        "            label = cq.get(\"label\")\n",
        "\n",
        "            if cq_id is None:\n",
        "                print(\"Warning: Missing ID in CQ ->\", cq)\n",
        "                continue  # Skip missing IDs\n",
        "\n",
        "            # Update global label counts\n",
        "            cq_stats[\"overall\"][\"labels_count\"][label] += 1\n",
        "\n",
        "            if \"_T_\" in cq_id:\n",
        "                cq_stats[\"theoretical\"][\"count\"] += 1\n",
        "                cq_stats[\"theoretical\"][\"labels_count\"][label] += 1\n",
        "\n",
        "                if cq_id in theoretical_cqs:\n",
        "                    print(f\"Duplicate theoretical CQ ID found: {cq_id}\")\n",
        "\n",
        "                theoretical_cqs[cq_id] = {\"cq\": cq[\"cq\"], \"label\": label}\n",
        "\n",
        "            elif \"_LLM_\" in cq_id:\n",
        "                cq_stats[\"llm_generated\"][\"count\"] += 1\n",
        "                cq_stats[\"llm_generated\"][\"labels_count\"][label] += 1\n",
        "\n",
        "                if cq_id in llm_cqs:\n",
        "                    print(f\"Duplicate LLM CQ ID found: {cq_id}\")\n",
        "\n",
        "                llm_cqs[cq_id] = {\"cq\": cq[\"cq\"], \"label\": label}\n",
        "\n",
        "    # Convert label defaultdicts to normal dicts and sort\n",
        "    for key in [\"overall\", \"theoretical\", \"llm_generated\"]:\n",
        "        cq_stats[key][\"labels_count\"] = dict(sorted(cq_stats[key][\"labels_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # Compute percentage of useful questions for each category\n",
        "    for key in [\"overall\", \"theoretical\", \"llm_generated\"]:\n",
        "        total = cq_stats[key][\"count\"]\n",
        "        useful = cq_stats[key][\"labels_count\"].get(\"Useful\", 0)\n",
        "        if total > 0:\n",
        "            cq_stats[key][\"percentage_useful\"] = round(useful / total, 2)\n",
        "\n",
        "    return cq_stats, theoretical_cqs, llm_cqs\n",
        "\n",
        "\n",
        "cq_stats_result, theoretical_cqs, llm_cqs = compute_cq_stats(data)\n",
        "\n",
        "# Check consistency\n",
        "print(\"\\n--- Consistency Check ---\")\n",
        "print(\"Expected theoretical CQs:\", cq_stats_result[\"theoretical\"][\"count\"], \"-> Found:\", len(theoretical_cqs))\n",
        "print(\"Expected LLM CQs:\", cq_stats_result[\"llm_generated\"][\"count\"], \"-> Found:\", len(llm_cqs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQu2ic6Mzoip",
        "outputId": "e21e227d-a211-4989-c365-359761efa588"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate LLM CQ ID found: TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L\n",
            "Duplicate LLM CQ ID found: TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L\n",
            "Duplicate theoretical CQ ID found: TRUMP_240_2_T__7\n",
            "\n",
            "--- Consistency Check ---\n",
            "Expected theoretical CQs: 993 -> Found: 992\n",
            "Expected LLM CQs: 3143 -> Found: 3141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_data = [\n",
        "    [\"Total CQs\", cq_stats_result[\"overall\"][\"count\"],\n",
        "     cq_stats_result[\"overall\"][\"labels_count\"][\"Useful\"],\n",
        "     cq_stats_result[\"overall\"][\"labels_count\"][\"Unhelpful\"],\n",
        "     cq_stats_result[\"overall\"][\"labels_count\"][\"Invalid\"],\n",
        "     f'{cq_stats_result[\"overall\"][\"percentage_useful\"] * 100:.0f}%'],\n",
        "    [\"Theoretical\", cq_stats_result[\"theoretical\"][\"count\"],\n",
        "     cq_stats_result[\"theoretical\"][\"labels_count\"][\"Useful\"],\n",
        "     cq_stats_result[\"theoretical\"][\"labels_count\"][\"Unhelpful\"],\n",
        "     cq_stats_result[\"theoretical\"][\"labels_count\"][\"Invalid\"],\n",
        "     f'{cq_stats_result[\"theoretical\"][\"percentage_useful\"] * 100:.0f}%'],\n",
        "    [\"LLM Generated\", cq_stats_result[\"llm_generated\"][\"count\"],\n",
        "     cq_stats_result[\"llm_generated\"][\"labels_count\"][\"Useful\"],\n",
        "     cq_stats_result[\"llm_generated\"][\"labels_count\"][\"Unhelpful\"],\n",
        "     cq_stats_result[\"llm_generated\"][\"labels_count\"][\"Invalid\"],\n",
        "     f'{cq_stats_result[\"llm_generated\"][\"percentage_useful\"] * 100:.0f}%']\n",
        "]\n",
        "\n",
        "print(tabulate(table_data, headers=[\"Category\", \"Total CQs\", \"Useful\", \"Unhelpful\", \"Invalid\", \"% Useful\"], tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuHufovyyP5",
        "outputId": "87571616-10f9-4165-fcdf-3451bcf80a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+----------+-------------+-----------+------------+\n",
            "| Category      |   Total CQs |   Useful |   Unhelpful |   Invalid | % Useful   |\n",
            "+===============+=============+==========+=============+===========+============+\n",
            "| Total CQs     |        4136 |     2790 |         893 |       453 | 67%        |\n",
            "+---------------+-------------+----------+-------------+-----------+------------+\n",
            "| Theoretical   |         993 |      415 |         394 |       184 | 42%        |\n",
            "+---------------+-------------+----------+-------------+-----------+------------+\n",
            "| LLM Generated |        3143 |     2375 |         499 |       269 | 76%        |\n",
            "+---------------+-------------+----------+-------------+-----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qADj-G3VX0k"
      },
      "source": [
        "### Overall stats per dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def compute_stats(data):\n",
        "    stats = defaultdict(lambda: {\n",
        "        \"num_interventions\": 0,\n",
        "        \"authors_count\": defaultdict(int),\n",
        "        \"schemes_count\": defaultdict(int),\n",
        "        \"authors\": {},\n",
        "        \"schemes\": {},\n",
        "        \"cqs\": {\n",
        "            \"total_cqs\": {\"count\": 0, \"labels_count\": defaultdict(int), \"percentage_useful\": 0.0},\n",
        "            \"theoretical\": {\"count\": 0, \"labels_count\": defaultdict(int), \"percentage_useful\": 0.0},\n",
        "            \"llm_generated\": {\"count\": 0, \"labels_count\": defaultdict(int), \"percentage_useful\": 0.0}\n",
        "        }\n",
        "    })\n",
        "\n",
        "    for key, value in data.items():\n",
        "        dataset = value[\"dataset\"]\n",
        "        author = key.split(\"_\")[0]  # Extract author name\n",
        "\n",
        "        stats[dataset][\"num_interventions\"] += 1\n",
        "        stats[dataset][\"authors_count\"][author] += 1\n",
        "\n",
        "        for scheme in value.get(\"schemes\", []):\n",
        "            stats[dataset][\"schemes_count\"][scheme] += 1\n",
        "\n",
        "        # Critical Questions (CQs)\n",
        "        for cq in value.get(\"cqs\", []):\n",
        "            stats[dataset][\"cqs\"][\"total_cqs\"][\"count\"] += 1\n",
        "            label = cq[\"label\"]\n",
        "            cq_id = cq[\"id\"]\n",
        "\n",
        "            stats[dataset][\"cqs\"][\"total_cqs\"][\"labels_count\"][label] += 1\n",
        "\n",
        "            if \"_T_\" in cq_id:\n",
        "                stats[dataset][\"cqs\"][\"theoretical\"][\"count\"] += 1\n",
        "                stats[dataset][\"cqs\"][\"theoretical\"][\"labels_count\"][label] += 1\n",
        "            elif \"_LLM_\" in cq_id:\n",
        "                stats[dataset][\"cqs\"][\"llm_generated\"][\"count\"] += 1\n",
        "                stats[dataset][\"cqs\"][\"llm_generated\"][\"labels_count\"][label] += 1\n",
        "\n",
        "    # Convert defaultdicts to regular dicts and compute percentages\n",
        "    result = {}\n",
        "    for ds, ds_data in stats.items():\n",
        "        num_authors = len(ds_data[\"authors_count\"])\n",
        "        ds_data[\"authors_count\"] = dict(sorted(ds_data[\"authors_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "        avg_interventions = ds_data[\"num_interventions\"] / num_authors if num_authors > 0 else 0\n",
        "\n",
        "        unique_schemes = len(ds_data[\"schemes_count\"])\n",
        "        num_schemes = sum(ds_data[\"schemes_count\"].values())\n",
        "        ds_data[\"schemes_count\"] = dict(sorted(ds_data[\"schemes_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        for cq_type in [\"total_cqs\", \"theoretical\", \"llm_generated\"]:\n",
        "            ds_data[\"cqs\"][cq_type][\"labels_count\"] = dict(sorted(ds_data[\"cqs\"][cq_type][\"labels_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "            useful_count = ds_data[\"cqs\"][cq_type][\"labels_count\"].get(\"Useful\", 0)\n",
        "            total_count = ds_data[\"cqs\"][cq_type][\"count\"]\n",
        "            ds_data[\"cqs\"][cq_type][\"percentage_useful\"] = round(useful_count / total_count, 2) if total_count > 0 else 0\n",
        "\n",
        "        result[ds] = {\n",
        "            \"num_interventions\": ds_data[\"num_interventions\"],\n",
        "            \"authors\": {\n",
        "                \"num_authors\": num_authors,\n",
        "                \"authors_count\": ds_data[\"authors_count\"],\n",
        "                \"avg_interventions_per_author\": avg_interventions\n",
        "            },\n",
        "            \"schemes\": {\n",
        "                \"unique_schemes\": unique_schemes,\n",
        "                \"num_schemes\": num_schemes,\n",
        "                \"schemes_count\": ds_data[\"schemes_count\"]\n",
        "            },\n",
        "            \"cqs\": ds_data[\"cqs\"]\n",
        "        }\n",
        "\n",
        "    return dict(sorted(result.items(), key=lambda item: item[1][\"num_interventions\"], reverse=True))\n",
        "\n",
        "stats_result = compute_stats(data)\n"
      ],
      "metadata": {
        "id": "186rQTIZ68bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats_result"
      ],
      "metadata": {
        "id": "HDhZUaNq7S-x",
        "outputId": "f8d93b8e-a86c-45db-94ba-fc457378e60e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'US2016': {'num_interventions': 80,\n",
              "  'authors': {'num_authors': 3,\n",
              "   'authors_count': {'TRUMP': 43, 'CLINTON': 34, 'HOLT': 3},\n",
              "   'avg_interventions_per_author': 26.666666666666668},\n",
              "  'schemes': {'unique_schemes': 18,\n",
              "   'num_schemes': 342,\n",
              "   'schemes_count': {'Example': 74,\n",
              "    'CauseToEffect': 45,\n",
              "    'Consequences': 36,\n",
              "    'PracticalReasoning': 34,\n",
              "    'VerbalClassification': 25,\n",
              "    'Sign': 24,\n",
              "    'CircumstantialAdHominem': 22,\n",
              "    'GenericAdHominem': 15,\n",
              "    'Values': 10,\n",
              "    'PositionToKnow': 10,\n",
              "    'Analogy': 7,\n",
              "    'FearAppeal': 7,\n",
              "    'DangerAppeal': 7,\n",
              "    'PopularOpinion': 6,\n",
              "    'Alternatives': 6,\n",
              "    'PopularPractice': 6,\n",
              "    'ExpertOpinion': 4,\n",
              "    'Bias': 4}},\n",
              "  'cqs': {'total_cqs': {'count': 2121,\n",
              "    'labels_count': {'Useful': 1387, 'Unhelpful': 449, 'Invalid': 285},\n",
              "    'percentage_useful': 0.65},\n",
              "   'theoretical': {'count': 722,\n",
              "    'labels_count': {'Unhelpful': 283, 'Useful': 270, 'Invalid': 169},\n",
              "    'percentage_useful': 0.37},\n",
              "   'llm_generated': {'count': 1399,\n",
              "    'labels_count': {'Useful': 1117, 'Unhelpful': 166, 'Invalid': 116},\n",
              "    'percentage_useful': 0.8}}},\n",
              " 'rrd': {'num_interventions': 72,\n",
              "  'authors': {'num_authors': 44,\n",
              "   'authors_count': {'Antanagoge': 4,\n",
              "    'JJMurray': 4,\n",
              "    'howie': 4,\n",
              "    'Mulder': 3,\n",
              "    'SofieM': 3,\n",
              "    'atraveller': 2,\n",
              "    'cd38': 2,\n",
              "    'citizen-s': 2,\n",
              "    'darawayne': 2,\n",
              "    'JetJock': 2,\n",
              "    'NYCMuscleman18': 2,\n",
              "    'PracticalJo': 2,\n",
              "    'SWong': 2,\n",
              "    'dberger': 2,\n",
              "    'dlpoole': 2,\n",
              "    'elizwestley': 2,\n",
              "    'golff4fun': 2,\n",
              "    'mcliverty': 2,\n",
              "    'smr': 2,\n",
              "    'travellots': 2,\n",
              "    'Doctor-Mom': 1,\n",
              "    'FoodAllergyMom': 1,\n",
              "    'Frequent-Flyer': 1,\n",
              "    'AFCHF': 1,\n",
              "    'AK-traveler': 1,\n",
              "    'AllergyDad': 1,\n",
              "    'Bill': 1,\n",
              "    'Vec': 1,\n",
              "    'aimwill': 1,\n",
              "    'ambersky': 1,\n",
              "    'annoyed': 1,\n",
              "    'JDwyer': 1,\n",
              "    'Javier': 1,\n",
              "    'KHenrickson': 1,\n",
              "    'Mpogoda': 1,\n",
              "    'PeanutAllergy': 1,\n",
              "    'Qubbin': 1,\n",
              "    'drgreg': 1,\n",
              "    'hgranato': 1,\n",
              "    'kateinhawaii': 1,\n",
              "    'lauraclare': 1,\n",
              "    'msrocker': 1,\n",
              "    'smg': 1,\n",
              "    'zuclinator': 1},\n",
              "   'avg_interventions_per_author': 1.6363636363636365},\n",
              "  'schemes': {'unique_schemes': 5,\n",
              "   'num_schemes': 195,\n",
              "   'schemes_count': {'ERPracticalReasoning': 97,\n",
              "    'ERExample': 84,\n",
              "    'ERAdHominem': 7,\n",
              "    'ERExpertOpinion': 4,\n",
              "    'Example': 3}},\n",
              "  'cqs': {'total_cqs': {'count': 1403,\n",
              "    'labels_count': {'Useful': 1022, 'Unhelpful': 301, 'Invalid': 80},\n",
              "    'percentage_useful': 0.73},\n",
              "   'theoretical': {'count': 203,\n",
              "    'labels_count': {'Useful': 110, 'Unhelpful': 84, 'Invalid': 9},\n",
              "    'percentage_useful': 0.54},\n",
              "   'llm_generated': {'count': 1200,\n",
              "    'labels_count': {'Useful': 912, 'Unhelpful': 217, 'Invalid': 71},\n",
              "    'percentage_useful': 0.76}}},\n",
              " 'moral_maze_schemes': {'num_interventions': 20,\n",
              "  'authors': {'num_authors': 9,\n",
              "   'authors_count': {'MT': 4,\n",
              "    'CF': 3,\n",
              "    'JL': 3,\n",
              "    'CL': 2,\n",
              "    'JW': 2,\n",
              "    'MP': 2,\n",
              "    'ND': 2,\n",
              "    'Helen': 1,\n",
              "    'Melanie': 1},\n",
              "   'avg_interventions_per_author': 2.2222222222222223},\n",
              "  'schemes': {'unique_schemes': 10,\n",
              "   'num_schemes': 34,\n",
              "   'schemes_count': {'CauseToEffect': 10,\n",
              "    'Example': 8,\n",
              "    'PracticalReasoning': 4,\n",
              "    'Analogy': 3,\n",
              "    'PopularOpinion': 2,\n",
              "    'ExpertOpinion': 2,\n",
              "    'DirectAdHominem': 2,\n",
              "    'PositiveConsequences': 1,\n",
              "    'NegativeConsequences': 1,\n",
              "    'SignFromOtherEvents': 1}},\n",
              "  'cqs': {'total_cqs': {'count': 372,\n",
              "    'labels_count': {'Useful': 248, 'Unhelpful': 71, 'Invalid': 53},\n",
              "    'percentage_useful': 0.67},\n",
              "   'theoretical': {'count': 43,\n",
              "    'labels_count': {'Useful': 24, 'Unhelpful': 15, 'Invalid': 4},\n",
              "    'percentage_useful': 0.56},\n",
              "   'llm_generated': {'count': 329,\n",
              "    'labels_count': {'Useful': 224, 'Unhelpful': 56, 'Invalid': 49},\n",
              "    'percentage_useful': 0.68}}},\n",
              " 'us2016reddit': {'num_interventions': 14,\n",
              "  'authors': {'num_authors': 13,\n",
              "   'authors_count': {'secretcurse': 2,\n",
              "    'Elmattador': 1,\n",
              "    'Glblwrmingisfak': 1,\n",
              "    '17th': 1,\n",
              "    'AngelComa': 1,\n",
              "    'Tuatho': 1,\n",
              "    'Velshtein': 1,\n",
              "    'Zewstain': 1,\n",
              "    'MR': 1,\n",
              "    'MrFordization': 1,\n",
              "    'Sithsaber': 1,\n",
              "    'grayk47': 1,\n",
              "    'mfball': 1},\n",
              "   'avg_interventions_per_author': 1.0769230769230769},\n",
              "  'schemes': {'unique_schemes': 4,\n",
              "   'num_schemes': 18,\n",
              "   'schemes_count': {'Ad hominem': 7,\n",
              "    'Example': 6,\n",
              "    'ArgumentFromAuthority': 4,\n",
              "    'Analogy': 1}},\n",
              "  'cqs': {'total_cqs': {'count': 240,\n",
              "    'labels_count': {'Useful': 133, 'Unhelpful': 72, 'Invalid': 35},\n",
              "    'percentage_useful': 0.55},\n",
              "   'theoretical': {'count': 25,\n",
              "    'labels_count': {'Unhelpful': 12, 'Useful': 11, 'Invalid': 2},\n",
              "    'percentage_useful': 0.44},\n",
              "   'llm_generated': {'count': 215,\n",
              "    'labels_count': {'Useful': 122, 'Unhelpful': 60, 'Invalid': 33},\n",
              "    'percentage_useful': 0.57}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save datasets stats\n",
        "filename = \"datasets_stats.json\"\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(stats_result, f, indent=4, ensure_ascii=False)\n",
        "print(f\"Stats saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxoFVhVtu_Hl",
        "outputId": "46c0c908-dc86-465e-af8a-be1854618ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats saved to datasets_stats.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "datasets = [\"Overall\"] + list(stats_result.keys())\n",
        "label_types = [\"Useful\", \"Unhelpful\", \"Invalid\"]\n",
        "\n",
        "colors = {\n",
        "    \"Useful\": (\"lightgreen\", \"green\"),\n",
        "    \"Unhelpful\": (\"navajowhite\", \"orange\"),\n",
        "    \"Invalid\": (\"lightcoral\", \"red\"),\n",
        "}\n",
        "\n",
        "# Extract data\n",
        "data_total = {label: [] for label in label_types}\n",
        "data_theoretical = {label: [] for label in label_types}\n",
        "\n",
        "for label in label_types:\n",
        "    data_total[label].append(cq_stats_result[\"llm_generated\"][\"labels_count\"].get(label, 0))\n",
        "    data_theoretical[label].append(cq_stats_result[\"theoretical\"][\"labels_count\"].get(label, 0))\n",
        "\n",
        "for dataset in stats_result.keys():\n",
        "    total_labels = stats_result[dataset][\"cqs\"][\"total_cqs\"][\"labels_count\"]\n",
        "    theoretical_labels = stats_result[dataset][\"cqs\"][\"theoretical\"][\"labels_count\"]\n",
        "\n",
        "    for label in label_types:\n",
        "        data_total[label].append(total_labels.get(label, 0))\n",
        "        data_theoretical[label].append(theoretical_labels.get(label, 0))\n",
        "\n",
        "# Plotting\n",
        "x = np.arange(len(datasets))\n",
        "width = 0.3\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# Plot bars\n",
        "for i, label in enumerate(label_types):\n",
        "    ax.bar(x + i * width, data_total[label], width,\n",
        "           label=f\"LLM-Generated {label}\", color=colors[label][0], edgecolor=\"black\")\n",
        "\n",
        "    ax.bar(x + i * width, data_theoretical[label], width,\n",
        "           label=f\"Theoretical {label}\", color=colors[label][1], edgecolor=\"black\", alpha=0.8)\n",
        "\n",
        "ax.set_xlabel(\"Datasets\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_xticks(x + width)\n",
        "ax.set_xticklabels(datasets, rotation=45, ha=\"right\", fontsize=11)\n",
        "\n",
        "ax.set_ylabel(\"Count\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_title(\"Critical Question Labels by Dataset\", fontsize=14, fontweight=\"bold\")\n",
        "ax.legend(title=\"Label Source\", loc=\"upper left\", bbox_to_anchor=(1,1), fontsize=10)\n",
        "\n",
        "ax.yaxis.grid(True, linestyle=\"--\", alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "9oGP2qcWGOzg",
        "outputId": "ab51228a-2610-4594-df20-d3a15e6b4186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-3iINQ-VX0l"
      },
      "source": [
        "### Critical questions in depth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers stats with LLM models"
      ],
      "metadata": {
        "id": "5FYfD5-NZ4JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "login()  # 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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "73892706ef784f39b574e334f0b44fe3",
            "a0d1159ae9704b3797d431d7af5c2a47",
            "5f50fefcc51047c69fa8bb24acd0c3e2",
            "aeeb1e4dfe4c4b1ca8b24897da06e074",
            "484ac1d1f15b43e2ba45b9243ceab68b",
            "f74292e4727e42f7b594abea3e6a381a",
            "5dc7fc71698b46269f4e25933385be88",
            "bcda1596f51d48ab965b3e9f8e974994",
            "fcde228d75db483e9e6b35393c216adf",
            "d6f73bdfacdf4c12850b72a2f6e76d77",
            "40dcba5cdcc248fb93293ca4806600da",
            "da4dbbf4e51b40368f4a4e2ab78542eb",
            "460c34dbd6ed4ee882ae4c0b1a8951ae",
            "6f8f25c186c049d19d1646bf12207882",
            "05892c518c8b4585a9bd03d2339ec1dd",
            "e61fc76eb1a7408db147cba4730d7266",
            "02c9183a56bd432a801ace6f100b11f7",
            "444433aa3dfc45ce84174528fa953dc3",
            "329a93c3e6bb4a0f95b8818bd8ffc574",
            "be785dc1fa6d4e7c846358f6a28c579b"
          ]
        },
        "id": "_EmldYjetUTf",
        "outputId": "2640a2eb-44df-4664-9c01-1ca735e8e39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73892706ef784f39b574e334f0b44fe3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_list = [\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "]"
      ],
      "metadata": {
        "id": "xXEnyIan4Go3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_token_stats_for_models(cq_dict, models_list):\n",
        "    stats_by_model = {}\n",
        "\n",
        "    for model_name in models_list:\n",
        "        print(f\"Processing {model_name}...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        token_lengths = [len(tokenizer(cq[\"cq\"])[\"input_ids\"]) for cq in cq_dict.values()]\n",
        "\n",
        "        if not token_lengths:\n",
        "            stats_by_model[model_name] = {\"max\": 0, \"min\": 0, \"avg\": 0.0}\n",
        "            continue\n",
        "\n",
        "        stats_by_model[model_name] = {\n",
        "            \"max\": max(token_lengths),\n",
        "            \"min\": min(token_lengths),\n",
        "            \"avg\": round(sum(token_lengths) / len(token_lengths), 2)\n",
        "        }\n",
        "\n",
        "    return stats_by_model\n",
        "\n",
        "# Compute stats for each model\n",
        "theoretical_token_stats = compute_token_stats_for_models(theoretical_cqs, models_list)\n",
        "llm_token_stats = compute_token_stats_for_models(llm_cqs, models_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696,
          "referenced_widgets": [
            "c1a2fd4ec02345e29e530b76e0aed61d",
            "aa246fed1db142efadfaa3b10d826746",
            "1037fe4fcb374f3aa767e512630e148e",
            "84ba40d320124ccba5d7a6b2460e9f0d",
            "9c48c91d8fac48759d1a490ffdfc3e33",
            "d66a3dd348ca4f799544ecfb9f358610",
            "b4771d1f5aca4883bfed97e9b50eb40b",
            "2d638548739c48258d9481c8d02eb8f4",
            "9e3cb30c39904dd4bb362600cf056013",
            "6cb172f7e6db4ab599a0d38cf28a32e2",
            "e4fec02962534535b06cb622e301a983",
            "c93f25216cb94e33915cf19ccee77283",
            "6a7dfb4f40264dfa96c948d25f79d9b1",
            "dc291016b0a548e4a49daecc9e7175d0",
            "7ad7b718986a422cba5b578074ce8667",
            "304ade0b31ee456b848202c0f869defb",
            "d7def69f300b4754bad50579daf53603",
            "2d9b38ae020d4a5c9ee904a91fd5989e",
            "35d79aa9809845e9bacfd93c36dbb95e",
            "f0e2680caf2c4db9b0e0038e2c35cd16",
            "5a5cfad6fb6841a4a093b78564bfc60b",
            "794a6342cb94404d8a4e70955aad7920",
            "44486a67da1644baa0deb8361b8aeb65",
            "734aa5d9a03e45bca1a14b86f6a5503b",
            "901a1413556a4ef38b1b929682160ce1",
            "caa1d87fe99d4e2c8de3ae9d9b39ffa0",
            "60063d3142ff4d10ae31488f82c1417b",
            "02bf55ebb2524a168cc4c4f3d11476c2",
            "971d54ab584c4134b439c255e492a6e7",
            "eadfa2825719488e89b332fe645efd27",
            "97a5b930284d45c592dc22972cc1b702",
            "70ce5482f40346c99e9a747873988933",
            "634dc50c5e3343f4a1feab88513de6d5",
            "8a82795213694bc4aec473dd2f06496a",
            "6ba07bf044e344459b0b508b37fe3ff9",
            "ae0684f2aaf547f7bf6e6111e0a56808",
            "9403bf5a72fd4875a179b7ed6b293893",
            "61d2946e2b2b47dc9af7c2b1d96ef3b6",
            "8ef297177e544ce78b7366eba2ea89b2",
            "c3ca3655567142108b82b9a999816d35",
            "dbccd0c6228146c2976051a08b6a0e2e",
            "34be573fce29468783ad20c453e9b325",
            "e612b42843d54b9b878eb13aa3ef59a7",
            "615cf754805f4e828303c42c6cfb611b",
            "69b48d2ac45649ab8cffb257c7b35808",
            "1ea4c99287134088914957bf087b9d38",
            "08e8b3973d35479286b91a825e61336c",
            "8023a4d176264e018c99761ebe8e766c",
            "e9c079363c67496f9f8c4c99375c443b",
            "0ec6f0b4c3bc43dea6502c910a668c8e",
            "a6813197451f4ff7b95397aaa56199c3",
            "78fce113bf5042e894cc43d5ec03e1f2",
            "a7dc8539a5d84e29839597bbde224fc4",
            "40b4291869f04945aa76e6b59b8bff18",
            "6635a82090e145a1b3012950ec2ab268",
            "dcc91aaf15f8406fbd570f9d36b59812",
            "1fc84112f8914625a7c87299f6ee531d",
            "54afe0e37257406cbc9fe28c22ad18d9",
            "ff661dbf6bdb440f83808f081610da6b",
            "0b673d9ce30f4a1dab52919bcbeeea24",
            "7d1f9051c6c944519cf3a1ae4ba36775",
            "c1b45ba2633d4d29b0274b3d0eec547e",
            "befebc2ec83145c2856bd110bbd097ab",
            "749bbb2d7b844c2a97932c9b800dc197",
            "408ad39921454c31ac235ea7ae915121",
            "006bfe2a2da34577adc706777184e7e6",
            "9c10eb3e14c349e9900a61d16305c3e3",
            "0f7a05d969374c77b6f434fba04f52a6",
            "b0bf57d1653d4f4094efd897430f01ee",
            "5e411a078f084538aa28dfae2a0bfc5b",
            "1cce7ad0c8fb40b1a94f7faebf69003e",
            "61c578612d1f4de1b3ab794dfc0fdd06",
            "2ec22dcbdd4440f6b56ec322e97e8054",
            "8065787ab34b4d45afd19ae73bfa5d66",
            "47a1c529d38b486bbb702719445f35c7",
            "1e216292830d4abfa96acac883368fd7",
            "5e3b472bda394ab1af596f48f03d0b9a",
            "fca878602d8540fe9adcb9a597035101",
            "7a2a488b0f69400c9d09409b135d739f",
            "6717b9e99ca04e798fc61efa7ff15955",
            "24aa7f8eafb7490ea9fe372d99925d88",
            "a29e1d7014d14033a19c83b04fdb8eb2",
            "2dc91eb33b474424b4453931e01ae022",
            "64cb66b05845479dadd2d92152ea72a0",
            "febe1f95c65448c6b34b2980c26ffa32",
            "bbf82e3328fc49cc9c7cf1c850c353ce",
            "c8a749bf1ae743a4a2603ab78322eab6",
            "ea1bbae417514f82b709bc76090aed96",
            "32cb44ed4a714cafb626a41c50b5a904",
            "c8e195b75d054065b14b4d574ba365ca",
            "775b3affbc8e4b76b25c00e11fced279",
            "15d4a383128a4603853fc1edfbbecd13",
            "1f67699aff5e4653964515c91a0cbcae",
            "58ea844ffc324029ba9e1f9aeb3d13ee",
            "39da706f1f7c4d2691c2b96b71de5c11",
            "cac6c88d09364597832f570ade2c8699",
            "37d1871967844fcca438a13e854c317d",
            "5efea9eb8a9849dba4865877b6d91626",
            "af8433da2e5549dd84e3aa74f272f947",
            "53df8a0819894845b2b3eb064e01b791",
            "5deab28f41554218bc45525c31885ecc",
            "77dcda9956cb4bd9954dc516da773095",
            "b97d1b411eb24465bd024cc4b5f2e460",
            "6ad37350068346e59c0972216e003151",
            "3e397dd378af4e5f838c929569fe138f",
            "b29e1fd1ba4b4d37b5fe8be17218ca8e",
            "2260a200e2484a71b08b8d1c6c2f007b",
            "45fe0b1a61ea44adae8f52984aebe309",
            "e5118c6f77b04829ae1443443e443a38",
            "533312f67e604fa1a0b231cc0e418b45",
            "930b0804f5124eec9402d2be4407139e",
            "df6aab18367e470bb6a40c142089c07c",
            "12ba88a77ec94f5e95e23ba328768236",
            "b7a5c9d658254860a0f112ef8bdee7fd",
            "bf889d6aab9943a9a6626d47b3f08b4a",
            "f9c92cd83d53454b8efcfaf7530df35e",
            "6241d47475de49cc88da8e010591cfdb",
            "d25920e8faac42329a488a2fd3e75ce8",
            "a6d5243452c5402685f57a9d3de69f73",
            "e676823891854656bc56275dad72cf3a",
            "09fe71513ded473c95e98499b342dc22",
            "a348584cf0ca4774899d1ace0bfb4610",
            "d59a069e01d64fa3b99c0cb1806885bd",
            "179af4f20a514480bf76ff88cf8ad537",
            "684b8cfea99043f6b7a93bfb9cacb251",
            "1a75211c96bb47209e9b99e293965b68",
            "ef8e76b2b1784098b4aebf8c2010bb65",
            "07cbf3c3388544f6a4b3c96bf079aada",
            "980edf69696f4addb51c9ab3a39745d5",
            "7fbf577b73a7410a946dc8a2d63b70ce",
            "1c2fa26c22e542b787d98e75a2e646f6",
            "fe6ab3ca368f4c87b3b577e05e26165f",
            "a1c25824fde74eff972c57182acf7f98",
            "22a236e9740a4f5c8e8422882a19a5a6",
            "1b8e79007c6d4260b120f2b913d294ad",
            "ee3914f837f94276b0614b4f414912d5",
            "708c781044b8415ab730cdc513169dee",
            "c4ec039bd8f54f8cbb4d174b057e7789",
            "daecae2326a64709ab44b4984bf544a1",
            "5e6b8d22b46e4046b80facdc6e8bfebc",
            "25cb59a241824fd0b58fcdbaf6e8d580",
            "0b8bc1d872d1470e81f5461771d128f2",
            "518e52f85a0446938c047f1eb6c76ef8"
          ]
        },
        "id": "ucgzf2Hrrx2W",
        "outputId": "8973f255-6ebf-424d-d2b7-af1e75663399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing meta-llama/Meta-Llama-3-8B-Instruct...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1a2fd4ec02345e29e530b76e0aed61d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c93f25216cb94e33915cf19ccee77283"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44486a67da1644baa0deb8361b8aeb65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing deepseek-ai/DeepSeek-R1-Distill-Llama-8B...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a82795213694bc4aec473dd2f06496a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69b48d2ac45649ab8cffb257c7b35808"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing mistralai/Mixtral-8x7B-Instruct-v0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcc91aaf15f8406fbd570f9d36b59812"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c10eb3e14c349e9900a61d16305c3e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca878602d8540fe9adcb9a597035101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32cb44ed4a714cafb626a41c50b5a904"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Qwen/Qwen2.5-7B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53df8a0819894845b2b3eb064e01b791"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "930b0804f5124eec9402d2be4407139e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a348584cf0ca4774899d1ace0bfb4610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1c25824fde74eff972c57182acf7f98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "Processing deepseek-ai/DeepSeek-R1-Distill-Llama-8B...\n",
            "Processing mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
            "Processing Qwen/Qwen2.5-7B-Instruct...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theoretical_data = [[model, stats[\"max\"], stats[\"min\"], stats[\"avg\"]] for model, stats in theoretical_token_stats.items()]\n",
        "llm_data = [[model, stats[\"max\"], stats[\"min\"], stats[\"avg\"]] for model, stats in llm_token_stats.items()]\n",
        "\n",
        "print(\"Theoretical CQ Token Stats:\")\n",
        "print(tabulate(theoretical_data, headers=[\"Model\", \"Max Tokens\", \"Min Tokens\", \"Avg Tokens\"], tablefmt=\"grid\"))\n",
        "print(\"\\n\")\n",
        "print(\"\\nLLM-Generated CQ Token Stats:\")\n",
        "print(tabulate(llm_data, headers=[\"Model\", \"Max Tokens\", \"Min Tokens\", \"Avg Tokens\"], tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyPcTTKJ21EH",
        "outputId": "438a480d-0b70-4a8b-fee6-24fed38a1236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical CQ Token Stats:\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| Model                                    |   Max Tokens |   Min Tokens |   Avg Tokens |\n",
            "+==========================================+==============+==============+==============+\n",
            "| meta-llama/Meta-Llama-3-8B-Instruct      |           65 |            9 |        27.01 |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| deepseek-ai/DeepSeek-R1-Distill-Llama-8B |           65 |            9 |        27.01 |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| mistralai/Mixtral-8x7B-Instruct-v0.1     |           73 |            9 |        28.8  |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| Qwen/Qwen2.5-7B-Instruct                 |           64 |            8 |        26.11 |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "\n",
            "\n",
            "\n",
            "LLM-Generated CQ Token Stats:\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| Model                                    |   Max Tokens |   Min Tokens |   Avg Tokens |\n",
            "+==========================================+==============+==============+==============+\n",
            "| meta-llama/Meta-Llama-3-8B-Instruct      |           72 |            9 |        28.9  |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| deepseek-ai/DeepSeek-R1-Distill-Llama-8B |           72 |            9 |        28.9  |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| mistralai/Mixtral-8x7B-Instruct-v0.1     |           79 |            9 |        31.55 |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n",
            "| Qwen/Qwen2.5-7B-Instruct                 |           71 |            8 |        27.96 |\n",
            "+------------------------------------------+--------------+--------------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy tokenizer"
      ],
      "metadata": {
        "id": "lUZdM_-WaFQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_cqs"
      ],
      "metadata": {
        "id": "zctfi2E5K6WK",
        "outputId": "56c95bc5-a7dc-440e-a679-42952576047a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What are the potential drawbacks or risks of increased cooperation with Muslim nations or communities, and how would Clinton mitigate these risks?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What evidence is there that Donald Trump's rhetoric has led to the alienation of Muslim communities, and how would Clinton's approach to working with these communities be more effective?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton define \"working more closely\" with allies, and what specific actions or policies would she implement to achieve this goal?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What is the evidence that Muslim communities are \"on the front lines\" of counter-terrorism efforts, and how would Clinton\\'s policies support and empower these communities in this role?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific intelligence benefits does Clinton expect to gain from working more closely with European and Middle Eastern allies?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How does Clinton plan to ensure that law enforcement cooperation with Muslim communities is effective and respectful, and what measures will be taken to prevent alienation and mistrust?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How does Clinton's plan to work with NATO and Middle Eastern nations differ from Trump's approach, and what specific results can be expected from Clinton's strategy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific intelligence benefits have been gained from working with European and Middle Eastern allies in the past, and how do these benefits justify increased cooperation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence is there that Donald Trump has been \"dismissive\" of working with allies, and how has this alleged dismissiveness impacted national security?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How would Clinton balance the need for intelligence gathering with the potential risks of relying on information from Muslim nations or communities that may have their own agendas or biases?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Clinton's approach to counter-terrorism take into account the complexities and nuances of different Muslim-majority nations and communities, and what efforts will be made to avoid stereotyping or stigmatizing entire groups?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton plan to \"vacuum up\" intelligence from these regions, and what methods will be used to ensure the accuracy and reliability of the gathered information?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Clinton plan to address the potential concerns of non-Muslim communities who may feel that increased cooperation with Muslim nations or communities could compromise their own security or interests?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific examples can Clinton provide of Trump \"insulting\" Muslims abroad and at home, and how have these alleged insults impacted cooperation with Muslim nations and the American Muslim community?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What evidence is there that the American Muslim community is \"on the front lines\" of providing information to combat terrorism, and how can their role be enhanced and supported?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'How would the proposed policies address the root causes of income inequality, rather than just providing temporary benefits?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How would you define \"the wealthy\" and what specific tax reforms would you implement to ensure they pay their \"fair share\", and how would you measure the impact of these reforms?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How would you ensure that the benefits of economic growth are shared equitably among all segments of society, and not just concentrated among the wealthy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'How would the policies be tailored to address the specific needs of different demographics?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific policies would you implement to ensure that workers share in the profits they help create, and how would you measure their effectiveness?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How would the proposed policies (paid family leave, earned sick days, affordable child care, debt-free college) be funded, and what would be the impact on the economy and taxpayers?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific corporate loopholes would you close, and how would you ensure that corporations do not find new ways to exploit the tax system?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What are the potential unintended consequences of the proposed policies, and how would they be addressed?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the definition of \"help create the profits\" and how does one measure it? Is it based on hours worked, productivity, or some other factor?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How would you balance the need to increase revenue from wealthy individuals and corporations with the potential impact on economic growth and job creation?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'How would the proposed policies be implemented, and what would be the timeline for their implementation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'How would you build bipartisan support for your policies, and what would you do if faced with opposition from Congress or other stakeholders?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the definition of \"fair share\" and how would it be determined? Is it based on a specific percentage of income or wealth, and how would it be enforced?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the proposed mechanism for providing paid family leave, earned sick days, affordable child care, and debt-free college, and how would you fund these initiatives?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What specific plans do you have to address the root causes of income inequality, and how would you measure the success of these plans?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How would you define and measure \"struggling to balance family and work\", and what specific actions would you take to support those individuals?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the evidence that increasing taxes on the wealthy would lead to economic growth and improved living standards for the majority of citizens?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What are the specific \"corporate loopholes\" that would be closed, and how would this impact businesses and job creation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': 'What is the track record of the speaker in implementing similar policies in the past, and what were the results?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_1_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What is your vision for the role of government in addressing social and economic issues, and how would you balance individual freedom with collective responsibility?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Clinton plan to \"get it done\", and what are the potential obstacles and challenges to implementing her policies?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"How does Clinton's proposal address the root causes of the previous economic crisis, and what specific policies does she propose to prevent another crisis?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Clinton's proposal address the issue of climate change, and what specific policies does she propose to mitigate its effects?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"Is Clinton's characterization of Trump's views on climate change accurate, and is her own stance on the issue supported by scientific evidence?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific policies is Clinton referring to when she says \"the policies that failed us in the first place\"? Are these policies accurately attributed to Trump?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What evidence is there to support the claim that 9 million people lost their jobs, 5 million people lost their homes, and $13 trillion in family wealth was wiped out during the previous economic crisis?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What is the basis for Clinton\\'s assertion that some country will be the \"clean-energy superpower of the 21st century\", and how does she plan to ensure that the US is that country?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What specific proposals has Clinton made to address the issues she raises, and are they feasible and effective?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific actions does Clinton plan to take to \"get it done\" in terms of implementing her policies, and what are the potential obstacles to success?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"What are the potential risks and unintended consequences of Clinton's proposed policies, and how does she plan to address them?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What evidence is there to support the claim that some country will be the \"clean-energy superpower of the 21st century\", and how does Clinton\\'s proposal address this opportunity?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"What is the basis for Clinton's claim that Trump thinks climate change is a hoax perpetrated by the Chinese, and is this an accurate representation of Trump's views?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What are the \"policies that failed us in the first place\" that Clinton is referring to, and how does she propose to avoid repeating them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"How does Clinton's proposal compare to Trump's proposal in terms of their potential impact on the economy, and what are the key differences between their approaches?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What are the \"independent experts\" that Clinton mentions, and are their opinions credible and unbiased?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What evidence does Clinton have to support her claim that her policies will lead to a \"potentially much better economy\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_21_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Who are the \"independent experts\" that have looked at Clinton\\'s and Trump\\'s proposals, and what are their qualifications and biases?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the evidence that Iran was \"weeks away\" from having enough nuclear material to form a bomb when Clinton became Secretary of State, and is this claim verifiable?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What evidence does Clinton have that Iran had built \"covert facilities\" and stocked them with centrifuges? Are these claims supported by credible sources?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"How does the fact that NATO nations are still fighting terrorism in Afghanistan alongside the US support Clinton's argument, and what is her point in mentioning this?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How do Clinton's statements about NATO and Iran relate to the broader topic of discussion, and what is her main point in bringing up these issues?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': \"What is the relevance of mentioning Article 5 of NATO in this context, and how does it support Clinton's argument?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"What is the relevance of Clinton's claims about NATO and Iran to the current political context, and how do they support her policy positions?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Are there any counterarguments or alternative perspectives that Clinton is not acknowledging or addressing in her statements about NATO and Iran?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the evidence for the claim that NATO has only invoked Article 5 once, after 9/11? Is this a verifiable fact?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What specific sanctions did the US impose on Iran during Clinton's tenure as secretary of state, and what was their impact on Iran's nuclear program?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"How did the Bush administration's policies contribute to Iran's mastery of the nuclear fuel cycle, and what specific actions did they take that allowed Iran to build covert facilities and stock them with centrifuges?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is it accurate to say that NATO only invoked Article 5 after 9/11, and are there any other instances where it was invoked?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What does Clinton mean by \"mastered the nuclear fuel cycle\"? Is this a technical term that can be verified, or is it a subjective judgment?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does Clinton's account of Iran's nuclear program square with the findings of the International Atomic Energy Agency (IAEA) or other independent organizations?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"What were the effects of the sanctions imposed on Iran during Clinton's tenure as Secretary of State, and did they have the desired outcome of preventing Iran from developing nuclear weapons?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton know that Iran was \"weeks away\" from having enough nuclear material to form a bomb when she became secretary of state? What is the source of this information?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific sanctions did Clinton vote for, and were they effective in achieving their intended goals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How did the deal \"put a lid\" on Iran\\'s nuclear program, and what mechanisms were put in place to ensure compliance?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"How did Clinton's efforts drive Iran to the negotiating table, and what were the key concessions that Iran made as a result?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Clinton\\'s role in the Iran nuclear deal demonstrate \"good judgment,\" and what specific decisions or actions does she point to as evidence of this?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What is the evidence that the deal was successful in preventing Iran from developing nuclear weapons, and how has it been verified?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Clinton\\'s claim that the deal \"put a lid on Iran\\'s nuclear program without firing a single shot\" align with the actual outcome of the deal, and are there any potential long-term risks or consequences associated with the agreement?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What was the nature of the coalition that Clinton put together, and how did it contribute to the imposition of sanctions on Iran?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific sanctions did Clinton vote for, and how effective were they in achieving their goals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"What were the specific terms of the deal negotiated by John Kerry and President Obama, and how did it address the concerns about Iran's nuclear program?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What were the key provisions of the deal negotiated by John Kerry and President Obama, and how did they address concerns about Iran's nuclear program?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What are the potential risks or drawbacks of the Iran nuclear deal, and how does Clinton respond to criticisms of the agreement?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"How did Clinton's coalition-building efforts with Russia and China contribute to the success of the Iran nuclear deal, and what were the terms of their involvement?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How does Clinton\\'s argument that her approach demonstrated \"good judgment\" hold up in light of potential criticisms or alternative perspectives on the deal and its outcomes?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What is the basis for Clinton\\'s assertion that her approach constitutes \"diplomacy,\" \"coalition-building,\" and \"working with other nations,\" and how does it differ from other approaches that might have been taken?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_223_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Clinton's approach to diplomacy and coalition-building in this instance compare to her approach in other international crises or conflicts?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton define \"temperament\" in this context, and what specific qualities does she believe are necessary for a commander-in-chief?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"How does Clinton's own temperament and behavior compare to the person she is criticizing? Does she have a track record of demonstrating the qualities she believes are necessary for a commander-in-chief?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"Is Clinton's criticism of the person's temperament based on a fair and objective assessment, or is it influenced by political bias or personal animosity?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is being able to withstand taunts a necessary or sufficient condition for being a good commander-in-chief?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Are there other factors that might influence a person's behavior in a given situation? Has Clinton taken these factors into account in her assessment of the person's temperament?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence is there to suggest that the person being referred to lacks the right temperament to be commander-in-chief?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How does Clinton's own behavior and temperament compare to the standard she is setting for others?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"Is Clinton's claim based on a subjective interpretation of the person's behavior, or is it supported by objective metrics or expert analysis?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"Are there other factors that are more important than temperament in determining a person's fitness to be commander-in-chief?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific behaviors or actions is Clinton referring to when she says that the person is \"taunted\"? Are these behaviors truly indicative of a flawed temperament, or are they being taken out of context?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific behavior or action is being referred to as \"being taunted\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What evidence does Clinton have to support her claim that the person's temperament is unsuitable for the role of commander-in-chief? Is this evidence based on a pattern of behavior, or is it an isolated incident?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does being taunted relate to the temperament required to be commander-in-chief?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is this criticism based on a specific incident or a pattern of behavior?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_225_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Is this a subjective judgment or is there an objective standard for evaluating a person's temperament for the role of commander-in-chief?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is it realistic to assume that terrorists would be able to acquire nuclear material if they wanted to, or are there sufficient safeguards in place to prevent this?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What steps has Clinton taken or proposed to prevent the proliferation of nuclear weapons, and how effective have they been?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What evidence does Clinton have to support the claim that Trump\\'s attitude about nuclear weapons is \"cavalier\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What specific actions or policies has Clinton\\'s opponent proposed or supported that demonstrate a \"cavalier\" attitude towards nuclear weapons?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How does Clinton\\'s own record on nuclear weapons and non-proliferation compare to that of her opponent? Has she proposed or supported policies that might be seen as \"cavalier\" or irresponsible?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton define \"cavalier\" in this context, and is it a fair characterization of Trump\\'s views?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How does Clinton plan to prevent terrorists from getting their hands on nuclear material, and what specific policies or actions has she proposed to address this threat?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is the threat of nuclear weapons truly the number-one threat the world faces, or is this an exaggeration or oversimplification?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there other global threats that might be equally or more pressing than nuclear weapons, and if so, how does Clinton propose to address them?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific evidence does Clinton have to support the claim that her opponent\\'s attitude about nuclear weapons is \"cavalier\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'How does Clinton propose to address the threat of nuclear weapons, and what are the potential consequences of her approach?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"What are the implications of Clinton's statement for US foreign policy and national security more broadly? Is she suggesting that the threat of nuclear terrorism should be the primary focus of US foreign policy, and if so, what would be the consequences of such an approach?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"What is the relationship between Clinton's opponent's attitude towards nuclear weapons and the threat of terrorists acquiring nuclear material? Is it a causal relationship, or is Clinton simply using the threat of terrorism to make her opponent's views seem more alarming?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'Is the threat of terrorists obtaining nuclear material a realistic concern, or is this a hypothetical scenario being used to score political points?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What specific policies or actions has Trump taken or proposed that suggest a lack of concern about nuclear weapons?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_231_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is the threat of nuclear weapons truly the number-one threat we face in the world, or is this an exaggeration? What are the other threats that Clinton considers, and how does she rank them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the problem being referred to? What is the context of the conversation?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the context of this conversation, and how does this statement fit into the larger discussion?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is Clinton agreeing with a previous statement or question, or is this a standalone statement?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How does this statement relate to the topic or issue being discussed?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the problem being referred to, and how does \"it\" describe it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What is \"it\" referring to? What is being described as \"a good one\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the basis for Clinton\\'s judgment that the description is \"good\"? Is it based on personal experience, expert opinion, or some other evidence?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What is the \"it\" that Clinton is referring to?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"Is Clinton's response a thoughtful and considered evaluation, or is it a superficial or dismissive comment?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is Clinton providing evidence or support for their claim, or is this simply an assertion?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"Is there any potential bias or conflict of interest that might affect Clinton's judgment or response?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the tone of the conversation? Is it formal and serious, or informal and casual?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"Are there any power dynamics at play in the conversation that might influence Clinton's response?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_235_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the criteria for evaluating whether a description of a problem is \"good\"? Is it based on accuracy, completeness, or some other factor?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What is Clinton's opponent's alternative plan to address the issues with Iran, and how does it differ from Clinton's approach?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Clinton\\'s emphasis on \"American interests and security\" balance with her commitment to promoting \"peace and prosperity\" globally?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Clinton's leadership style and decision-making process ensure that the US leads the world with strength and in accordance with its values?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What evidence does Clinton have to support her claim that her opponent has no plan to defeat ISIS, and is her own plan more effective?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What concrete actions does Clinton propose to take to \"stand up to bullies\" abroad and at home, and how would these actions differ from current policies?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What specific values does Clinton intend to uphold in her foreign policy decisions, and how would these values be prioritized in situations where they conflict with other interests?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How does Clinton define a \"bully,\" and what specific actions would she take to stand up to bullies abroad and at home?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What are the specific American interests and security concerns that Clinton believes need to be protected, and how does she plan to address them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does Clinton\\'s call for \"precise\" language in discussing foreign policy issues square with her own use of vague phrases like \"lead the world with strength and in accordance with our values\"?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What specific decisions has Clinton made in the past that demonstrate her ability to promote peace and prosperity, and how do those decisions relate to her current policy proposals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What are the specific benefits of the Iranian deal that Clinton claims have been successful, and are those benefits measurable?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What specific details of Clinton's own plan to defeat ISIS can she provide to counter her opponent's alleged secrecy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"How does Clinton's criticism of her opponent's lack of a plan to defeat ISIS address the substance of his criticism of the Iran deal?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Clinton define \"precise\" language in discussing foreign policy issues, and is her own language always precise and clear?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How would Clinton's leadership style and decision-making process differ from those of her opponent, and what evidence is there to support her claim that she would be a more reliable and effective leader?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific evidence is there to support the claim that the deal with Iran has been \"very successful\" in giving the US access to Iranian facilities?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"How does Clinton's approach to foreign policy balance the need for strength and leadership with the need for cooperation and diplomacy with other nations?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_244_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': \"What specific aspects of the Iranian deal does Clinton's opponent criticize, and are those criticisms valid?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What are the potential drawbacks or unintended consequences of the proposed initiatives, and how would they be mitigated?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What are the potential risks and challenges associated with these initiatives, and how will they be addressed?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How will the clean energy generated by these solar panels be distributed and accessed by every home, and what infrastructure changes are needed to make this possible?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the feasibility of deploying half a billion more solar panels, and what would be the cost and timeline for such an undertaking?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What are the specific policies and regulations that will be put in place to prevent a return to the economic practices that led to trouble in the first place?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How will the economic activity generated by these initiatives be measured and tracked, and what are the projected economic benefits?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'How will the impact of these initiatives on different regions and communities be taken into account, and what measures will be taken to ensure that all areas benefit equally?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What kind of jobs will be created by these initiatives, and how many jobs will be lost in industries that are negatively impacted by the shift to clean energy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the definition of \"progress\" in the context of the last eight years, and how would the proposed initiatives build upon it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What is the cost of deploying half a billion more solar panels, and how will it be funded?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'How would the proposed initiatives be coordinated and implemented at the international level?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How would the proposed clean energy infrastructure be funded, and what would be the impact on taxpayers and consumers?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How will the progress made over the last eight years be built upon, and what specific actions will be taken to ensure continued economic growth?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the evidence that building a new modern electric grid would lead to a significant number of new jobs and economic activity?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What specific policies and regulations would be needed to support the proposed clean energy initiatives, and how would they be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What are the specific plans for building a new modern electric grid, and how will it be integrated with existing infrastructure?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_10_L': {'cq': 'What is the timeline for implementing these initiatives, and what are the key milestones and benchmarks for success?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_25_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'How would the proposed initiatives address the root causes of economic troubles, and what guarantees are there that they would not lead to similar problems in the future?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What was the context for the 30% and 50% increases in American exports during Clinton's tenure as Secretary of State, and how much of that growth can be attributed to her policies versus other factors?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How will Clinton's approach to trade deals balance the interests of different stakeholders, such as workers, businesses, and consumers?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What specific actions will Clinton take to support American industries and workers who may be negatively impacted by trade deals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton plan to \"enforce\" trade deals and \"hold people accountable\" through a special prosecutor, and what specific mechanisms will be put in place?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton plan to enforce trade deals and hold people accountable, and what mechanisms will she put in place to ensure compliance?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Clinton's opposition to CAFTA inform her broader approach to trade policy, and what lessons does she take from that experience?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How does Clinton's experience as Secretary of State, where she increased American exports globally and to China, translate to her ability to create new jobs and increase exports as President?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How do Clinton's past actions on trade deals, including her vote in favor of some of them, align with her current rhetoric and promises?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What is the evidence that Clinton\\'s policies will lead to the creation of \"new jobs\" and \"more new jobs\", and how will their effectiveness be measured?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the role of the special prosecutor in enforcing trade deals, and how will they be empowered to take action?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How will Clinton balance the interests of different stakeholders, including businesses, workers, and environmental groups, in her trade policy decisions?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What other challenges does Clinton identify as being part of the economy, and how does she plan to address them in addition to trade?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the specific standards that Clinton holds for evaluating trade deals, and how do they inform her decisions?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What specific steps will Clinton take to address the broader economic challenges she acknowledges, beyond just enforcing trade deals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_37_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific provisions of CAFTA did Clinton oppose, and how did they align with her \"standards\" for trade deals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"What specific aspects of being a senator and secretary of state are most relevant to being a successful president, and how does Clinton's experience in these areas translate to the presidency?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How does Clinton's argument about her experience address the concerns of voters who may be looking for a change in Washington or who are dissatisfied with the political establishment?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"How do Clinton's experiences as a senator and secretary of state prepare her to make the kinds of decisions required of a president?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What are the criteria by which Clinton is judging Trump's lack of experience, and are those criteria fair and relevant to the presidency?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"What alternative forms of experience or qualifications might be relevant to the presidency, and how does Clinton's argument account for these?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How does Clinton's experience compare to Trump's experience, and are there any ways in which Trump's experience could be seen as relevant to the presidency??\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"Are there any potential drawbacks or limitations to Clinton's experience as a senator and secretary of state that might affect her ability to be an effective president?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"Is Clinton's argument based on a assumption that experience in government is the only relevant kind of experience for a president, and is that assumption justified?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Are there any counterexamples or exceptions that could challenge Clinton's claim that she has more experience and qualifications than Trump?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What specific accomplishments can Clinton point to as a senator and secretary of state that demonstrate her qualifications for the presidency?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': \"Clinton is making an argument about her qualifications and experience compared to Trump's. What critical questions should be asked about this argument?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"How does Clinton's experience in these roles prepare her to make the kinds of decisions a president would have to make, and are there any gaps in her experience that could be a problem?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"How does Clinton's experience compare to Trump's in terms of leadership and management skills, and what evidence is there that her experience is superior?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_39_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific accomplishments can Clinton point to during her time as senator and secretary of state that demonstrate her ability to be a effective president?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What is the definition of \"rising incomes\" and how will it be measured?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': \"What are the different views about what's good for our country, our economy, and our leadership in the world?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific views is Clinton referring to, and how do they differ from her own views?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'How will the success of these economic solutions be evaluated, and what metrics will be used to measure their effectiveness?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"What is the basis for Clinton's assertion that tax cuts would add $5 trillion to the debt, and are there any alternative estimates or analyses that contradict this claim?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the basis for the claim that tax cuts would add $5 trillion to the debt?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"What is the timeline for Clinton's proposed policies, and how would she measure their success in the short-term and long-term?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How does Clinton's proposal address the needs of different groups, such as low-income workers, small business owners, and large corporations?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'How does Clinton define \"new jobs with rising incomes\", and what metrics would she use to measure the success of such a policy?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What evidence does Clinton have to support her claim that the economy needs to be \"gotten going again\", and what specific policies does she propose to achieve this?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What kind of investments are being referred to, and how will they benefit the economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How will creating new jobs with rising incomes help the economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What is the timeline for implementing these economic solutions, and what are the expected outcomes?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What are the specific actions needed to get the economy going again?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How will the proposed investments be funded, and what will be the impact on the national debt?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does Clinton's proposal for investments align with her goal of reducing the debt, and are there any potential trade-offs or conflicts between these two goals?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is Clinton's plan for investments, and how would she propose to fund them?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_55_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Are there any alternative solutions to creating new jobs and stimulating the economy besides investments and not tax cuts?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What are the sources of the claims that Clinton's plan would create 10 million jobs and Trump's plan would lose 3.5 million jobs, and how reliable are these sources?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"What are the specific details of Clinton's plan, and how does it address the complexities of the economy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific policies does Clinton\\'s book \"Stronger Together\" propose to achieve \"strong growth, fair growth, sustained growth\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': \"Has Clinton's book been independently reviewed and verified to ensure that it contains a credible and well-thought-out plan for the economy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"What are the criteria used to evaluate the success of Clinton's plan, and how would its effectiveness be measured?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"What is the timeline for implementing Clinton's plan, and how would it be funded?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Who are the \"people\" that have looked at both plans and concluded that Clinton\\'s plan is superior, and what are their qualifications and biases?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"How does Clinton's plan help families balance responsibilities at home and at business, and what are the specific measures proposed to achieve this?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Is Clinton\\'s claim that her opponent\\'s plan would \"explode the debt\" and lead to a recession supported by credible evidence and analysis, or is it a rhetorical flourish?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"How does Clinton's plan take into account the potential impact of external factors, such as global economic trends or unforeseen events, on its success?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How does Clinton's plan address potential challenges and obstacles that may arise during its implementation, and what contingency measures are proposed?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"What opportunities are there for bipartisan cooperation and compromise in implementing Clinton's plan, and how would she work with opposing parties to achieve its goals?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What are the specifics of Trump\\'s plan that would allegedly \"explode the debt\" and lead to a recession, and how does Clinton\\'s plan address these issues?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does Clinton's plan address potential criticisms or weaknesses, and what are the potential risks and downsides of implementing it?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Clinton\\'s plan ensure that the growth it proposes is \"fair\" and benefits all segments of society, rather than just a privileged few?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What are the assumptions and methodologies used to estimate the job creation and debt impact of each plan, and are they credible and transparent?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_57_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Are there any independent, non-partisan analyses or evaluations of the two plans that could provide a more objective assessment of their merits?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Are these regulations truly burdensome, or do they serve important purposes?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"How would Clinton's plan address these underlying issues, and are there alternative solutions that might be more effective?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'How would the proposed policies be implemented, and what would be the timeline for their implementation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'Is this claim supported by independent analysis or experts?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the definition of \"fair share\" in terms of taxes, and how would it be determined?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific regulations would be cut and streamlined, and how would this affect small businesses?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What are the underlying assumptions and methodologies used to estimate the debt impact of each plan?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What are the alternatives to the proposed policies, and how would they compare in terms of effectiveness and impact on the economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How would the proposed changes affect different types of small businesses, and are there potential unintended consequences?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the evidence that the wealthy and corporations have made all the gains in the economy, and how would this be measured?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Would the tax increases be progressive, and would they target only the very wealthy or also affect upper-middle-class individuals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'Are there other factors contributing to income inequality?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'How would the proposed policies be funded, and what would be the impact on the federal budget?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How would the proposed tax increases on the wealthy be structured, and what would be the impact on the economy as a whole?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Are there any potential flaws or biases in the calculation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_11_L': {'cq': \"How would Clinton's plan balance the need for revenue with the potential impact on economic growth and competitiveness?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_10_L': {'cq': 'Is the concept of \"fair share\" subjective, or are there objective criteria for determining it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"Are the opponents' plans as flawed as Clinton claims, or are there alternative perspectives?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How would the tax changes affect economic growth, investment, and job creation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_66_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What are the potential unintended consequences of the proposed policies, and how would they be addressed?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What are the specific \"investments\" that Clinton proposes to make to produce jobs and rising incomes, and how would they be funded?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the basis for the claim that Trump\\'s plan would provide the \"biggest tax cuts for the top percent of the people in this country than we\\'ve ever had\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"What is the evidence for the claim that the proposed tax plan would primarily benefit the top 1% of the population, and how would Clinton's alternative policies affect income inequality?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'How does Clinton define \"smart, fair trade deals\" and what specific policies would she propose to achieve them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': \"How does Clinton's view of economic growth differ from Trump's, and what are the underlying assumptions and values that drive their respective approaches?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"What is the basis for Clinton's assertion that the proposed tax plan would not grow the economy, and what alternative policies does she propose?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What are the \"different perspectives\" that Clinton and Trump bring to the issue of economic growth, and how do these perspectives shape their policy proposals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"How would Clinton's alternative plan for growing the economy actually produce jobs and rising incomes?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What specific provisions in Trump\\'s plan would constitute \"trickle-down economics\" and how would they harm the economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"How does Clinton's personal attack on Trump's fortunate life experiences relate to the argument about economic policy, and is it a relevant or distracting point?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What is the evidence for the claim that the proposed tax plan would be the most extreme version of trickle-down economics?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"What is the relevance of Trump's personal fortune to the discussion of economic policy, and how does it relate to Clinton's argument about growing the economy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Clinton's argument about the need for a tax system that rewards work and not just financial transactions relate to her broader economic policy vision?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How would Clinton\\'s proposed tax system \"reward work and not just financial transactions\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What evidence is there that the US needs to have \"smart, fair trade deals\" in order to grow the economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"Is it fair to assume that Trump's business success is solely due to the initial loan from his father, or did he also contribute his own efforts and skills to the business?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What specific policies does Clinton propose to \"invest in\" the middle class, and how will these policies lead to economic growth and prosperity?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How does Clinton's proposed economic approach differ from Trump's, and what are the potential consequences of each approach? Are there any potential drawbacks or trade-offs to Clinton's approach that should be considered?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What is the basis for Clinton's assertion that helping the middle class will lead to better economic growth? Is this supported by empirical evidence or economic theory?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific policies does Clinton propose to \"do more\" for the middle class, and how will these policies achieve the desired economic outcomes?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the source of the information that Trump borrowed $14 million from his father? Is this a verified fact or an unproven claim?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"How does Clinton's economic vision differ from Trump's, and what are the implications of each approach for different segments of society?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Is Clinton's characterization of Trump's economic views a fair and accurate representation, or is it a caricature or oversimplification?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What evidence does Clinton have to support the claim that helping the wealthy does not benefit the economy as a whole? Is this a proven economic theory or a personal opinion?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Clinton's focus on investing in education, skills, and the future of individuals contribute to economic growth, and what is the evidence for this claim?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How does Clinton's personal experience with her father's small business relate to the broader economic policies being discussed? Is this anecdote relevant to the argument or simply a emotional appeal?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Clinton define the \"middle class,\" and are her policies targeted towards a specific segment of the population or the middle class as a whole?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"What is the basis for Clinton's claim that investing in the middle class will lead to economic growth, and is this a widely accepted economic theory or a contested idea?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"What are the potential drawbacks or limitations of Clinton's approach, and how might critics respond to her argument?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How does Clinton's personal experience as the child of a small-businessman inform her economic policies? Is this anecdote relevant to the broader economic discussion?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"Does Clinton provide evidence to support the claim that Trump believes that helping wealthy people will benefit everyone? Is this a fair representation of Trump's views?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the definition of the \"middle class\" in this context, and how does Clinton\\'s policy approach address the needs of this group?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_6_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'Is it accurate that Trump started his business with $14 million borrowed from his father? What is the source of this information?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence does Clinton have that the proposed tax rates or policies would not lead to repatriation of money stranded overseas?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is Clinton's motivation for supporting these proposals, and are there any potential conflicts of interest or biases at play?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the current state of corporate tax rates, and how do they compare to other countries?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Clinton's support for repatriation of money stranded overseas align with her broader economic policies and goals?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How would the implementation of these proposals affect different stakeholders, such as corporations, individuals, and the overall economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the current status of the money stranded overseas, and how would changing tax rates or implementing these proposals actually bring it back to the US?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"How would Clinton's proposed policies address the issue of tax avoidance or evasion by corporations, and what measures would be taken to prevent abuse?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the definition of \"stranded overseas\" and how does Clinton determine which money falls into that category?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What specific tax proposals is Clinton referring to, and what are the details of those proposals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Are there any potential drawbacks or unintended consequences to changing corporate tax rates or implementing policies to encourage repatriation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the potential consequences or drawbacks of implementing these proposals, and have they been fully considered?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any alternative solutions or approaches that could achieve the same goal of repatriating money stranded overseas, and have they been considered?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What evidence is there to support the claim that changing corporate tax rates or implementing certain proposals would lead to the repatriation of money stranded overseas?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific tax proposals is Clinton referring to, and what are the details of these proposals?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_85_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What are the potential consequences of not changing corporate tax rates, and how would that impact the economy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"Are there any other factors or motivations that could be driving Clinton's criticism of Trump's proposal, beyond a genuine concern for the country's well-being?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"Are there any independent experts or organizations that have analyzed Trump's proposal and reached a different conclusion than Clinton?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the specific proposal that Trump has made, and how would it benefit him personally?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What exactly does Clinton mean by \"in a way that will actually work to our benefit\"? How does her proposal differ from Trump\\'s, and what specific benefits does she claim it will bring?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'Has Clinton provided a thorough and balanced analysis of the proposal, or is she selectively presenting information to support her claim?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What evidence does Clinton have to support her claim that Trump's proposal would benefit him personally, and is that evidence credible?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What evidence does Clinton have to support her claim that Trump's proposal would advantage him and his business? Is this a personal attack or a legitimate criticism of his policy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Is Clinton\\'s characterization of the proposal as a \"loophole\" accurate, or is it a biased interpretation?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How does Clinton's proposal address the issues that she claims are problematic in Trump's proposal? What specific measures does she propose to prevent the kinds of advantages she accuses Trump of seeking?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"What are the underlying assumptions and values that guide Clinton's argument? Is she assuming that Trump is acting out of self-interest, or is she making a more nuanced critique of his policy? How does her argument reflect her own values and priorities?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What is the \"Trump loophole\" that Clinton is referring to? How does it specifically advantage Trump and his business, and what are the implications of this loophole?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_87_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"Is Clinton's own proposal truly in the best interest of the country, or does it also have potential benefits for her or her supporters?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What is the empirical evidence for the claim that \"broad-based, inclusive growth\" is more effective than trickle-down economics in stimulating economic growth and reducing inequality?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the mechanism by which Clinton believes that increasing taxes on the wealthy will lead to more contributions to rebuild the middle class, and is there any evidence to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How does Clinton respond to potential counterarguments that increasing taxes on the wealthy could lead to decreased economic growth, job losses, and capital flight?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How would Clinton's proposed policies address the root causes of income inequality, rather than just its symptoms?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the definition of \"the very top\" in terms of income or wealth, and how does Clinton propose to determine who should be subject to increased taxes and benefits?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Clinton's proposal for debt refinancing at a lower rate address the underlying causes of high college debt, and is it a sustainable solution in the long term?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the evidence that trickle-down economics led to the economic crisis of 2008 and 2009? Is there a clear causal link between the two?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"How does Clinton's argument account for the potential impact of global economic trends, technological changes, and demographic shifts on the US economy, and how does she propose to address these broader challenges?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton define \"really smart, wealthy people\" and what specific examples or quotes can she provide to support the claim that they agree with her assessment of trickle-down economics?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the definition of \"middle class\" in this context, and how does Clinton plan to rebuild it? What are the specific policies she would implement to achieve this goal?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': \"How would Clinton's policies be funded, and what are the potential trade-offs with other government priorities, such as defense spending or social security reform?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How does Clinton's approach differ from previous attempts to stimulate economic growth and reduce inequality, and what makes her approach more likely to succeed?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"What is the relationship between Clinton's proposed policies and the existing social safety net, and how does she propose to ensure that the benefits of her policies are targeted towards those who need them most?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': \"What is the role of individual initiative and entrepreneurship in Clinton's vision of economic growth, and how would her policies support or hinder these factors?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How would making college debt-free and refinancing college debt at a lower rate boost the economy, and what are the potential costs and trade-offs of such policies?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How does Clinton define \"broad-based, inclusive growth\" and what specific policies does she propose to achieve it, beyond just increasing taxes on the wealthy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"What are the potential unintended consequences of Clinton's policies, such as increased taxes on the wealthy or increased government spending on education and debt relief?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the empirical evidence that investing in the middle class and making college debt-free will boost the economy, and how does Clinton propose to fund these initiatives?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Who are the \"really smart, wealthy people\" who agree with Clinton\\'s assessment, and what are their credentials? Are they representative of the wealthy class as a whole?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_92_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific evidence does Clinton have to support the claim that trickle-down economics did not work and led to the economic mess in 2008 and 2009?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What does it mean to \"suppress\" moral hazard, and how is this done in practice?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"Are there any exceptions or counterexamples to the argument that moral hazard is a natural part of lending and borrowing, and if so, how do they affect the speaker's conclusion?\",\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the definition of moral hazard and how does it apply to lending and borrowing?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How does the concept of moral hazard relate to other economic concepts, such as risk management or asymmetric information?',\n",
              "  'label': 'Invalid'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What evidence is there to support the claim that suppressing moral hazard leads to the creation of \"fool\\'s gold\" rather than real wealth?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does ignoring moral hazard lead to suffering for both sides of the transaction?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Is moral hazard always a natural part of the process of lending and borrowing, or are there circumstances under which it can be mitigated or avoided?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does the speaker define \"real wealth\" and how does it differ from \"fool\\'s gold\"?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What evidence or data supports the claim that ignoring moral hazard leads to suffering for both sides of the transaction?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How can one distinguish between \"real wealth\" and \"fool\\'s gold\" in the context of lending and borrowing?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How does the speaker's argument account for the role of regulation and oversight in mitigating moral hazard in lending and borrowing?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What are the potential consequences of ignoring moral hazard, and are they universally applicable or dependent on specific circumstances?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"Is the speaker's argument based on theoretical assumptions or empirical evidence, and if so, what is the nature of that evidence?\",\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the relationship between moral hazard and the generation of \"fool\\'s gold\" (i.e., artificial or illusory wealth)?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Are there any exceptions or circumstances under which ignoring moral hazard might not lead to negative consequences?',\n",
              "  'label': 'Useful'},\n",
              " 'CL_57_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is meant by \"moral hazard\" in the context of lending and borrowing?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there any potential unintended consequences of a full ban on peanut products on airplanes?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': 'Are there any other factors that should be considered in the decision-making process, such as the impact on the airline industry, passenger convenience, or food options for passengers with other dietary restrictions?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the scientific evidence for the claim that peanut reactions can be life-threatening through contact or inhalation?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'What is the evidence that a ban on peanut products on airlines would actually save lives, and are there any other measures that could be taken to reduce the risk of allergic reactions on flights?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How representative are the 3000 members of the online support forum of the broader allergic community, and do they reflect a diverse range of perspectives and experiences?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Are peanuts a necessary choice for airlines, or are they simply a convenient and popular snack? Are there alternative snacks that could be offered instead?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the expertise of the author, beyond being a physician and author on the subject, in terms of allergy research and policy-making?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How would a ban on peanut products on airlines be balanced against the rights and preferences of passengers who do not have peanut allergies?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'How would a full ban on peanut products be enforced and monitored, and what would be the consequences for non-compliance?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'How representative are the 3000 members of the online support forums of the broader allergic community, and do they reflect a diverse range of perspectives and experiences?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How common are life-threatening reactions to peanuts, and what is the likelihood of such reactions occurring on an airplane?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the evidence that restricting peanut products to certain flights is not enough, and that residue can be rampant on all flights?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What specific measures would the author propose to implement a full ban of peanut products on airlines, and how would these measures be enforced?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is there any data to support the claim that restricting peanut products to certain flights is not enough, and that residue can be rampant?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Have buffer zones been implemented on any airlines, and if so, what were the results? Is there any data to support the claim that buffer zones do not work?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Are there any alternative solutions that could be implemented to minimize the risk of peanut allergies on airplanes, rather than a full ban?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What is the expertise of the author on the subject of food allergies, and what are their qualifications to make recommendations on airline policy?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the basis for the assertion that providing buffer zones is not a practical solution?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How common are contact or inhalation reactions to peanuts, and what is the scientific evidence to support the claim that these reactions can be life-threatening?',\n",
              "  'label': 'Useful'},\n",
              " 'Doctor-Mom_205_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the prevalence of life-threatening peanut reactions on airlines, and how many fatalities have occurred due to peanut exposure on flights?',\n",
              "  'label': 'Useful'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How do these points relate to the current conversation or topic?',\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is the author's motivation for responding to Elmattador's statement?** Are they genuinely trying to understand Elmattador's perspective, or are they trying to discredit or persuade them?\",\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is the assumption being made that Elmattador did not watch the debates, or is it being implied that they should have watched them?',\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What evidence or supporting information is being provided to back up the claim?',\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the relevance of the GOP debates to the argument being made?',\n",
              "  'label': 'Useful'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific information or points from the GOP debates are being referenced?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the purpose of bringing up the GOP debates in this context?',\n",
              "  'label': 'Useful'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Is this a personal attack or an ad hominem argument, rather than a substantive response to Elmattador's points?\",\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is this a valid or relevant counterargument, or is it a distraction or red herring?',\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the author\\'s definition of \"informative\"?** Does it align with Elmattador\\'s understanding of the term? Are they using the same criteria to evaluate the debates\\' informativeness?',\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What specific insights or information did the debates provide that Elmattador might have missed?** The author makes a general statement about the debates being informative, but what specific examples can they provide to support this claim?',\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"Is the author's argument an ad hominem attack?** Are they attacking Elmattador's character or attention span rather than addressing the substance of their original statement?\",\n",
              "  'label': 'Invalid'},\n",
              " 'Elmattador__92_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"How does the author know that Elmattador wasn't paying attention?** Is the author making an assumption about Elmattador's engagement with the debates, or do they have evidence to support this claim?\",\n",
              "  'label': 'Invalid'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What is the current state of research into peanut allergy treatment and prevention, and are there any promising developments that could improve the lives of individuals with peanut allergies?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Are there any accommodations that airlines could make to reduce the risk of an allergic reaction?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'Which critical questions should be raised before accepting the arguments in this text?',\n",
              "  'label': 'Invalid'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Are there any alternative ways to manage peanut allergies while flying, such as using a different type of medication or taking precautions to minimize exposure?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What are the protocols in place for handling allergic reactions on airplanes, and are they sufficient to address the concerns raised by FoodAllergyMom?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'Is the author\\'s emotional appeal to the reader (e.g. \"it is heartbreaking to think that our child will not get to experience the world\") effective in making their argument, or is it overly sentimental and lacking in substance?',\n",
              "  'label': 'Invalid'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does the risk of an allergic reaction on an airplane compare to the risk of an allergic reaction in other environments, such as schools or restaurants?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'Is the author\\'s characterization of a peanut allergy as a \"disability\" accurate, and what implications does this have for how we think about and accommodate people with food allergies?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How does the severity of peanut allergy impact daily life, and are there any accommodations or modifications that can be made to minimize the risks associated with the allergy?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there any potential solutions to the problem of peanut allergies on planes that the author has not considered?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'Are there any studies or data that support the claim that peanut particles can be circulated through an airplane cabin and pose a significant risk to individuals with peanut allergies?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How common is it for children with peanut allergies to react to airborne particles, and what is the severity of such reactions?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'How common are severe peanut allergies, and are they truly a significant public health concern?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the scientific basis for the claim that airborne particles from someone opening a bag of nuts can trigger a severe allergic reaction in a child with a peanut allergy?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is the risk of an allergic reaction on a plane truly as high as the author suggests, or are there statistics that suggest otherwise?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"Is the author's account of their child's allergy supported by medical evidence?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What are the alternative options for individuals with peanut allergies who need to travel, and are they feasible and accessible?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there any policy or regulatory changes that could be implemented to better support individuals with peanut allergies and improve their access to safe and inclusive environments?',\n",
              "  'label': 'Useful'},\n",
              " 'FoodAllergyMom_199_1_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How does the emotional and psychological impact of living with a peanut allergy affect individuals and their families, and what support systems are in place to address these concerns?',\n",
              "  'label': 'Invalid'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is the author\\'s emphasis on individual responsibility (e.g., carrying \"rescue\" medication, wearing protective clothing) sufficient to mitigate the risks associated with peanut allergies on flights, or does it overlook the role of collective responsibility and regulation in ensuring public safety?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is the author\\'s emphasis on individual responsibility (e.g., carrying \"rescue\" medication, wearing protective clothing) sufficient to mitigate the risks associated with peanut allergies on flights?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"Does the author's personal experience with a medical condition provide a valid analogy for peanut allergies, or are there significant differences between the two that affect the argument?\",\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Are there other measures that could be taken to reduce the risk of peanut exposure on flights?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Are there any underlying biases or assumptions in the author's argument that may influence their perspective on peanut allergies and air travel?\",\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is the risk of an allergic reaction to peanuts on a flight significant enough to warrant a ban, or can it be mitigated by other means?',\n",
              "  'label': 'Invalid'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How often do peanut-allergic passengers actually fly, and is it reasonable to assume that they could easily choose alternative travel methods?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the critical questions that should be asked regarding the arguments in the above paragraph?',\n",
              "  'label': 'Invalid'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is the comparison between the \"right\" to eat peanuts and the \"right\" to fly a fair one, or are these two rights not equivalent?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How would a ban on peanuts on flights be enforced, and what would be the consequences for passengers who fail to comply?',\n",
              "  'label': 'Invalid'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'Is the author\\'s suggestion of \"seating blocks\" for allergy sufferers and willing non-sufferers a feasible and practical solution, considering the logistics of flight operations and passenger management?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is it reasonable to expect peanut-allergic passengers to take on the burden of minimizing their own risk by wearing face masks and carrying rescue medication, or should the airline industry take more responsibility for ensuring their safety?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Would a ban on peanuts on flights be a disproportionate response to the risk posed by peanut allergies, or would it be a necessary step to ensure the safety of all passengers?',\n",
              "  'label': 'Invalid'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Does the author\\'s framing of the issue as a conflict between the \"right\" to eat peanuts and the \"right\" to fly accurately capture the ethical and moral dimensions of the debate, or does it oversimplify the complexities involved?',\n",
              "  'label': 'Useful'},\n",
              " 'Frequent-Flyer_157_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'Would voluntary seating blocks for peanut-allergic passengers be effective in reducing the risk of exposure, or would they be difficult to implement and enforce?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': \"What is the source of the claim that the DOJ determined an assault weapons ban won't be effective in addressing homicide rates? Is it a credible source, and what was the methodology used to reach this conclusion?\",\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the definition of \"common sense gun solutions\" and how does the author know that nobody proposes them?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How does the author know that an assault weapons ban \"won\\'t do shit\" for addressing homicide rates in the US, and what evidence do they have to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does the author define \"our problems\" and what evidence is there to support the claim that an assault weapons ban would not address them?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is it accurate to say that nobody proposes common sense gun solutions, or are there specific proposals being made that are being ignored or dismissed?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What are the \"problems\" that the author is referring to, and how does an assault weapons ban fail to address them?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'How does the author\\'s tone and language (e.g. \"Glblwrmingisfak\", \"won\\'t do shit\") affect the credibility and persuasiveness of their argument?',\n",
              "  'label': 'Invalid'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What is the specific study or report from the DOJ that the author is referring to, and what were its exact findings regarding the effectiveness of an assault weapons ban?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the basis for the author\\'s assertion that Bernie Sanders \"should know\" that an assault weapons ban is not a solution, and what expertise or knowledge does the author attribute to Sanders on this issue?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What specific \"common sense gun solutions\" are being referred to, and how do they differ from the proposed assault weapons ban?',\n",
              "  'label': 'Useful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is it fair to assume that Bernie Sanders, a politician, is not stupid simply because he proposes a solution that the author disagrees with? Shouldn't the focus be on evaluating the merits of the proposal rather than making personal attacks?\",\n",
              "  'label': 'Invalid'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there any other factors that contribute to homicide rates in the US, and how do they interact with the availability of certain types of firearms?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How does the author\\'s statement that Sanders \"clearly isn\\'t stupid\" relate to the argument being made, and what relevance does Sanders\\' intelligence have to the effectiveness of an assault weapons ban?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Glblwrmingisfak__552_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What alternative solutions does the author propose to address homicide rates in the US, and what evidence is there to support their effectiveness?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What were the consequences of stopping the practice, and were there any unintended effects on public safety?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'For example, are there underlying socioeconomic or demographic factors that could have influenced the implementation of the policy?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Are there statistics or data to back up this assertion?',\n",
              "  'label': 'Invalid'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'What are the implications of the alleged unconstitutionality of stop-and-frisk, and how does it relate to the broader issue of racial profiling in law enforcement?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence is there to support the claim that stop-and-frisk \"largely singled out black and Hispanic young men\"?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What role did community input and engagement play in the development and implementation of the stop-and-frisk policy?',\n",
              "  'label': 'Invalid'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Were there any other factors that contributed to the demographics of those stopped?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Are there any other factors that could have contributed to the disproportionate impact of stop-and-frisk on black and Hispanic young men?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What alternative solutions or strategies were proposed or implemented to address the issues that led to the use of stop-and-frisk in the first place?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is there a causal link between the implementation of stop-and-frisk and the alleged targeting of black and Hispanic young men?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is there a specific court ruling or legal precedent that supports this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the basis for the claim that stop-and-frisk was ruled unconstitutional in New York?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What is the evidence that stop-and-frisk \"largely singled out\" black and Hispanic young men?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any other cities or jurisdictions where stop-and-frisk has been implemented, and if so, what were the outcomes?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does the author define \"largely singled out\" and what criteria were used to determine this?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the definition of \"unconstitutional\" in this context, and who made the ruling?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_10_L': {'cq': 'Are there any proposed solutions or alternatives to address the concerns raised by the author?',\n",
              "  'label': 'Invalid'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Was the policy intentionally designed to target these groups, or was it an unintended consequence?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_126_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is it possible that the demographics of those stopped reflect the demographics of the communities most affected by crime, rather than racial bias?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is Mr. Trump's stance on nuclear weapons and national security, and how does it align with his overall political platform?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How would Mr. Trump's stance on this issue affect his presidency and decision-making on national security matters?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What are the potential consequences of not changing the current policy on first use of nuclear weapons?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What are the potential consequences of changing the policy on first use of nuclear weapons, and how would it impact national security?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the current policy on first use of nuclear weapons, and what are the implications of changing it?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How will Mr. Trump's response be evaluated, and what criteria will be used to assess its validity and effectiveness?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the current policy on first use of nuclear weapons in the United States?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"What is President Obama's reported consideration of changing the policy based on, and what are the underlying reasons for this consideration?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What are the implications of changing or maintaining the current policy on first use of nuclear weapons?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What is the context of this discussion on nuclear weapons, and how does it relate to the broader topic of securing America?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"Are there any potential biases or assumptions embedded in HOLT's question, and how might they influence Mr. Trump's response?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does HOLT's question relate to the broader topic of securing America, and what other factors should be considered in this discussion?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What is the context of the debate and what are the stakes? Is this a primary debate or a general election debate?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'Who is HOLT and what is his role in this conversation? Is he a neutral moderator or does he have a vested interest in the outcome of the debate?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What are the potential risks and benefits of adopting a no-first-use policy on nuclear weapons?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_239_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How would a change in the policy on first use of nuclear weapons affect national security and international relations?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"Would the release of Trump's tax returns necessarily provide a complete picture of his financial situation and potential conflicts of interest, or are there other sources of information that might be more relevant?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"Is Holt's argument based on a assumption that Trump's tax returns would reveal something significant, and is this assumption justified?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'How would the release of tax returns be verified and authenticated to ensure that the information is accurate and trustworthy?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'Would releasing tax returns set a precedent for future nominees, and could this lead to unintended consequences?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"Are there any legitimate reasons why Trump might not want to release his tax returns, and have these reasons been adequately addressed by Holt's argument?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Are tax returns the only way to determine potential conflicts of interest, or are there other means of disclosing such information?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"Is it fair to assume that a presidential nominee's tax returns would be a comprehensive reflection of their financial situation, or could there be other factors at play that are not reflected in the returns?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"What specific information about potential business conflicts or debts would Trump's tax returns reveal that is not already publicly known?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"Is the burden of paying taxes a relevant consideration in this context, and how does it relate to the issue of Trump's tax returns?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Holt's argument account for the possibility that Trump's tax returns might not reveal any significant conflicts of interest or debts?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"How would the release of Trump's tax returns mitigate the risk of conflicts of interest, and are there other ways to address this concern?\",\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'How would releasing tax returns guarantee that voters would be aware of all potential conflicts of interest, considering that tax returns may not reveal all relevant information?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'Is it a long-standing tradition for presidential nominees to release their tax returns, and is this tradition based on a legitimate concern for transparency?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any legal or ethical issues that could arise from releasing tax returns?',\n",
              "  'label': 'Useful'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What specific conflicts of interest is the moderator implying that the nominee might have, and are these concerns based on credible evidence or speculation?',\n",
              "  'label': 'Invalid'},\n",
              " 'HOLT_94_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'Is the tradition of releasing tax returns a universally accepted practice, or is it a recent phenomenon that has been adopted by some nominees but not others?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What kind of \"help\" is being proposed, and how would it be delivered to families in need?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What are the potential consequences of intervening in families, and how would they be mitigated?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the criteria for determining which families need help and which need punishment?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the definition of \"high levels of aggression and violence\" and how is it measured?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the critical questions that should be asked regarding the arguments in this paragraph?',\n",
              "  'label': 'Invalid'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What are the potential consequences of stigmatizing certain families as \"terrorising communities\", and how would Helen mitigate these effects?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How would Helen define \"high levels of aggression and violence\" and what evidence would be required to support such a claim?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What kind of help and support are we talking about, and how would it be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How would the speaker's approach balance the need to protect communities with the need to support families?\",\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What role would the state or other authorities play in identifying and intervening in families, and how would their powers be limited to prevent abuse?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What specific criteria would be used to determine which families are \"terrorising communities\" and which ones need \"a great deal of help\"?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How would Helen ensure that families receive support that is tailored to their specific needs, rather than a one-size-fits-all approach?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does the speaker define \"terrorising communities\" and what evidence is there to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the evidence that supporting families can help them \"get back on their feet and able to parent their children effectively\"?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"How would the speaker's approach address the root causes of the problems faced by these families, rather than just treating the symptoms?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How would Helen address concerns that intervention could be intrusive or even harmful to families, particularly those from marginalized communities?',\n",
              "  'label': 'Useful'},\n",
              " 'Helen__66_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the basis for Helen\\'s claim that intervention can help families \"get back on their feet and able to parent their children effectively\"? Is there empirical evidence to support this?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does the fact that undocumented immigrants \"work\" justify their access to health insurance coverage? Is it a matter of fairness, or is there another reason why they should be entitled to health insurance?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How does the argument for health insurance coverage for undocumented immigrants relate to broader issues of immigration reform and border control? Should these issues be addressed separately or in conjunction with the issue of health insurance coverage?',\n",
              "  'label': 'Unhelpful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What are the potential consequences of providing health insurance coverage to undocumented immigrants? Would it create an incentive for more people to immigrate illegally, or would it improve public health outcomes?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is the accusation of \"xenophobia\" a valid critique of opposing views, or is it a form of ad hominem attack? Are there legitimate reasons why someone might oppose health insurance coverage for undocumented immigrants that have nothing to do with xenophobia?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How would providing health insurance coverage to undocumented immigrants be funded? Would it require an increase in taxes, or would it be funded through other means?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What does it mean to be \"a part of this society\"? Is it a matter of cultural integration, economic contribution, or something else? How does this relate to the issue of health insurance coverage?',\n",
              "  'label': 'Unhelpful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What are the author's underlying values and assumptions about the role of government in providing health care, and how do these influence the argument?\",\n",
              "  'label': 'Unhelpful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Are there alternative solutions to providing health insurance coverage to undocumented immigrants, such as emergency medical care or community clinics? Would these alternatives be more effective or efficient?',\n",
              "  'label': 'Unhelpful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there alternative solutions or compromises that could address the health care needs of immigrants, such as targeted public health programs or community-based initiatives?',\n",
              "  'label': 'Unhelpful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is the author\\'s dismissal of opposing views as \"xenophobic\" a fair characterization, or are there legitimate concerns about the impact of immigration on the health care system that should be considered?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does the author's argument account for the diversity of immigrant experiences, including those who may not be employed, or who may be undocumented?\",\n",
              "  'label': 'Unhelpful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the potential economic and logistical implications of providing health care coverage to immigrants? How would this impact the existing health care system, and are there sufficient resources to support this expansion?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the author\\'s definition of \"they are a part of this society\"? Is it based on residency, employment, or something else? How does this definition impact the argument?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the definition of \"they are here\" and how does it relate to the issue of health insurance coverage? Is it a moral obligation to provide health insurance to anyone who is physically present in a country, regardless of their immigration status?',\n",
              "  'label': 'Useful'},\n",
              " '17th_knight__247_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does the author\\'s assertion that immigrants \"work\" support the claim that they should have access to health care coverage? Are there other factors that should be considered, such as their employment status, income level, or access to employer-sponsored health insurance?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Is the comparison between deplaning during gate holds and open-ended delays prior to boarding a fair and accurate analogy, or are there significant differences between these two scenarios?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there any alternative solutions or approaches that could be more effective in addressing tarmac delays, such as improving airline communication, enhancing air traffic control systems, or increasing airport infrastructure?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What are the potential consequences or drawbacks of allowing passengers to deplane during gate holds, such as increased security risks or disruptions to airport operations?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How would the proposed solution address the root causes of tarmac delays, such as air traffic control issues, weather conditions, or airline operational problems?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What are the potential consequences of requiring airlines to make a legal commitment (contract of carriage) to adhere to a maximum tarmac delay trigger, and how would this be enforced?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the potential security risks associated with allowing passengers to deplane during gate holds, and how would these risks be mitigated?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How would allowing passengers to deplane during gate holds, but requiring them to remain in the immediate area, be logistically and practically implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the basis for the assertion that deplaning without CBP screening is a viable option? Is there evidence to support this claim, or is it based on personal experience?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How would the logistics of deplaning passengers during gate holds work in practice? Would it require significant changes to airport infrastructure or procedures?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What are the potential costs and resource implications of implementing the proposed solution, and how would these be funded?',\n",
              "  'label': 'Invalid'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is deplaning without CBP screening truly a viable option for commercial airlines, considering the security protocols and regulations in place?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How does the author's experience with deplaning without CBP screening during overseas contract flights translate to the context of domestic airport security and tarmac delays?\",\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What would be the legal implications of requiring airlines to make a legal commitment (contract of carriage) regarding tarmac delays, and how would this be enforced?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'How would the proposed solution address the root causes of tarmac delays, such as air traffic control issues or weather conditions?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'How would the proposed solution be fair and equitable for all airlines and classes of aircraft, considering that different airlines and aircraft may have different operational capabilities and constraints?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Why should the maximum tarmac delay trigger apply to all airlines and all classes of aircraft, without any exceptions or variations?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the basis for setting a maximum tarmac delay trigger, and how was this threshold determined?',\n",
              "  'label': 'Useful'},\n",
              " 'AFCHF_154_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Would requiring passengers to remain in the immediate area after deplaning be feasible or practical, especially in cases where the delay is extended or the airport is busy?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'Are there any other factors that contribute to deep vein thrombosis (DVT) besides seat size and configuration, and if so, how significant are they compared to seat size and configuration?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How do the fees charged by the airline for carrying pets onboard compare to the costs of other transportation options for pets, such as cargo shipping or pet-sitting services?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What exactly constitutes an \"item necessary because of factors out of the passengers\\' control\"? How would this be defined and enforced?',\n",
              "  'label': 'Invalid'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"Is the airline's role in handling the pet carrier truly minimal, or do they provide any additional services or accommodations that justify the fee?\",\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'Are the exercises shown in the inflight magazines actually effective in preventing deep vein thrombosis, and if so, how much space is required to perform them safely?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Are there any regulations or industry standards that govern the treatment and transportation of pets onboard airlines, and if so, do they provide any protections or safeguards for pet owners?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Is there any scientific evidence to support the claim that the current seat size and configuration on airlines increases the risk of DVT, and if so, what are the specific risks and consequences?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does the current seat size and configuration pose a health risk, and what evidence is there to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Are there any alternative solutions or compromises that could be proposed to address the concerns around seat size, pet fees, and other issues, rather than simply banning certain practices or fees?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any other options or alternatives available to pet owners who cannot afford the fees charged by airlines, such as traveling by car or using pet-sitting services?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is the airline\\'s pet fee truly \"exorbitant\" and unfair, or is it a reasonable reflection of the costs and logistical challenges involved in accommodating pets in the cabin?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How do the fees charged by airlines for carrying pets onboard compare to the costs of transporting pets as checked baggage or cargo, and are there any safety or logistical considerations that justify the higher fees?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Are there any alternative solutions to increasing seat size and configuration that could address the health concerns, such as providing additional exercise opportunities during flights or offering compression stockings to passengers?',\n",
              "  'label': 'Useful'},\n",
              " 'AK-traveler_186_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How do the fees charged by airlines for carrying pets onboard compare to the fees charged by other transportation providers, such as trains or buses, and are they proportionate to the costs and services provided?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'Are there any other allergens or substances that pose a similar risk of severe allergic reactions on flights, and should they also be banned?',\n",
              "  'label': 'Invalid'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'How would a ban on peanut and tree nut products affect other passengers who may not have severe allergies, but may still want to consume these products during flights?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the methodology and reliability of the study cited, which claims that 1 in 3 airplane reactions was anaphylaxis? Is the sample size sufficient, and are the results generalizable to all flights?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': \"How does the author's personal experience and emotional appeal affect the argument, and are there any objective, evidence-based arguments that support the call for a ban?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'Are there any alternative measures that could be taken to minimize the risk of allergic reactions on airplanes, short of a complete ban?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How would a ban on peanut and tree nut products on flights impact the quality of life for people with other allergies or medical conditions?',\n",
              "  'label': 'Invalid'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How common are severe allergic reactions to peanuts and tree nuts on airplanes, and what is the actual risk of anaphylaxis or death? Are there any statistics or data to support the claim that a ban is necessary?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Are there any other allergens or substances that pose a similar risk to peanuts and tree nuts, and should they also be banned from airplanes?',\n",
              "  'label': 'Invalid'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there any existing protocols or procedures in place for dealing with severe allergic reactions on flights, and have they been effective in preventing serious harm or death?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What is the potential impact of a ban on peanut and tree nut products on flights on the airline industry and the economy as a whole?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Would a ban on peanut and tree nut products on flights be effective in preventing allergic reactions, or would it be difficult to enforce and potentially lead to false sense of security?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Are there any alternative modes of transportation that could be used by individuals with severe allergies, and what are the trade-offs in terms of convenience, cost, and accessibility?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Are there alternative measures that could be taken to minimize the risk of allergic reactions on flights, such as providing epinephrine injectors or emergency medical kits, or training flight attendants to respond to allergic emergencies?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How common are severe allergic reactions to peanuts and tree nuts on airplanes, and what are the actual risks?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there any other solutions that could be implemented to make air travel safer for individuals with severe allergies, such as providing emergency medical supplies or training flight attendants to respond to allergic reactions?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How would a ban on peanut and tree nut products on flights be enforced, and what would be the potential consequences for airlines and passengers?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How does the risk of allergic reactions on airplanes compare to other risks associated with air travel, such as turbulence or engine failure?',\n",
              "  'label': 'Invalid'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What is the impact of a peanut and tree nut ban on the airline industry, and would it be a significant burden or cost to implement and enforce such a ban?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': 'What is the role of personal responsibility in managing severe allergies, and should individuals with life-threatening allergies take additional precautions to protect themselves, rather than relying solely on a ban?',\n",
              "  'label': 'Useful'},\n",
              " 'AllergyDad_200_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the reliability of the study cited, and how was the data collected?',\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Is the respondent's tone condescending or dismissive, and if so, does this undermine their argument or make it more persuasive?\",\n",
              "  'label': 'Invalid'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What is the basis for AngelComa\\'s assertion that critics are \"used to politicians saying things and not having a plan\"? Is this a generalization based on personal experience, or is there evidence to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"What specific aspects of the politician's plan are being referred to, and how do they address the critic's concerns?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"Are there any potential biases or conflicts of interest that may be influencing AngelComa's perspective on the politician's performance?\",\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific aspects of the politician\\'s performance is AngelComa referring to when they say \"he did good\"? What criteria are they using to evaluate the performance?',\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What does AngelComa mean by \"his plan is all there\"? Is the plan publicly available, and if so, where can it be accessed? What specific details of the plan are being referred to?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"How do we know that the politician's responses were timed? Is there evidence to support this claim?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is the respondent implying that the critic is naive or uninformed about how politicians typically operate? If so, is this a fair characterization, or is it a way of deflecting criticism?',\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What does it mean for a plan to be \"all there\"? Is this a vague phrase meant to deflect criticism, or is there a specific aspect of the plan that the respondent is referring to?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"Why can't the politician talk about everything in their plan? Is there a legitimate reason for not providing more detail, or is this a convenient excuse?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the basis for the claim that the politician did well in the speech? Is it based on the content of the speech or some other factor?',\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How does the respondent\\'s final sentence (\"Must be how used you are to politicians saying things and not having a plan\") relate to the original criticism of the politician\\'s speech? Is this a non sequitur, or is there a logical connection that is not immediately apparent?',\n",
              "  'label': 'Invalid'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is AngelComa's defense of the politician based on a thorough evaluation of their performance, or is it a knee-jerk reaction to criticism?\",\n",
              "  'label': 'Useful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What is the nature of the \"timed responses\" mentioned? Are they referring to a specific format or constraint of the debate/interview, and if so, how does this affect the politician\\'s ability to respond?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'AngelComa__638_LLM_us2016reddit_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does AngelComa know that the politician cannot talk about everything in the plan? Is there a specific constraint or limitation that prevents them from doing so?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there any other factors that contribute to the risk of serious reactions in flight, and how do they interact with the quality of information from airline customer service departments?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What specific solutions or policies are being proposed to address the issue, and are they supported by evidence and feasibility studies?',\n",
              "  'label': 'Invalid'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Are there any alternative explanations for the variability in information from airline customer service departments, such as differences in training or resources?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Are there any alternative explanations for the reported incidents, such as human error or miscommunication, rather than intentional wrongdoing by airline staff?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any existing regulations or industry standards in place to address peanut allergies, and if so, are they inadequate or ineffective?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How representative are the stories told by families of peanut-allergic individuals, and are they anecdotal or based on systematic data collection?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the evidence for the claim that the risk of death is significant in this context?',\n",
              "  'label': 'Invalid'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What are the specific psychological effects being referred to, and how do they impact individuals with peanut allergies?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How would regulation or external monitoring of the airline industry address the issue of inconsistent information, and what would be the potential costs and benefits of such an approach?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What are the specific studies being referred to, and are they credible and reliable sources?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How do the experiences of peanut-allergic individuals and their families compare to those of individuals with other types of allergies or medical conditions?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the proposed solution to the problem of inconsistent information from airline customer service departments, and how would it address the concerns raised in the paragraph?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is the claim that the airline industry cannot be left to self-monitor accurate, and what evidence supports this assertion?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the specific studies that have shown the quality of information from airline customer service departments to be highly variable, incomplete, or inaccurate?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How significant are the psychological effects of inaccurate or incomplete information from airline customer service departments, and are they supported by empirical research?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the scope of the problem, and how widespread are the issues with peanut allergies and airline customer service?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Are the stories told by families of peanut-allergic individuals representative of a larger trend, or are they anecdotal and exceptional cases?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_104_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How were these studies conducted, and what were their methodologies?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What evidence is there to support the claim that the emphasis on minority interests and needs in current American politics is contributing to the tone of the discussion board?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does the author's own tone and language contribute to the tone of the discussion board, and is it consistent with the call for compromise and civility?\",\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the nature of the \"hypothetical compromise position\" that was proposed, and why did the discussion of it get muted?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'Is the author\\'s characterization of the opponents of accommodations as unwilling to compromise and \"calling for funerals\" a fair representation of their views, or is it a straw man argument?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific studies are being referred to, and what are their conclusions?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How do the opponents of accommodations have a prejudgment against the researchers and their conclusions, and is this assumption justified?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the hypothetical compromise position that was proposed, and why was it muted by those demanding funerals?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does the author define \"meanspiritedness,\" and is it a fair characterization of the tone of the discussion board?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the relevance of the mention of medical associations in other countries, and how do their studies relate to the discussion at hand?',\n",
              "  'label': 'Invalid'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How do the people calling for funerals before advancing the discussion embody the Founders' fears of the tyranny of the majority, and is this a fair characterization?\",\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does the author know that the opponents of accommodations have a \"prejudgment\" against the researchers and their conclusions? Is this an assumption or based on evidence?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Who are the \"others on the board\" with good reasonable ideas, and what are their proposals?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific studies are being referred to, and what are their methodologies and findings? Are they credible and relevant to the discussion of accommodations?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What evidence is there to support the claim that meanspiritedness abounds on the board, and is it a result of current American politics?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_111_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Is the tone of the paragraph itself meanspirited, and does it contribute to the perceived lack of civility in the discussion?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How does the study's findings translate to real-world scenarios, and what are the practical implications for peanut-sensitive travelers?\",\n",
              "  'label': 'Invalid'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the methodology of the study, and is it reliable and unbiased?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Were there any controls or comparisons made to flights where peanuts were not served?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What are the limitations of the study, and how do they impact the validity of the conclusions?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"Were there any conflicts of interest or funding sources that could have influenced the study's design, methodology, or conclusions?\",\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is the conclusion that the most likely source of the allergens is the peanuts served during flights supported by sufficient evidence, or are there other possible sources that were not considered?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How were the ventilation filters collected and analyzed, and were proper controls in place?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'How were the peanut allergens identified and quantified? Were they measured using standardized methods?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there any alternative explanations for the presence of peanut allergens in the ventilation filters that were not considered in the study?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What is the threshold of peanut allergen exposure that would be considered hazardous to peanut-sensitive individuals, and did the study provide any information on this?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'What is the funding source for the study? Is there any potential bias or conflict of interest?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': 'Are the conclusions of the study supported by the data, or are they overstated or misinterpreted?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"Are there any potential confounding variables that could have affected the study's results (e.g. air circulation patterns, filter maintenance, etc.)?\",\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"What are the implications of the study's findings for peanut-sensitive travelers? Are there any recommendations for airlines or passengers?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the significance of the detection of peanut allergens in the ventilation filters, and does it necessarily imply exposure to peanut aeroallergens during flights?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What is the methodology used to collect and analyze the ventilation system filters? Is it reliable and unbiased?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Were the results replicated or validated through additional studies or experiments, or is this a single, isolated finding?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the sample size of the study? Is it representative of all commercial airliners?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Has the study been replicated or validated by other researchers?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_227_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the concentration of peanut allergens found in the ventilation system filters? Is it clinically significant?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Would discontinuing the distribution of peanuts on airplanes be an effective or practical solution to any perceived problems, or would it be an overreaction or unnecessary restriction?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the basis for the assumption that \"just saying no\" to peanuts would be an effective remedy for those who may be experiencing problems with peanut consumption?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Are the symptoms described (e.g. compulsion to consume peanuts, loss of priorities) actually common among peanut eaters on airplanes, or are they exaggerated or hypothetical?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Are there any other factors contributing to the perceived problem of peanut consumption on airplanes, such as individual allergies or sensitivities, that should be considered?',\n",
              "  'label': 'Invalid'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there alternative solutions or approaches that could be explored instead of a blanket ban on peanuts on airplanes?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'Is the argument relying on emotional appeals or scare tactics rather than evidence-based reasoning?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is the comparison between peanut consumption on airplanes and tobacco smoking a fair one, or is it an exaggeration or false equivalence?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Are there other factors or variables that contribute to the issue of peanut consumption on airplanes that are not being considered in this argument?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is the claim that serving peanuts on airplanes has risen to the level of a public health menace supported by evidence, or is it an overstatement?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is it true that peanut advocates are unable to prioritize other things over their desire for peanuts while flying, or is this an unfair assumption?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'Is the comparison between peanut consumption and addiction/disordered eating a valid one, or is it an overstatement or false analogy?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What evidence is there to support the claim that eating peanuts on airplanes is a public health menace comparable to tobacco smoking?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'Is the characterization of peanut advocates as \"blindly partisan\" a fair representation, or is it an exaggeration or straw man argument?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is it accurate to equate a desire for peanuts with disordered eating and addiction, or is this a flawed analogy?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"What is the author's expertise or qualifications in the area of public health or addiction, and are their arguments based on credible research or evidence?\",\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is the proposed solution of simply avoiding peanuts or \"just saying no\" a realistic or effective remedy for those who may have issues with peanut consumption?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any alternative solutions or approaches that could be taken to address the issue of peanut consumption on airplanes, rather than a complete ban?',\n",
              "  'label': 'Useful'},\n",
              " 'Antanagoge_239_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Would banning peanuts on airplanes be an effective solution, or would it simply lead to other snack options being substituted?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'Is the comparison between the retail business and the airline industry a fair one? Are there differences in the way prices are set in these industries that might affect the argument?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Are there any other factors that might contribute to the perceived distrust and anger towards airlines, beyond just their pricing practices?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is the \"bait and add-on\" practice Bill describes truly illegal, or is it a common and accepted business practice in the airline industry?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'Is the definition of \"bait and switch\" accurate in this context? Is the airline\\'s practice of listing taxes separately from the ticket price truly equivalent to bait and switch tactics?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Is the tone of the argument, which is largely emotional and anecdotal, effective in making a persuasive case, or would a more objective and evidence-based approach be more convincing?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is the assumption that airlines are \"forcing\" customers to buy round-trip tickets when they only need one-way tickets a valid one? Are there options available for customers who only need one-way tickets?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Is the writer's personal experience with trying to buy a ticket representative of the typical customer's experience, or is it an anomaly?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is the claim that taxes are never higher than the price of the ticket accurate? Are there any exceptions or circumstances where this might not be the case?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is the assumption that customers would never come back to a retail business that changed prices frequently a valid one? Is there evidence to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is the argument that one-way fares should be half of round-trip fares a logical one? Are there any economic or industry-specific factors that might affect the pricing of one-way vs. round-trip tickets?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'Are there any regulatory or industry standards that govern the way airlines and ticket sellers list prices and taxes? If so, are they being followed in this case?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': \"Is Bill's analogy between his retail business and the airline industry a fair comparison? Are there significant differences between the two industries that might affect pricing strategies?\",\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is it accurate to say that taxes are never higher than the price of the ticket? Are there any circumstances under which taxes might exceed the ticket price?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is Bill's expectation that one-way fares should be half of round-trip fares a reasonable one? Are there any industry standards or economic factors that might influence pricing decisions?\",\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"Is Bill's personal experience of spending hours trying to buy a ticket without getting ripped off representative of the typical customer experience, or is it an anomaly?\",\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"Is Bill's assertion that customers would never come back if he changed prices frequently supported by evidence? Has he conducted any research or surveys to support this claim?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Has Bill explored alternative explanations for why one-way fares might be higher than round-trip fares, such as differences in demand or operational costs?',\n",
              "  'label': 'Useful'},\n",
              " 'Bill_106_LLM_rrd_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Has Bill considered the possibility that airlines or ticket sellers are not intentionally trying to deceive customers, but rather are operating under complex pricing structures and regulatory requirements?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the definition of \"social progress\" being used here, and is it universally accepted?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"What is the speaker's understanding of the root causes of poverty and inequality, and how does their proposal address these issues?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence is there to support the claim that the only thing banks have done is provide consumer credit, and is this a fair characterization of their activities?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What evidence is there to support the claim that banks should be taking more risk and giving out more money, and what are the potential consequences of such actions?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Are there any counterexamples of social progress that have come from non-rich individuals or groups?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does the speaker's argument take into account issues of power and inequality, and how would their proposed solutions address these underlying structural problems?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are there any potential conflicts of interest or biases that the speaker may have that could be influencing their argument?',\n",
              "  'label': 'Invalid'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How would the speaker address concerns that increasing lending and taking on more risk could lead to financial instability and exacerbate existing social problems?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does the speaker define \"taking more risk\" and what kind of risks are they referring to?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is it accurate to say that banks have only been involved in consumer credit, or have they also been involved in other activities that have contributed to social progress?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Is the speaker implying that the only way to achieve social progress is through the actions of rich individuals and banks, or are there other factors at play?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the basis for the assertion that banks should be taking more risk and giving out more money, and how would this lead to social progress?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does the speaker define \"productive investment\" and what criteria are used to determine whether an investment is productive or not?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What alternative solutions or policies is the speaker proposing to address social problems, and how would they be implemented and funded?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_29_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is the speaker\\'s argument that the rich are not inherently \"bad guys\" and that they can be a force for social progress compatible with the idea that the poor are the ones who suffer, or are these two claims in tension with each other?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What do you mean by \"money doesn\\'t grow on trees,\" and how does this phrase relate to the economic issues being discussed?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What is the basis for the speaker's frustration with people's assumptions about the economy? Is this a representative sample of public opinion or a anecdotal experience?\",\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"What are the implications of the speaker's argument? Are they advocating for a specific economic policy or simply expressing frustration with public perceptions?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How do you define \"beneficiaries\" and \"sufferers\" in the context of sub-prime mortgages, and what are the implications of this definition?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the critical questions that should be asked regarding the arguments in the above paragraph?',\n",
              "  'label': 'Invalid'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the \"real state of the economy\" that you are trying to convey, and how do you support your assertion that people are not aware of it?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"Is the speaker's argument based on empirical evidence or personal opinion? Are they providing data or anecdotal evidence to support their claims?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What evidence is there to support the claim that the people who took sub-prime mortgages were the beneficiaries of a political strategy? Is this a subjective interpretation or a verifiable fact?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What is the relationship between the sub-prime mortgage crisis and the need for cuts, and how do you support your assertion that they are connected?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does the speaker's perspective account for the role of other factors, such as government regulation, corporate greed, or global economic trends, in the economic crisis?\",\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Is the speaker\\'s statement \"Money doesn\\'t grow on trees\" a truism or a simplistic oversimplification of complex economic issues?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What alternative policies or solutions do you propose to address the economic issues you are highlighting, and how do you support their effectiveness?',\n",
              "  'label': 'Invalid'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does the speaker define \"the real state of the economy\"? Is this a objective assessment or a biased perspective?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'How do you respond to the potential counterargument that the sub-prime mortgage crisis was a result of individual greed and lack of personal responsibility, rather than a political strategy?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is the basis for your claim that people assume that \"everything can just carry on\" and that a \"credit-based consumer thing\" is all that is needed?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is the speaker\\'s definition of a \"credit-based consumer thing\"? Is this a pejorative term or a neutral description?',\n",
              "  'label': 'Useful'},\n",
              " 'CF_46_LLM_moral_maze_schemes_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What evidence do you have to support the claim that the sub-prime mortgage crisis was a political strategy to cover things up?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"Are the two years of tax returns that were released during Trump's casino license application representative of his typical tax situation?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"Are there other issues or concerns that should take priority over Trump's tax returns? Is this a distraction from more pressing policy debates or character assessments?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"What is the basis for Clinton's claim that the IRS has made it clear that there is no prohibition on releasing tax returns while under audit? Is this a widely accepted interpretation?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"What is the relevance of Trump's tax returns to his fitness for office, and are there other issues that should take priority in the election?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is the relevance of Trump's tax returns to his fitness for office? Would releasing them necessarily address concerns about his policies or character?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'Is it true that all presidential candidates have released their tax returns for 40 years? Is this a universal expectation or a recent trend?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Are there any legitimate reasons why Trump might not want to release his tax returns, aside from the possibilities Clinton suggests? For example, might he be concerned about privacy or the potential for political opponents to misinterpret the information?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Is it fair to assume that Trump has paid no federal taxes based on those two years, or is it possible that he has paid taxes in other years?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'Is it true that every presidential candidate has released their tax returns for the past 40 years?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is Clinton's argument based on speculation and insinuation rather than concrete evidence? Is she making unsubstantiated claims about Trump's motives or behavior?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence is there to support the claim that Trump is not as rich as he says he is?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What evidence is there to support the claim that Trump is not as charitable as he claims to be?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How would releasing tax returns provide conclusive evidence of Trump's wealth, charitable giving, or tax payments? Could the returns be incomplete or misleading in some way?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"Is the IRS's statement about releasing tax returns during an audit relevant to this situation?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is it possible that Trump has legitimate reasons for not releasing his tax returns?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_103_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Clinton's own history of transparency and accountability compare to Trump's? Has she released her own tax returns, and if so, what do they reveal about her financial situation?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How will Clinton's policies address the issue of mass incarceration, which disproportionately affects communities of color, and what steps will she take to reduce the number of people in prison?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How does Clinton plan to address the systemic and institutional racism that contributes to the disparities in the criminal justice system, rather than just focusing on individual police officers' behavior?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton define \"race\" and how does she distinguish it from other factors that may also influence these outcomes?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the \"best training\" and \"best techniques\" that Clinton refers to, and how will she ensure that police officers receive them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What evidence does Clinton have to support her claim that race determines where people live, the quality of education they receive, and how they are treated in the criminal justice system?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton plan to \"restore trust between communities and the police\" and what specific policies will she implement to achieve this goal?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How will Clinton's policies address the root causes of racial tensions and disparities?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Clinton's call for criminal justice reform address the root causes of racial disparities in the criminal justice system, rather than just their symptoms?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': \"How will Clinton's policies be funded, and what resources will be allocated to support community-based initiatives and programs that address racial disparities and promote social justice?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What specific policies or actions does Clinton propose to address the issue of race in these areas, and how would they be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Clinton's approach to addressing racial disparities in the criminal justice system differ from that of her political opponents, and what are the implications of these differences?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"What is Clinton's plan to hold police officers accountable for their actions, and how will she ensure that those who engage in racial profiling or use excessive force are held accountable?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What does Clinton mean by \"best training\" and \"best techniques\" for police, and how would she ensure that these are being used consistently across different jurisdictions?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What evidence does Clinton provide to support her claim that race determines where people live, their education, and their treatment in the criminal justice system?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What role does Clinton see for community-based initiatives and grassroots organizations in addressing these issues, and how would she support and empower them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'What role does Clinton believe the federal government should play in addressing issues of racial bias and discrimination in the criminal justice system, and how will she work with state and local governments to achieve reform?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What specific criminal justice reforms does Clinton propose, and how will they address the issues of racial bias and discrimination in the system?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How does Clinton plan to \"restore trust\" between communities and the police, and what metrics would she use to measure success in this effort?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton plan to \"bring communities together\" to work towards reform, and what specific steps would be taken to achieve this goal?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton define \"good, brave police officers\" and what criteria would be used to identify them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What concrete steps would be taken to \"bring communities together\" and how would success be measured?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What specific measures would Clinton take to \"get guns out of the hands of people who should not have them\", and how would these measures be enforced?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is the statement \"The gun epidemic is the leading cause of death of young African-American men, more than the next nine causes put together\" supported by reliable data, and if so, what are the sources of this data?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific policies does Clinton propose to \"remedy some of the problems we have in the criminal justice system\", and how would they be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What does Clinton mean by \"respect\" between law enforcement and communities, and how would this respect be fostered and maintained?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How would Clinton\\'s proposal to \"get guns out of the hands of people who should not have them\" be enforced, and what are the potential consequences for law-abiding citizens?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific policies does Clinton propose to \"remedy some of the problems we have in the criminal justice system\", and how would they be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How would Clinton's proposals be funded, and what would be the cost-benefit analysis of implementing these reforms?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How does Clinton plan to \"tackle the plague of gun violence\", and what specific policies or programs would be implemented to address this issue?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What is the evidence for the claim that the \"gun epidemic is the leading cause of death of young African-American men\", and how does this statistic compare to other demographic groups?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"How would Clinton's policies be funded, and what would be the estimated cost of implementing these policies?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Are there any potential unintended consequences of Clinton's proposed policies, and how would they be addressed?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How would Clinton\\'s proposal to \"restore trust\" between law enforcement and communities be achieved, and what specific actions would be taken to address systemic issues?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Clinton plan to \"restore trust\" between law enforcement and communities, and what specific actions would be taken to achieve this goal?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'What is the timeline for implementing these policies, and how would progress be measured and evaluated?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_123_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What is the relationship between gun violence and the \"plague of problems that we\\'re seeing today\", and how would addressing gun violence solve these broader issues?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What specific policies or actions does Clinton propose to address the racial disparities in the criminal justice system that she acknowledges?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How does Clinton's argument that stop-and-frisk is ineffective and unconstitutional address the concerns of those who argue that it is a necessary tool for public safety?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What is Clinton\\'s vision for how to \"lift up\" black communities, and what role does she see for government and policy in achieving this goal?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What evidence does Clinton have to support her claims about the vibrancy of black communities and the effectiveness of community policing?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Clinton define \"community policing,\" and what evidence does she have to support her claim that it is an effective approach to law enforcement?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"What is the basis for Clinton's claim that violent crime is one-half of what it was in 1991, and property crime is down 40 percent? Is this based on national data or specific to certain cities or regions?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the critical questions that should be asked regarding the arguments in the above paragraph?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How does Clinton plan to balance the need to \"keep people safe\" with the need to address the concerns of communities of color who may feel targeted or marginalized by law enforcement practices?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What specific examples can Clinton provide to support her claim that Trump paints a \"dire negative picture\" of black communities?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What specific \"problems\" and \"unintended consequences\" is Clinton referring to in the context of the 25 years of cooperation between law enforcement and communities, and how does she propose to address these issues?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How does Clinton define \"vibrancy\" in the context of the black church, and what evidence does she have to support her assertion that it is a positive aspect of black communities?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does Clinton's focus on community policing and reducing crime rates address the root causes of the problems in black communities?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What evidence does Clinton have to support her claim that too many young African-American and Latino men ended up in jail for nonviolent offenses, and what specific policies does she propose to address this issue?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"How does Clinton's emphasis on the positive aspects of black communities address the very real problems of systemic racism and discrimination that Trump's portrayal highlights?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is the basis for Clinton's claim that crime rates have decreased significantly over the past 25 years, and how does this trend relate to the experiences of black and Latino communities?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What specific data or studies can Clinton cite to support her claim that stop-and-frisk is ineffective and was found to be unconstitutional?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"How would Clinton's proposed second chance programs work, and what is the evidence that they would be effective in reducing recidivism?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What is the evidence that military-style weapons are a significant contributor to gun violence, and how would banning them address the issue?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'Is there evidence that the police are \"outgunned\" in many places, and if so, what is the solution to this problem?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What is the evidence that private prisons have a \"profit motivation\" that leads to overcrowding, and how would ending private prisons in the state system address this issue?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the proposed plan to \"divert people from the criminal justice system,\" and how would it work in practice?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How do Clinton\\'s proposed gun safety measures, such as comprehensive background checks and a prohibition on gun sales to those on the terrorist watch list, address the issue of \"military-style weapons on the streets\"?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the proposed plan for diverting people from the criminal justice system, and how would it be implemented and funded?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How do Clinton's proposed gun safety measures, such as comprehensive background checks and a prohibition on gun sales to those on the terrorist watch list, address the issue of systemic racism in the criminal justice system?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What is the evidence that mandatory minimum sentences have led to too many people being incarcerated for too long for minor crimes? Are there alternative sentencing approaches that could be more effective?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"How would Clinton's proposed gun control measures be enforced, and what would be the consequences for non-compliance?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': \"What are the potential unintended consequences of Clinton's proposed reforms, and how would they be addressed?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the evidence for the claim that young African-American men are more likely to be arrested, charged, convicted, and incarcerated than young white men for the same crimes? Are there any studies or data that support this assertion?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Is there a clear definition of what constitutes a \"military-style weapon,\" and how would this definition be used to determine which weapons are prohibited?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_11_L': {'cq': 'What is the evidence that a bipartisan approach to gun control is possible, given the strong opposition from the gun lobby and some lawmakers?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton define \"systemic racism\" in the criminal justice system, and what specific policies or practices does she believe contribute to it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"What is the evidence that mandatory minimum sentences have put too many people away for too long for doing too little, and how would Clinton's proposed reforms address this issue?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What are the specific \"second chance programs\" that Clinton is proposing, and how would they be funded and implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton define \"systemic racism\" in the criminal justice system, and what specific policies or practices does she believe are responsible for it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': \"How would Clinton's proposed reforms be implemented in a bipartisan way, and what is the likelihood of achieving bipartisan support for these measures?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the evidence for the claim that young African-American men are more likely to be arrested, charged, convicted, and incarcerated for the same crimes as young white men? Is this a statistical fact, and if so, what are the numbers?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_10_L': {'cq': \"How would Clinton's proposal to prohibit gun sales to those on the terrorist watch list be implemented, and what safeguards would be in place to prevent false positives or abuse of the list?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_130_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the evidence that private prisons have a profit motivation to fill prison cells, and how would ending private prisons in the state system address this issue?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"What is Clinton's stance on issues like police accountability, racial profiling, and mass incarceration, and how do these issues relate to her broader approach to crime reduction?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What specific policies or actions has New York taken to reduce crime, and are they applicable to other communities?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What are the \"things that sound good\" but have not had a significant impact on reducing crime, and how does Clinton know they are ineffective?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How does Clinton propose to balance the need to keep neighborhoods safe with the need to respect the rights of young men who live in those neighborhoods, particularly in communities where there may be tensions between law enforcement and residents?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific role does Clinton envision for faith communities and business communities in addressing crime, and how does she propose to engage them in this effort?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How does Clinton plan to measure the success of her crime reduction strategies, and what metrics or benchmarks will she use to evaluate their effectiveness?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What are the root causes of crime that Clinton believes need to be addressed, and how does she propose to address them through her policies?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does Clinton propose to \"respect the rights of young men who live in those neighborhoods\" while also keeping them safe, and what specific actions would she take to achieve this balance?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'How does Clinton plan to ensure that her approach is equitable and does not disproportionately affect certain communities or groups?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Clinton's approach differ from previous approaches to addressing crime, and what makes her think it will be more effective?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton define \"what has been effective\" in reducing crime, and what evidence does she have to support her claims?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What role does Clinton see for the police in addressing crime, and how does she think they should work with communities to achieve this goal?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What evidence is there that these policies have been effective, and how do we measure their success?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the \"things that sound good\" that Clinton is referring to, and why does she think they are ineffective?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific policies or strategies have been implemented in New York that have contributed to its success in reducing crime, and are they replicable in other communities?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"What are the potential drawbacks or unintended consequences of Clinton's approach, and how does she plan to mitigate them?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_147_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What is the nature of the \"problem\" that Clinton is trying to address, and how does she think it can be solved?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What are the specific policies that led to the Great Recession, and how did they contribute to it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What evidence is there that Donald Trump \"rooted for the housing crisis\", and what does this claim even mean?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"What role did Wall Street's lack of regulation play in the crisis, and how did the Bush administration's policies contribute to this lack of regulation?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Clinton's argument account for the global nature of the financial crisis, and were there international factors that contributed to the crisis?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"How does Clinton's argument address the potential counterargument that the crisis was caused by factors outside of the government's control, such as global economic trends or demographic changes?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': 'What is the evidence for the claim that the financial crisis was the \"worst since the 1930s\", and how does it compare to other economic downturns in US history?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How does Clinton's argument account for other potential causes of the Great Recession?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there any counterarguments or alternative explanations for the causes of the financial crisis that should be considered?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What role did Wall Street play in the financial crisis, and how did the lack of regulation or oversight contribute to the crisis?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What specific investments in the middle class were neglected, and how would they have prevented or mitigated the crisis?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What is the evidence that the tax policies of the Bush administration were the primary cause of the crisis, and how do they compare to other potential causes?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Is it accurate to say that the wealthy benefited from tax cuts, and did these cuts have a direct impact on the financial crisis?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How did the lack of investment in the middle class contribute to the financial crisis, and what specific policies or actions would have prevented or mitigated the crisis?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'How did the tax cuts for the wealthy specifically harm the economy, and what evidence is there to support this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': \"How does Clinton's argument relate to the broader economic policies and decisions made during the previous administration, and are there any other factors that should be taken into account?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What evidence is there to support the claim that Donald Trump \"rooted for the housing crisis\"? Is this a verifiable fact or an opinion?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What are the specific tax policies that contributed to the financial crisis, and how did they do so?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_17_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Is it fair to say that the financial crisis was \"in large part\" due to tax policies, or were there other significant contributing factors?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton know that Putin is \"playing a really tough, long game\" and what does this phrase even mean in the context of cyber warfare?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is Clinton's concern about cyber security and Russia's alleged actions a genuine expression of worry about national security, or is it a political tactic to attack her opponent and gain an advantage in the election?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'How does Clinton know that Putin is \"playing a really tough, long game\" and what does this phrase even mean in the context of cyber warfare?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Are there any other possible explanations for the cyber attacks, or has Russia been definitively linked to them?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What evidence is there to support the claim that Russia has used cyber attacks against organizations in the US?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'What is the evidence for the claim that Russia has hacked into government files, personal files, and the Democratic National Committee, and has this evidence been independently verified?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is the relationship between Russia's alleged cyber attacks and the Democratic National Committee, and how does Clinton know that Russia was involved in the hacking of the DNC?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Clinton's own record on cyber security and national security compare to her criticism of Russia and her opponent, and are there any potential vulnerabilities or contradictions in her position?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': \"What is the evidence for Russia's involvement in cyber attacks against the US, and how reliable is this evidence?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is Clinton\\'s characterization of Putin\\'s actions as \"troubling\" and \"really tough\" a fair and balanced assessment, or is it an attempt to create a negative impression of Putin and Russia?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Are there any other countries or actors that engage in cyber warfare, and why is Russia being singled out as a particular threat?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What is the nature of the \"independent hacking groups\" mentioned by Clinton, and how do they differ from state-sponsored hacking groups?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"Is it accurate to imply that Russia's alleged cyber attacks are primarily motivated by a desire to support Trump's candidacy, or are there other factors at play?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"How does Clinton's emphasis on Russia's role in cyber warfare square with the reality of other countries, including allies, engaging in similar activities?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the basis for Clinton\\'s assertion that Donald Trump is \"very praiseworthy\" of Vladimir Putin, and is this a fair characterization of Trump\\'s views?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How does Clinton plan to balance the need to protect against cyber threats with the need to protect individual privacy and civil liberties?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific policies or actions would Clinton propose to address the threat of cyber warfare, and how would they differ from those of her opponent?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How does Clinton's criticism of Russia's alleged cyber attacks square with the US's own history of engaging in cyber espionage and hacking activities?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"How does Clinton's emphasis on defending against cyber attacks balance with the need to also engage in diplomatic efforts to prevent cyber warfare, and what role does she see for international cooperation in addressing this issue?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"What is the context of Donald's statement about Putin hacking into Americans, and how does it relate to the broader issue of cyber warfare?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What are the \"kinds of tools\" that the United States has, and why are they not being used to defend against cyber attacks?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': \"What are the implications of publicly inviting a foreign leader to hack into Americans, and how does this statement reflect on the speaker's understanding of cyber security?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What are the potential consequences of the US taking a more aggressive stance against state actors who engage in cyber attacks, and how might this escalate into a larger conflict?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How does the United States plan to \"defend the citizens of this country\" from cyber attacks, and what specific measures will be taken to do so?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What is meant by \"a different kind of warfare\", and how does it differ from traditional forms of warfare?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the evidence that Russia has been \"treating it as almost a probing, how far would we go, how much would we do\", and how has the United States responded to these probes?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does Clinton\\'s proposal to \"make it clear\" to state actors that the US will not tolerate cyber attacks differ from current US policy, and what evidence is there that this approach would be more effective?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How does Clinton's proposal to defend against cyber attacks address the root causes of these attacks, such as the lack of international norms and agreements on cyber warfare?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What evidence is there to support the claim that Russia, China, and Iran are engaging in cyber attacks against the US, and how reliable is this evidence?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What specific methods of cyber warfare are being referred to, and how do they pose a threat to national security?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What specific actions is Clinton proposing to take in order to defend against cyber attacks, and how would these actions be effective in deterring state actors?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What is the basis for the claim that the United States has a \"much greater capacity\" to engage in cyber warfare, and how does this capacity compare to that of other nations?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton define \"much greater capacity\" in terms of cyber warfare, and what specific capabilities is she referring to?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What evidence is there to support the claim that Russia, China, Iran, or other state actors are engaging in cyber attacks on the United States?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': \"How does the speaker's statement about defending the citizens of the United States from cyber attacks align with the country's current cyber security policies and strategies?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_176_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Is Clinton\\'s characterization of Donald Trump\\'s statement as an \"invitation\" to hack into Americans an accurate representation of his words, and what is the context in which he made this statement?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': \"What is the estimated cost of Clinton's plan, and how would it be funded?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How would taking out ISIS leadership, including Baghdadi, lead to the defeat of the organization, and what is the plan for dealing with the potential power vacuum that could result?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How would Clinton's plan balance the need to defeat ISIS with the need to protect human rights and prevent civilian casualties?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"How would Clinton's plan ensure that tech companies would cooperate in preventing ISIS from using the internet for radicalization and direction?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How does Clinton define \"making progress\" in the fight against ISIS, and what metrics does she use to measure success?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How would Clinton's plan address the root causes of ISIS's appeal, such as political and economic instability in the region?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"What is the plan for rebuilding and stabilizing the regions liberated from ISIS control, and how would Clinton's administration ensure that the local populations are protected and their needs are met?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific details does Clinton\\'s plan entail to \"go after\" ISIS online, and how does she propose to balance this with free speech and privacy concerns?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What is the current status of Clinton's proposed plan to support Arab and Kurdish partners in taking out ISIS in Raqqa, and what specific actions has she taken or will take to make this happen?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is the plan for dealing with the foreign fighters, money, and weapons that are supporting ISIS, and how would Clinton's administration coordinate with international partners to address these issues?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton plan to \"do much more\" with tech companies to prevent ISIS from using the internet, and what measures does she propose to ensure that these efforts are effective and do not infringe on civil liberties?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the evidence that the current military efforts in Iraq are making progress, and what are the metrics for measuring success?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What is Clinton\\'s definition of \"defeating\" ISIS, and what does she envision as the endgame in this conflict?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'How does Clinton plan to \"disrupt\" ISIS\\'s propaganda efforts online, and what role does she see for social media companies, governments, and civil society in this effort?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the specific plan to \"go after\" ISIS online, and how would it be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific efforts has Clinton made or will make to \"squeeze\" ISIS in Syria, and how does she plan to address the complexities of the Syrian conflict?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': \"How would Clinton's plan be adapted to respond to the evolving nature of ISIS and its tactics?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Clinton's experience in taking out Al Qaida leadership, including bin Laden, inform her approach to defeating ISIS, and what lessons has she learned from these efforts?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'How does Clinton propose to address the issue of foreign fighters, money, and weapons supporting ISIS, and what international cooperation does she envision to tackle these challenges?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_186_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What specific actions has Clinton taken or will take to \"take out\" ISIS leadership, including Baghdadi, and what is her strategy for dealing with the potential consequences of such actions?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the significance of Clinton\\'s phrase \"let\\'s talk about the question you asked, Lester\"? Is this an attempt to change the subject or avoid addressing the previous points?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Is it accurate to say that George W. Bush made the agreement about when American troops would leave Iraq, or is this a simplification or distortion of the facts?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What is the evidence that Donald Trump supported the invasion of Iraq? Are there any credible sources to back up this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': \"What is the context of Trump's comments on Libya?** Clinton mentions that Trump advocated for the removal of Gadhafi, but what was the context of those comments? Were they made before or after the Libyan civil war? Were they part of a larger discussion on foreign policy?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What is the relevance of Trump's business dealings with Gadhafi to the discussion of Iraq and Libya? Is this an ad hominem attack or a legitimate point?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What are the details of the agreement that would have protected American troops in Iraq, and is it true that the Iraqi government refused to provide such protection?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': \"What is the evidence for Donald Trump's support of the invasion of Iraq?** Clinton claims that Trump supported the invasion, but what are the sources that prove this? Is there a quote or a statement from Trump that explicitly shows his support?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"Was the Iraqi government's refusal to give an agreement the only reason American troops left Iraq?** Clinton implies that the Iraqi government's refusal to provide protection for American troops was the sole reason for their withdrawal. Were there other factors at play, such as the Obama administration's desire to end the war or the lack of a clear strategy for staying in Iraq?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is it accurate to say that George W. Bush made the agreement about when American troops would leave Iraq?** Clinton seems to be shifting the blame for the withdrawal of troops from Iraq from Obama to Bush. Is this a fair representation of the facts? What was the role of the Obama administration in the withdrawal of troops?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"What is the relevance of Trump's business dealings with Gadhafi?** Clinton mentions that Trump did business with Gadhafi, but how is this relevant to the discussion on foreign policy or the invasion of Libya? Is Clinton trying to imply that Trump's business dealings influenced his views on Libya, or is this just a distraction?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Clinton's statement about the need for an agreement to protect American troops relate to the broader point about Trump's position on Iraq? Is this a red herring or a relevant consideration?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Clinton\\'s statement that Trump \"actually advocated for the actions we took in Libya\" relate to the topic of Iraq? Is this a non-sequitur or an attempt to distract from the original claim?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_193_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is Clinton's response a deflection from the original question?** The moderator, Lester, asked a question, but Clinton seems to have sidestepped it and launched into a critique of Trump's foreign policy views. Is she avoiding the question or trying to change the subject?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How does Clinton's praise for law enforcement in this instance square with her previous criticisms of systemic racism and police brutality in the United States?\",\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What specific policies or strategies does Clinton propose to prevent attacks, beyond simply gathering more intelligence?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Clinton plan to address the root causes of terrorism, such as poverty, political instability, and religious extremism?',\n",
              "  'label': 'Invalid'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific measures does Clinton propose to prevent attacks, and how would they be implemented?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What is the timeline for implementing Clinton\\'s proposed \"intelligence surge\", and what resources would be allocated to support it?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What does Clinton mean by an \"intelligence surge\"? How would it be implemented, and what would be the consequences of such a surge?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"What additional information might be discovered about Rahami's motivations and actions, and how would this information inform future counter-terrorism efforts?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What does Clinton mean by \"bringing down\" Rahami, and does she support the use of deadly force in such situations?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What specific actions did law enforcement in New York, Minnesota, and New Jersey take that Clinton is praising, and how did they contribute to the capture of Rahami?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"What is the role of international cooperation in Clinton's counter-terrorism strategy, and how would she work with other countries to share intelligence and coordinate efforts?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How would Clinton balance the need for increased surveillance and intelligence gathering with the need to protect civil liberties and privacy?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Clinton\\'s proposal for an \"intelligence surge\" address the root causes of terrorism, and what other strategies would be employed to prevent radicalization?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How would an \"intelligence surge\" be achieved, and what would be the criteria for evaluating its success?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"How would Clinton's proposals differ from those of her political opponents, and what evidence does she have that her approach would be more effective?\",\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Clinton propose to balance the need for intelligence gathering with the need to protect individual privacy and civil liberties?',\n",
              "  'label': 'Useful'},\n",
              " 'CLINTON_199_1_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What role does Clinton see for diplomacy and international cooperation in preventing attacks and countering terrorism?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"Would it have been feasible to include additional countries or issues in the deal, and if so, why wasn't it done?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific aspects of the Iran deal make it \"the worst deal ever made by any country in history\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the evidence for Iran having \"power over North Korea\"? Is this a widely accepted fact or an unsubstantiated claim?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What alternative deal or approach would have been more effective, and how would it have addressed the concerns raised about the current deal?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What is the basis for the assertion that Iran can simply \"sit back 10 years\" and achieve its nuclear goals? Is this a realistic scenario, and what are the implications of such a scenario?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_S': {'cq': 'What is the role of verification and enforcement mechanisms in the deal, and how do they ensure that Iran complies with its obligations?',\n",
              "  'label': 'Invalid'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'Why should the Iran deal have included provisions related to North Korea and Yemen? What is the logical connection between these countries and the Iran nuclear deal?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is the claim that the Iran deal will lead to nuclear problems supported by credible evidence or expert opinions?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What specific \"nuclear problems\" will the deal lead to, and how will they arise?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"How does Iran's status as a trading partner justify including North Korea and Yemen in the deal?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'How does the deal address the issue of hostages, and is the payment of $1.7 billion in cash a legitimate concern or a red herring?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does the deal's 10-year duration affect its overall effectiveness, and is this a reasonable criticism of the agreement?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What are the motivations behind the deal, and are they purely driven by a desire to appease Iran, or are there other strategic or diplomatic considerations at play?',\n",
              "  'label': 'Invalid'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'Is the payment of $1.7 billion in cash a \"giveaway\" or a legitimate payment for the release of hostages, and what is the evidence for either interpretation?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What is the basis for the claim that Iran only has to \"sit back 10 years\" and do little else to achieve its goals, and is this a accurate representation of the deal\\'s terms?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What are the specific \"great giveaways\" being referred to, and how do they compare to other international agreements or deals?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does the deal with Iran compare to other international agreements or deals in terms of its provisions and outcomes? Is it truly \"one of the worst deals ever made by any country in history\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Trump plan to defeat ISIS, and what specific strategies does he propose?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': \"What is the basis for Trump's claim that it's getting tougher and tougher to defeat ISIS, and is this claim supported by credible sources?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'Is it possible to quantify the \"billions and billions of dollars\" that the US is supposedly losing by protecting other countries, and what would be the economic and strategic consequences of reducing or eliminating these expenditures?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What evidence does Trump have to support the claim that ISIS is getting stronger and spreading to more places, states, and nations?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What is the evidence that Hillary Clinton could have defeated ISIS if she had taken a different approach in the past?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'How does Trump\\'s statement that \"we can not be the policemen of the world\" square with the fact that the US has a long history of providing security guarantees to its allies, and is this approach really unsustainable?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'How would Trump balance the need to protect American interests and allies with the desire to avoid being the \"policemen of the world\", and what would be the criteria for deciding when and where to intervene militarily?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What alternative approach does Trump propose for protecting US allies and interests, and how would this approach be more effective or cost-effective than the current approach?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': \"What specific actions would Trump take to defeat ISIS, and how would they be more effective than Hillary's proposed strategies?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': \"Is Trump's criticism of Hillary Clinton's approach to ISIS fair and accurate, or is it based on a misleading or incomplete characterization of her policies and actions?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'Is it accurate to say that Hillary could have prevented the rise of ISIS in the first place, and what evidence supports this claim?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What specific allies is Trump referring to, and how would he propose to help them while also reducing the financial burden of being the \"policemen of the world\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'How does Trump\\'s statement that ISIS is in \"more and more places, more and more states, more and more nations\" square with the fact that ISIS has lost significant territory in recent years?',\n",
              "  'label': 'Invalid'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What is the evidence for Trump\\'s claim that the US is \"losing billions and billions of dollars\" by protecting its allies, and is this a fair characterization of the costs and benefits of these alliances?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_249_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"How would Trump's approach to foreign policy and national security differ from Hillary's, and what would be the consequences of adopting a more isolationist approach?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"Are the criticisms being made based on a fair and balanced assessment of the candidate's strengths and weaknesses, or are they selectively highlighting perceived flaws while ignoring potential strengths?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What are the \"lot of things\" that Trump claims are needed, and how does Clinton fail to possess them?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"What is the basis for Trump's assertion that Clinton's policies will not be implemented if she wins the election?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'What does Trump mean by \"heart\"? Is this a relevant qualification for a president, and if so, how does Clinton lack it?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'Are the arguments being made based on logical reasoning and evidence, or are they relying on emotional appeals and personal attacks?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': 'What specific statement or action by Clinton is Trump referencing when he says \"she doesn\\'t say that\"? Is this a fair criticism, or is Trump taking Clinton\\'s words out of context?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What specific \"things\" are being referred to that need to be taken care of, and how can it be claimed that they were not addressed during the previous 10 years?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_S': {'cq': \"How do the speaker's claims align with verifiable facts and credible sources, and are they open to alternative perspectives and counterarguments?\",\n",
              "  'label': 'Invalid'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'What specific examples can Trump provide to demonstrate Clinton\\'s lack of \"basic ability\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What is meant by \"presidential look\", and is this a relevant or superficial criterion for evaluating a candidate\\'s fitness for office?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is the speaker's own track record and ability to deliver on promises being held to the same standards as the candidate being criticized?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What evidence is there to support the claim that the person being referred to (presumably Hillary Clinton) lacks business ability? Is this a subjective opinion or based on verifiable facts?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'What opportunities did Clinton have to address the issues Trump mentions during her time in power, and what prevented her from doing so?',\n",
              "  'label': 'Invalid'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific evidence does Trump provide to support his claim that Clinton \"doesn\\'t have business ability\"? Is this claim based on facts or personal opinion?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Is the speaker\\'s use of phrases like \"sadly, she doesn\\'t have that\" and \"she doesn\\'t say that\" intended to convey a sense of authority and objectivity, or are they simply rhetorical devices used to sway opinion?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_251_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What does Trump mean by a \"presidential look\", and is this a relevant criterion for evaluating a candidate\\'s qualifications?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': 'What are the specific trade deals that Trump is referring to, and how does he plan to negotiate them better than Clinton?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What specific evidence does Trump have to support his claim that Hillary Clinton lacks stamina?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'Are there any other factors beyond stamina and experience that Trump believes are important for a president to have, and how does he embody those qualities himself?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'Is it accurate to say that the United States is \"defending\" Saudi Arabia, and is it true that they are not paying for it? What is the nature of the US-Saudi relationship, and how does it benefit the United States?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'What specific examples can Trump provide to demonstrate that Clinton lacks stamina? Has he observed her in situations that require stamina? ',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'Are there any examples of successful deals or diplomatic efforts that Hillary Clinton has been involved in, and if so, how do they counter Trump\\'s claim that she has only \"bad experience\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': 'What specific aspects of the Iran deal does Trump object to, and what alternative approach would he have taken?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'How does Trump\\'s emphasis on stamina and \"presidential look\" relate to the substantive qualifications and skills required for the presidency?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What does Trump mean by \"presidential look\"? Is it based on physical appearance or something else? How is it relevant to being a good president?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': \"How does Trump's own experience and track record compare to Clinton's? Does he have any experience in politics or governance that would make him a better candidate?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': \"What is the basis for Trump's claim that Saudi Arabia is not paying for its defense, and is this a accurate representation of the situation?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'How does Trump define \"bad experience\"? Are there any specific instances where Clinton\\'s experience has led to negative outcomes? Can Trump provide evidence to support his claim?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': 'How does Trump define \"bad experience,\" and is it a fair characterization of Hillary Clinton\\'s experience in government?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"Is stamina a fixed trait, or can it be developed and improved through effort and training? If so, how does Trump's claim that Hillary Clinton lacks stamina account for the possibility of growth and development?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What are the specific trade deals that Trump believes require stamina to negotiate, and how does he plan to negotiate them more effectively?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Trump define \"presidential look,\" and is it a relevant qualification for the presidency?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'Is stamina the most important quality required to be a good president? Are there other qualities, such as intelligence, empathy, or communication skills, that are more important?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_253_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': \"Is it fair to generalize that all of Clinton's experience is bad? Are there any instances where her experience has led to positive outcomes?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': 'What specific incentives does Trump propose to give companies to build new companies or expand, and how will these incentives be funded?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_9_L': {'cq': 'How does Trump\\'s argument that \"you can\\'t bring back jobs\" square with his promise to \"bring back jobs\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_S': {'cq': \"How does Trump's criticism of his opponent's 30-year record on job creation and energy policy square with his own lack of experience in these areas?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_S': {'cq': 'What are the specific details of the solar company investment that Trump is referring to, and what evidence is there that it was a disaster?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"How does Trump's own business experience and record on job creation compare to Clinton's?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'What specific plans does Trump have to \"bring back jobs\", and how would these plans address the complex issues surrounding job creation and economic development?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'What is the basis for Trump\\'s assertion that the country is \"losing so much in terms of energy\" and \"paying off our debt\"? Is this supported by credible data or sources?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'What specific incentives does Trump propose to give companies to build new companies or expand, and how would these incentives be funded?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What evidence does Trump have that investing in solar energy has been a disaster?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_8_L': {'cq': 'What is the relationship between the national debt and energy policy, and how does Trump propose to address the debt while implementing new energy policies?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': \"How accurate is Trump's claim that the Obama administration has added $20 trillion in debt, and what is the context for this increase in debt?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'How does Trump plan to \"bring back jobs\" that have left the country, and what evidence does he have that his approach will be successful?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_S': {'cq': 'What evidence is there that companies are leaving Michigan, Ohio, and other places solely due to energy policies, and not due to other factors such as globalization, automation, or market trends?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_L': {'cq': \"What is the basis for Trump's claim that the Obama administration has increased the national debt, and is this a fair criticism?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': \"What specific energy policies is Trump proposing, and how do they differ from Clinton's?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_L': {'cq': \"How does Trump's criticism of Clinton's 30 years of experience square with his own lack of experience in government and energy policy?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': 'How does Trump define \"disaster\" in this context, and what metrics is he using to measure the success or failure of solar energy investments?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_26_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Trump\\'s claim that the country is \"putting a lot of people out of work\" due to energy policies square with the fact that the US unemployment rate has been declining since 2010?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_6_L': {'cq': \"What is the relevance of Trump's personal feelings about Hillary's ads to the issues and policies at stake in the election?\",\n",
              "  'label': 'Unhelpful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_S': {'cq': 'What evidence does Trump have to support his claim that many of the ads run by Hillary Clinton are \"absolutely untrue\" and \"misrepresentations\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L': {'cq': 'Can Trump provide evidence to support his claim that many of Hillary\\'s ads are \"absolutely untrue\" and \"misrepresentations\"?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_S': {'cq': 'Is Trump being hypocritical by accusing Hillary Clinton of running negative ads, when he himself has been known to engage in similar tactics?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_7_S': {'cq': 'How does Trump\\'s statement that \"nobody feels sorry for\" Rosie O\\'Donnell reflect his views on women and his willingness to engage in personal attacks?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_S': {'cq': 'How does Trump justify his attacks on Rosie O\\'Donnell, and does he really believe that she \"deserves\" them?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_5_L': {'cq': \"Is Trump's criticism of Hillary's negative ads hypocritical, given his own history of using negative rhetoric and personal attacks?\",\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L': {'cq': 'What are the \"tremendous commercials\" Hillary is running, and are they factually accurate?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_4_S': {'cq': 'Is Trump\\'s claim that he has spent \"practically nothing\" on ads a accurate representation of his campaign\\'s advertising expenditures?',\n",
              "  'label': 'Useful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_2_L': {'cq': 'What was Trump planning to say about Hillary and her family that he deemed \"extremely rough\" and inappropriate?',\n",
              "  'label': 'Unhelpful'},\n",
              " 'TRUMP_275_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_1_L': {'cq': \"How does Trump justify his past behavior towards Rosie O'Donnell, and is it relevant to the current election?\",\n",
              "  'label': 'Unhelpful'},\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "qo20wcxgbJao"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy tokenizer for validation theoretical CQs\n",
        "\n",
        "theoretical_token_lengths = []\n",
        "theoretical_tokenized_cqs= []\n",
        "theoretical_useful_token_lengths = []\n",
        "theoretical_useful_tokenized_cqs = []\n",
        "theoretical_unhelpful_token_lengths = []\n",
        "theoretical_unhelpful_tokenized_cqs = []\n",
        "theoretical_invalid_token_lengths = []\n",
        "theoretical_invalid_tokenized_cqs = []\n",
        "\n",
        "for cq in theoretical_cqs.values():\n",
        "    tokenized_cq = nlp(cq[\"cq\"])\n",
        "    theoretical_tokenized_cqs.append(tokenized_cq)\n",
        "    theoretical_token_lengths.append(len(tokenized_cq))\n",
        "    if cq[\"label\"] == \"Useful\":\n",
        "        theoretical_useful_tokenized_cqs.append(tokenized_cq)\n",
        "        theoretical_useful_token_lengths.append(len(tokenized_cq))\n",
        "    elif cq[\"label\"] == \"Unhelpful\":\n",
        "        theoretical_unhelpful_tokenized_cqs.append(tokenized_cq)\n",
        "        theoretical_unhelpful_token_lengths.append(len(tokenized_cq))\n",
        "    elif cq[\"label\"] == \"Invalid\":\n",
        "        theoretical_invalid_tokenized_cqs.append(tokenized_cq)\n",
        "        theoretical_invalid_token_lengths.append(len(tokenized_cq))"
      ],
      "metadata": {
        "id": "Ye-xyEljbRu9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SpaCy tokenizer for validation LLM-generated CQs\n",
        "\n",
        "llm_token_lengths = []\n",
        "llm_tokenized_cqs = []\n",
        "llm_useful_token_lengths = []\n",
        "llm_useful_tokenized_cqs = []\n",
        "llm_unhelpful_token_lengths = []\n",
        "llm_unhelpful_tokenized_cqs = []\n",
        "llm_invalid_token_lengths = []\n",
        "llm_invalid_tokenized_cqs = []\n",
        "\n",
        "for cq in llm_cqs.values():\n",
        "    tokenized_cq = nlp(cq[\"cq\"])\n",
        "    llm_tokenized_cqs.append(tokenized_cq)\n",
        "    llm_token_lengths.append(len(tokenized_cq))\n",
        "    if cq[\"label\"] == \"Useful\":\n",
        "        llm_useful_tokenized_cqs.append(tokenized_cq)\n",
        "        llm_useful_token_lengths.append(len(tokenized_cq))\n",
        "    elif cq[\"label\"] == \"Unhelpful\":\n",
        "        llm_unhelpful_tokenized_cqs.append(tokenized_cq)\n",
        "        llm_unhelpful_token_lengths.append(len(tokenized_cq))\n",
        "    elif cq[\"label\"] == \"Invalid\":\n",
        "        llm_invalid_tokenized_cqs.append(tokenized_cq)\n",
        "        llm_invalid_token_lengths.append(len(tokenized_cq))"
      ],
      "metadata": {
        "id": "ZvvLmebbdgJH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_token_stats(name, token_lengths):\n",
        "    print(f\"{name}\\tMin token length:\\t {np.min(token_lengths)}\")\n",
        "    print(f\"\\t\\tQ1 token length:\\t {np.percentile(token_lengths, 25):.2f}\")\n",
        "    print(f\"\\t\\tMedian token length:\\t {np.median(token_lengths):.2f}\")\n",
        "    print(f\"\\t\\tAvg token length:\\t {np.mean(token_lengths):.2f}\")\n",
        "    print(f\"\\t\\tQ3 token length:\\t {np.percentile(token_lengths, 75):.2f}\")\n",
        "    print(f\"\\t\\tMax token length:\\t {np.max(token_lengths)}\\n\")"
      ],
      "metadata": {
        "id": "KEIvRbUhPjFC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Theoretical CQ Token Stats with SpaCy:\\n\")\n",
        "print_token_stats(\"All:\\t\", theoretical_token_lengths)\n",
        "print_token_stats(\"Useful:\\t\", theoretical_useful_token_lengths)\n",
        "print_token_stats(\"Unhelpful:\", theoretical_unhelpful_token_lengths)\n",
        "print_token_stats(\"Invalid:\", theoretical_invalid_token_lengths)"
      ],
      "metadata": {
        "id": "kxFuL1hIPx_N",
        "outputId": "f3dc8e26-f94c-4e76-f6c4-eff128b1709d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Theoretical CQ Token Stats with SpaCy:\n",
            "\n",
            "All:\t\tMin token length:\t 8\n",
            "\t\tQ1 token length:\t 19.00\n",
            "\t\tMedian token length:\t 25.00\n",
            "\t\tAvg token length:\t 25.30\n",
            "\t\tQ3 token length:\t 30.00\n",
            "\t\tMax token length:\t 63\n",
            "\n",
            "Useful:\t\tMin token length:\t 8\n",
            "\t\tQ1 token length:\t 21.00\n",
            "\t\tMedian token length:\t 25.00\n",
            "\t\tAvg token length:\t 25.81\n",
            "\t\tQ3 token length:\t 31.00\n",
            "\t\tMax token length:\t 49\n",
            "\n",
            "Unhelpful:\tMin token length:\t 9\n",
            "\t\tQ1 token length:\t 19.00\n",
            "\t\tMedian token length:\t 25.00\n",
            "\t\tAvg token length:\t 25.05\n",
            "\t\tQ3 token length:\t 29.00\n",
            "\t\tMax token length:\t 62\n",
            "\n",
            "Invalid:\tMin token length:\t 8\n",
            "\t\tQ1 token length:\t 18.00\n",
            "\t\tMedian token length:\t 24.00\n",
            "\t\tAvg token length:\t 24.69\n",
            "\t\tQ3 token length:\t 30.00\n",
            "\t\tMax token length:\t 63\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nLLM-Generated CQ Token Stats with SpaCy:\\n\")\n",
        "print_token_stats(\"All:\\t\", llm_token_lengths)\n",
        "print_token_stats(\"Useful:\\t\", llm_useful_token_lengths)\n",
        "print_token_stats(\"Unhelpful:\", llm_unhelpful_token_lengths)\n",
        "print_token_stats(\"Invalid:\", llm_invalid_token_lengths)"
      ],
      "metadata": {
        "id": "F--F7RitPvhh",
        "outputId": "9fcf9d97-247f-49c0-9618-30a7bda65f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "LLM-Generated CQ Token Stats with SpaCy:\n",
            "\n",
            "All:\t\tMin token length:\t 8\n",
            "\t\tQ1 token length:\t 23.00\n",
            "\t\tMedian token length:\t 27.00\n",
            "\t\tAvg token length:\t 27.47\n",
            "\t\tQ3 token length:\t 32.00\n",
            "\t\tMax token length:\t 72\n",
            "\n",
            "Useful:\t\tMin token length:\t 8\n",
            "\t\tQ1 token length:\t 23.00\n",
            "\t\tMedian token length:\t 27.00\n",
            "\t\tAvg token length:\t 27.62\n",
            "\t\tQ3 token length:\t 32.00\n",
            "\t\tMax token length:\t 64\n",
            "\n",
            "Unhelpful:\tMin token length:\t 10\n",
            "\t\tQ1 token length:\t 23.00\n",
            "\t\tMedian token length:\t 27.00\n",
            "\t\tAvg token length:\t 27.48\n",
            "\t\tQ3 token length:\t 31.00\n",
            "\t\tMax token length:\t 54\n",
            "\n",
            "Invalid:\tMin token length:\t 8\n",
            "\t\tQ1 token length:\t 20.00\n",
            "\t\tMedian token length:\t 25.00\n",
            "\t\tAvg token length:\t 26.11\n",
            "\t\tQ3 token length:\t 31.00\n",
            "\t\tMax token length:\t 72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (theoretical_token_lengths, \"All\", \"Theoretical\"),\n",
        "    (theoretical_useful_token_lengths, \"Useful\", \"Theoretical\"),\n",
        "    (theoretical_unhelpful_token_lengths, \"Unhelpful\", \"Theoretical\"),\n",
        "    (theoretical_invalid_token_lengths, \"Invalid\", \"Theoretical\"),\n",
        "    (llm_token_lengths, \"All\", \"LLM-Generated\"),\n",
        "    (llm_useful_token_lengths, \"Useful\", \"LLM-Generated\"),\n",
        "    (llm_unhelpful_token_lengths, \"Unhelpful\", \"LLM-Generated\"),\n",
        "    (llm_invalid_token_lengths, \"Invalid\", \"LLM-Generated\"),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    [(length, category, source) for lengths, category, source in data for length in lengths],\n",
        "    columns=[\"Token Length\", \"Category\", \"Source\"]\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.boxplot(x=\"Category\", y=\"Token Length\", hue=\"Source\", data=df, palette=\"Set2\")\n",
        "\n",
        "for i, (token_lengths, category, source) in enumerate(data):\n",
        "    q1 = np.percentile(token_lengths, 25)\n",
        "    median = np.median(token_lengths)\n",
        "    q3 = np.percentile(token_lengths, 75)\n",
        "    min_val = np.min(token_lengths)\n",
        "    max_val = np.max(token_lengths)\n",
        "\n",
        "    x_pos = i % 4 - (-0.3 if source == \"LLM-Generated\" else 0.1)\n",
        "\n",
        "    ax.text(x_pos, min_val, f\"{min_val}\", horizontalalignment='right', size=9)\n",
        "    ax.text(x_pos, q1, f\"{q1:.1f}\", horizontalalignment='center', size=9)\n",
        "    ax.text(x_pos, median, f\"{median:.1f}\", horizontalalignment='center', size=9)\n",
        "    ax.text(x_pos, q3, f\"{q3:.1f}\", horizontalalignment='center', size=9)\n",
        "    ax.text(x_pos, max_val, f\"{max_val}\", horizontalalignment='right', size=9)\n",
        "\n",
        "plt.title(\"Theoretical and LLM-Generated CQ Token Lengths\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Token Length\")\n",
        "plt.legend(title=\"CQ Type\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R30KUiwYRt0I",
        "outputId": "c14bc155-d861-4f26-9ef1-538c3478ce29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tokenized_cq in theoretical_tokenized_cqs[:5]:\n",
        "    print([(token.text, token.lemma_, token.pos_) for token in tokenized_cq])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Kzt3MGmwq8",
        "outputId": "d447b797-a993-48da-8e42-e734c3cdafa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Is', 'be', 'AUX'), ('the', 'the', 'DET'), ('current', 'current', 'ADJ'), ('political', 'political', 'ADJ'), ('situation', 'situation', 'NOUN'), ('actually', 'actually', 'ADV'), ('a', 'a', 'DET'), ('typical', 'typical', 'ADJ'), ('case', 'case', 'NOUN'), ('of', 'of', 'ADP'), ('other', 'other', 'ADJ'), ('political', 'political', 'ADJ'), ('situations', 'situation', 'NOUN'), ('that', 'that', 'PRON'), ('require', 'require', 'VERB'), ('working', 'work', 'VERB'), ('closely', 'closely', 'ADV'), ('with', 'with', 'ADP'), ('NATO', 'NATO', 'PROPN'), ('and', 'and', 'CCONJ'), ('our', 'our', 'PRON'), ('allies', 'ally', 'NOUN'), ('?', '?', 'PUNCT'), ('How', 'how', 'SCONJ'), ('widely', 'widely', 'ADV'), ('applicable', 'applicable', 'ADJ'), ('is', 'be', 'AUX'), ('the', 'the', 'DET'), ('generalization', 'generalization', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('Is', 'be', 'AUX'), ('it', 'it', 'PRON'), ('actually', 'actually', 'ADV'), ('the', 'the', 'DET'), ('case', 'case', 'NOUN'), ('that', 'that', 'SCONJ'), ('working', 'work', 'VERB'), ('with', 'with', 'ADP'), ('our', 'our', 'PRON'), ('friends', 'friend', 'NOUN'), ('in', 'in', 'ADP'), ('the', 'the', 'DET'), ('Middel', 'Middel', 'PROPN'), ('East', 'East', 'PROPN'), ('is', 'be', 'AUX'), ('necessary', 'necessary', 'ADJ'), ('and', 'and', 'CCONJ'), ('Trump', 'Trump', 'PROPN'), ('has', 'have', 'AUX'), ('been', 'be', 'AUX'), ('very', 'very', 'ADV'), ('dismissive', 'dismissive', 'ADJ'), ('of', 'of', 'ADP'), ('this', 'this', 'PRON'), ('?', '?', 'PUNCT'), ('Is', 'be', 'AUX'), ('there', 'there', 'PRON'), ('evidence', 'evidence', 'NOUN'), ('for', 'for', 'ADP'), ('this', 'this', 'DET'), ('claim', 'claim', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('Are', 'be', 'AUX'), ('there', 'there', 'PRON'), ('special', 'special', 'ADJ'), ('circumstances', 'circumstance', 'NOUN'), ('pertaining', 'pertain', 'VERB'), ('to', 'to', 'ADP'), ('the', 'the', 'DET'), ('current', 'current', 'ADJ'), ('political', 'political', 'ADJ'), ('situation', 'situation', 'NOUN'), ('that', 'that', 'PRON'), ('undermine', 'undermine', 'VERB'), ('its', 'its', 'PRON'), ('generalisability', 'generalisability', 'NOUN'), ('to', 'to', 'ADP'), ('other', 'other', 'ADJ'), ('political', 'political', 'ADJ'), ('situations', 'situation', 'NOUN'), ('that', 'that', 'PRON'), ('require', 'require', 'VERB'), ('working', 'work', 'VERB'), ('closely', 'closely', 'ADV'), ('with', 'with', 'ADP'), ('NATO', 'NATO', 'PROPN'), ('and', 'and', 'CCONJ'), ('our', 'our', 'PRON'), ('allies', 'ally', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('Are', 'be', 'AUX'), ('there', 'there', 'PRON'), ('other', 'other', 'ADJ'), ('relevant', 'relevant', 'ADJ'), ('goals', 'goal', 'NOUN'), ('that', 'that', 'PRON'), ('conflict', 'conflict', 'VERB'), ('with', 'with', 'ADP'), ('vacuuming', 'vacuum', 'VERB'), ('up', 'up', 'ADP'), ('intelligence', 'intelligence', 'NOUN'), ('from', 'from', 'ADP'), ('Europe', 'Europe', 'PROPN'), ('and', 'and', 'CCONJ'), ('the', 'the', 'DET'), ('Middle', 'Middle', 'PROPN'), ('East', 'East', 'PROPN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('Is', 'be', 'AUX'), ('it', 'it', 'PRON'), ('actually', 'actually', 'ADV'), ('the', 'the', 'DET'), ('case', 'case', 'NOUN'), ('that', 'that', 'SCONJ'), ('the', 'the', 'DET'), ('current', 'current', 'ADJ'), ('political', 'political', 'ADJ'), ('situation', 'situation', 'NOUN'), ('requires', 'require', 'VERB'), ('working', 'work', 'VERB'), ('closely', 'closely', 'ADV'), ('with', 'with', 'ADP'), ('NATO', 'NATO', 'PROPN'), ('and', 'and', 'CCONJ'), ('our', 'our', 'PRON'), ('allies', 'ally', 'NOUN'), ('and', 'and', 'CCONJ'), ('Trump', 'Trump', 'PROPN'), ('is', 'be', 'AUX'), ('desistive', 'desistive', 'NOUN'), ('of', 'of', 'ADP'), ('working', 'work', 'VERB'), ('with', 'with', 'ADP'), ('our', 'our', 'PRON'), ('allies', 'ally', 'NOUN'), ('?', '?', 'PUNCT'), ('Is', 'be', 'AUX'), ('there', 'there', 'PRON'), ('evidence', 'evidence', 'NOUN'), ('for', 'for', 'ADP'), ('this', 'this', 'DET'), ('claim', 'claim', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tokenized_cq in llm_tokenized_cqs[:5]:\n",
        "    print([(token.text, token.lemma_, token.pos_) for token in tokenized_cq])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMr_9f7-mujt",
        "outputId": "aeda7e2a-8f7d-4783-ccb7-95dc9cd6a13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('What', 'what', 'PRON'), ('are', 'be', 'AUX'), ('the', 'the', 'DET'), ('potential', 'potential', 'ADJ'), ('drawbacks', 'drawback', 'NOUN'), ('or', 'or', 'CCONJ'), ('risks', 'risk', 'NOUN'), ('of', 'of', 'ADP'), ('increased', 'increase', 'VERB'), ('cooperation', 'cooperation', 'NOUN'), ('with', 'with', 'ADP'), ('Muslim', 'muslim', 'ADJ'), ('nations', 'nation', 'NOUN'), ('or', 'or', 'CCONJ'), ('communities', 'community', 'NOUN'), (',', ',', 'PUNCT'), ('and', 'and', 'CCONJ'), ('how', 'how', 'SCONJ'), ('would', 'would', 'AUX'), ('Clinton', 'Clinton', 'PROPN'), ('mitigate', 'mitigate', 'VERB'), ('these', 'these', 'DET'), ('risks', 'risk', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('What', 'what', 'DET'), ('evidence', 'evidence', 'NOUN'), ('is', 'be', 'AUX'), ('there', 'there', 'ADV'), ('that', 'that', 'SCONJ'), ('Donald', 'Donald', 'PROPN'), ('Trump', 'Trump', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('rhetoric', 'rhetoric', 'NOUN'), ('has', 'have', 'AUX'), ('led', 'lead', 'VERB'), ('to', 'to', 'ADP'), ('the', 'the', 'DET'), ('alienation', 'alienation', 'NOUN'), ('of', 'of', 'ADP'), ('Muslim', 'muslim', 'ADJ'), ('communities', 'community', 'NOUN'), (',', ',', 'PUNCT'), ('and', 'and', 'CCONJ'), ('how', 'how', 'SCONJ'), ('would', 'would', 'AUX'), ('Clinton', 'Clinton', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('approach', 'approach', 'NOUN'), ('to', 'to', 'ADP'), ('working', 'work', 'VERB'), ('with', 'with', 'ADP'), ('these', 'these', 'DET'), ('communities', 'community', 'NOUN'), ('be', 'be', 'AUX'), ('more', 'more', 'ADV'), ('effective', 'effective', 'ADJ'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('How', 'how', 'SCONJ'), ('does', 'do', 'AUX'), ('Clinton', 'Clinton', 'PROPN'), ('define', 'define', 'VERB'), ('\"', '\"', 'PUNCT'), ('working', 'work', 'VERB'), ('more', 'more', 'ADV'), ('closely', 'closely', 'ADV'), ('\"', '\"', 'PUNCT'), ('with', 'with', 'ADP'), ('allies', 'ally', 'NOUN'), (',', ',', 'PUNCT'), ('and', 'and', 'CCONJ'), ('what', 'what', 'DET'), ('specific', 'specific', 'ADJ'), ('actions', 'action', 'NOUN'), ('or', 'or', 'CCONJ'), ('policies', 'policy', 'NOUN'), ('would', 'would', 'AUX'), ('she', 'she', 'PRON'), ('implement', 'implement', 'VERB'), ('to', 'to', 'PART'), ('achieve', 'achieve', 'VERB'), ('this', 'this', 'DET'), ('goal', 'goal', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('What', 'what', 'PRON'), ('is', 'be', 'AUX'), ('the', 'the', 'DET'), ('evidence', 'evidence', 'NOUN'), ('that', 'that', 'SCONJ'), ('Muslim', 'muslim', 'ADJ'), ('communities', 'community', 'NOUN'), ('are', 'be', 'AUX'), ('\"', '\"', 'PUNCT'), ('on', 'on', 'ADP'), ('the', 'the', 'DET'), ('front', 'front', 'ADJ'), ('lines', 'line', 'NOUN'), ('\"', '\"', 'PUNCT'), ('of', 'of', 'ADP'), ('counter', 'counter', 'ADJ'), ('-', '-', 'NOUN'), ('terrorism', 'terrorism', 'NOUN'), ('efforts', 'effort', 'NOUN'), (',', ',', 'PUNCT'), ('and', 'and', 'CCONJ'), ('how', 'how', 'SCONJ'), ('would', 'would', 'AUX'), ('Clinton', 'Clinton', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('policies', 'policy', 'NOUN'), ('support', 'support', 'VERB'), ('and', 'and', 'CCONJ'), ('empower', 'empower', 'VERB'), ('these', 'these', 'DET'), ('communities', 'community', 'NOUN'), ('in', 'in', 'ADP'), ('this', 'this', 'DET'), ('role', 'role', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n",
            "[('What', 'what', 'DET'), ('specific', 'specific', 'ADJ'), ('intelligence', 'intelligence', 'NOUN'), ('benefits', 'benefit', 'NOUN'), ('does', 'do', 'AUX'), ('Clinton', 'Clinton', 'PROPN'), ('expect', 'expect', 'VERB'), ('to', 'to', 'PART'), ('gain', 'gain', 'VERB'), ('from', 'from', 'ADP'), ('working', 'work', 'VERB'), ('more', 'more', 'ADV'), ('closely', 'closely', 'ADV'), ('with', 'with', 'ADP'), ('European', 'european', 'ADJ'), ('and', 'and', 'CCONJ'), ('Middle', 'middle', 'ADJ'), ('Eastern', 'eastern', 'ADJ'), ('allies', 'ally', 'NOUN'), ('?', '?', 'PUNCT')]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clusterization (with TF-IDF vectorization)"
      ],
      "metadata": {
        "id": "ciVUICyLZ98M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_cqs_with_labels(cq_dict, num_clusters=5):\n",
        "    cqs = [cq[\"cq\"] for cq in cq_dict.values()]\n",
        "    labels = [cq[\"label\"] for cq in cq_dict.values()]\n",
        "\n",
        "    # Convert to numerical representation\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(cqs)  # TF-IDF matrix\n",
        "\n",
        "    # Apply K-Means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "\n",
        "    # Initialize cluster dictionary\n",
        "    clustered_cqs = {\n",
        "        i: {\n",
        "            \"questions\": [],\n",
        "            \"labels\": [],  # Store labels alongside questions\n",
        "            \"metadata\": {\"tot_questions\": 0, \"labels\": {'Useful': 0, 'Unhelpful': 0, \"Invalid\": 0}, \"percentage_useful\": 0.0}\n",
        "        }\n",
        "        for i in range(num_clusters)\n",
        "    }\n",
        "\n",
        "    for i, cq in enumerate(cqs):\n",
        "        cluster_id = clusters[i]\n",
        "        clustered_cqs[cluster_id][\"questions\"].append(cq)\n",
        "        clustered_cqs[cluster_id][\"labels\"].append(labels[i])  # Store label here\n",
        "        clustered_cqs[cluster_id][\"metadata\"][\"tot_questions\"] += 1\n",
        "        clustered_cqs[cluster_id][\"metadata\"][\"labels\"][labels[i]] += 1\n",
        "\n",
        "    for cluster_id, data in clustered_cqs.items():\n",
        "        useful_count = data[\"metadata\"][\"labels\"][\"Useful\"]\n",
        "        total_count = data[\"metadata\"][\"tot_questions\"]\n",
        "        if total_count > 0:\n",
        "            data[\"metadata\"][\"percentage_useful\"] = round(useful_count / total_count, 2)\n",
        "\n",
        "    return clustered_cqs\n"
      ],
      "metadata": {
        "id": "a0Q4k4zzMQKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Theoretical CQs\n",
        "theoretical_clusters = cluster_cqs_with_labels(theoretical_cqs, num_clusters=17)"
      ],
      "metadata": {
        "id": "BtIixoEcOdvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Theoretical CQs\n",
        "for cluster_id, cluster_data in theoretical_clusters.items():\n",
        "    print(f\"\\n=== Cluster {cluster_id} ===\")\n",
        "    print(f\"Total Questions: {cluster_data['metadata']['tot_questions']}\")\n",
        "    print(f\"Label Distribution: {cluster_data['metadata']['labels']}\")\n",
        "    print(f\"Percentage Useful: {cluster_data['metadata']['percentage_useful'] * 100:.0f}%\")\n",
        "\n",
        "    # Separate questions by label\n",
        "    categorized_questions = {\"Useful\": [], \"Unhelpful\": [], \"Invalid\": []}\n",
        "\n",
        "    for question, label in zip(cluster_data[\"questions\"], cluster_data[\"labels\"]):\n",
        "        categorized_questions[label].append(question)\n",
        "\n",
        "    print(\"\\nSample Questions:\")\n",
        "    for label, questions in categorized_questions.items():\n",
        "        print(f\"\\n  {label} Questions:\")\n",
        "        for question in questions[:3]:  # Show up to 3 samples per label\n",
        "            print(f\"    - {question}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6xeXgprL5N1",
        "outputId": "4d433879-67d3-4f05-cd6b-a6635103bd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cluster 0 ===\n",
            "Total Questions: 6\n",
            "Label Distribution: {'Useful': 0, 'Unhelpful': 2, 'Invalid': 4}\n",
            "Percentage Useful: 0%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Is what the study said clear? Are there technical terms used that are not explained clearly?\n",
            "    - Is what Simon Rose said clear? Are there technical terms used that are not explained clearly?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Is what these people said clear? Are there technical terms used that are not explained clearly?\n",
            "    - Is what the report said clear? Are there technical terms used that are not explained clearly?\n",
            "    - Is what the police said clear? Are there technical terms used that are not explained clearly?\n",
            "\n",
            "=== Cluster 1 ===\n",
            "Total Questions: 109\n",
            "Label Distribution: {'Useful': 39, 'Unhelpful': 57, 'Invalid': 13}\n",
            "Percentage Useful: 36%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Is the current political situation actually a typical case of other political situations that require working closely with NATO and our allies? How widely applicable is the generalization?\n",
            "    - How strong is the generalization that if Clinton achieved putting together a coalition to impose tough sanctions on Iran, then the USA would drive Iranians to the negotiation table?\n",
            "    - How strong is the generalization that if we deploy half a billion more solar panels there will be enough clean energy to power every home?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Is working with our friends in the Mideast actually a typical case of other political moves that are necessary and Trump is very dismissive of? How widely applicable is the generalization?\n",
            "    - Is China's exports actually a typical case of other countries' exports that Clinton has been working on? How widely applicable is the generalization?\n",
            "    - Is American exports actually a typical case of other relevant statistics, such as new jobs, that have imrpoved under Clinton's leadership? How widely applicable is the generalization?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Is the USA actually a typical case of other countries that have other problems than Iran? How widely applicable is the generalization?\n",
            "    - Is too many men actually a typical case of other men that are African-American and Latino? How widely applicable is the generalization?\n",
            "    - Is the opportunities that so many families are working to provide for their kids actually a typical case of other matters that we should be proud of? How widely applicable is the generalization?\n",
            "\n",
            "=== Cluster 2 ===\n",
            "Total Questions: 74\n",
            "Label Distribution: {'Useful': 18, 'Unhelpful': 43, 'Invalid': 13}\n",
            "Percentage Useful: 24%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Are there special circumstances pertaining to Clinton that undermine its generalisability to other people that have been Secretary of State?\n",
            "    - Are there special circumstances pertaining to Clinton that undermine its generalisability to other people that have been a senator?\n",
            "    - Are there special circumstances pertaining to current trickle-down economics that undermine its generalisability to other trickle-down policies that got us into the mess we were in 2008 and 2009?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Are there special circumstances pertaining to the current political situation that undermine its generalisability to other political situations that require working closely with NATO and our allies?\n",
            "    - Are there special circumstances pertaining to working with our friends in the Middel East that undermine its generalisability to other political moves that are necessary?\n",
            "    - Are there special circumstances pertaining to China's exports that undermine its generalisability to other countries' exports that Clinton has been working on?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Are there special circumstances pertaining to the USA that undermine its generalisability to other countries that have other problems than Iran?\n",
            "    - Are there special circumstances pertaining to Trump that undermine its generalisability to other people that started their business with millions borrowed from their father?\n",
            "    - Are there special circumstances pertaining to the opportunities that so many families are working to provide for their kids that undermine their generalisability to other matters, which is something we should be proud of?\n",
            "\n",
            "=== Cluster 3 ===\n",
            "Total Questions: 35\n",
            "Label Distribution: {'Useful': 12, 'Unhelpful': 13, 'Invalid': 10}\n",
            "Percentage Useful: 34%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Are there reasons to believe that profit-sharing is not good in this situation?\n",
            "    - Are these people a genuine experts in economics?\n",
            "    - Is this study done by genuine experts in peanut allergies? Is the claim that \"peanut allergens can be eluted from ventilation system filters in commercial airliners\" relevant to domain peanut allergies?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Did the study really assert that peanut allergens can be eluted from ventilation system filters in commercial airliners?\n",
            "    - Is the claim that \"peanut allergens can be eluted from ventilation system filters in commercial airliners\" consistent with what other experts in peanut allergies say? Is it consistent with known evidence in peanut allergies?\n",
            "    - How strong is the generalization that if law enforcement responds quickly and professionally, then Clinton will be proud of law enforcement?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Will a subject that sees profit-sharing as not good agree with retaining commitment to \"improving labour benefits\"?\n",
            "    - Is the $6 trillion we've spent in the Middle East relevant to the domain Middle East?\n",
            "    - Is needing law and order consistent with known evidence in law and order?\n",
            "\n",
            "=== Cluster 4 ===\n",
            "Total Questions: 32\n",
            "Label Distribution: {'Useful': 20, 'Unhelpful': 8, 'Invalid': 4}\n",
            "Percentage Useful: 62%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Is it actually the case that working with our friends in the Middel East is necessary and Trump has been very dismissive of this? Is there evidence for this claim?\n",
            "    - Is it actually the case that Clinton has been a senator and has done a lot? Is there evidence for this claim?\n",
            "    - Is it actually the case that Clinton has been a Secreatary of State and has done a lot? Is there evidence for this claim?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Is it actually the case that the USA has other problems with Iran and that it has to look at the entire global situation? Is there evidence for this claim?\n",
            "    - Is it the case that they have different perspectives on what's best for growing the economy, or is there room for doubt?\n",
            "    - Is it the case that Trump doesn't release their taxes, or is there room for doubt?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Is it the case that there have been 500 murders during this period, or is there room for doubt?\n",
            "    - Is it possible for the particular case of this period that there have been a lot of murders is not the case?\n",
            "    - Is it possible for the particular case of Trump that it is not the case that he doesn't?\n",
            "\n",
            "=== Cluster 5 ===\n",
            "Total Questions: 35\n",
            "Label Distribution: {'Useful': 15, 'Unhelpful': 14, 'Invalid': 6}\n",
            "Percentage Useful: 43%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - If Trump has a cavalier attitude toward nuclear weapons, might a nuclear war happen? What evidence supports this claim?\n",
            "    - If we can build a new modern electric grid, will creating a lot of new jobs occur? What evidence supports this claim? And how likely are the consequences?\n",
            "    - If we can build a new modern electric grid, will a lot of new economic activity occur? What evidence supports this claim? And how likely are the consequences?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - If Donald insults Muslims, will they not be on the front lines anymore? What evidence supports this claim? And how likely are the consequences?\n",
            "    - If Donald Trump insults Muslims abroad and at home, will they not cooperate with us and provide information that we can't get elsewhere? What evidence supports this claim? How likely are the consequences?\n",
            "    - If we are not precise in how we talk about what candidates are going to do, will we not be able to be relyable to people around the world? What evidence supports this claim? And how likely are the consequences?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - If Clinton voted for every sanction against Iran, will the toughest sanctions on Iran occur? What evidence supports this claim? And how likely are the consequences?\n",
            "    - Is it plausibly the case that there won't community policing? What evidence supports this claim?\n",
            "    - If the Russians hack Americans, might we have to engage in warfare? What evidence supports this claim?\n",
            "\n",
            "=== Cluster 6 ===\n",
            "Total Questions: 93\n",
            "Label Distribution: {'Useful': 33, 'Unhelpful': 52, 'Invalid': 8}\n",
            "Percentage Useful: 35%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Are there other relevant goals that conflict with supporting people who are struggling to balance family and work?\n",
            "    - Are there other relevant goals that conflict with the goal of Clinton holding the same standards as she looks at all of these trade deals?\n",
            "    - Are there other relevant goals that conflict with making the wealthy and corporations pay their fair share to support this country?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Are there other relevant goals that conflict with vacuuming up intelligence from Europe and the Middle East?\n",
            "    - Are there other relevant goals that conflict with Iran not having nuclear bombs?\n",
            "    - Are there other relevant goals that conflict with trading with the other 95 percent of the world?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Are there other relevant goals that conflict with being better off and growing better?\n",
            "    - Are there other relevant goals that conflict with inclusive growth in America?\n",
            "    - Are there other relevant goals that conflict with coming forward with a plan that is going to divert people from the criminal justice system?\n",
            "\n",
            "=== Cluster 7 ===\n",
            "Total Questions: 94\n",
            "Label Distribution: {'Useful': 49, 'Unhelpful': 35, 'Invalid': 10}\n",
            "Percentage Useful: 52%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Are there alternative actions to working more closely with the USA's allies to achieve vacuuming up intelligence from Europe and the Middle East? If so, which is the most efficient action?\n",
            "    - Are there alternative actions to having paid family leave and earned sick days to support people who are struggling to balance family and work? If so, which is the most efficient action?\n",
            "    - Are there alternative actions to USA sanctioning Iran to achieve Iran not having nuclear bombs? If so, which is the most efficient action?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Are there alternative actions to making banks take more risk and give more money out to achieve making banks invest productively? If so, which is the most efficient action?\n",
            "    - Are there alternative actions to getting guns out of the hands of people who should not have them to achieve stopping death of young African-American men caused by the gun epidemic? If so, which is the most efficient action?\n",
            "    - Are there alternative actions to dealing with mandatory minimum sentences to achieve coming forward with a plan that is going to divert people from the criminal justice system? If so, which is the most efficient action?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Are there alternative actions to DOT setting a maximun tarmac delay trigger to achieve allowing passengers to deplane during gate holds? If so, which is the most efficient action?\n",
            "    - Are there alternative actions to deplaneing without CBP screening to achieve allowing passengers to deplane during gate holds? If so, which is the most efficient action?\n",
            "    - Are there alternative actions to fixing these matters in a bipartisan way to achieve dealing with mandatory minimum sentences? If so, which is the most efficient action?\n",
            "\n",
            "=== Cluster 8 ===\n",
            "Total Questions: 91\n",
            "Label Distribution: {'Useful': 63, 'Unhelpful': 20, 'Invalid': 8}\n",
            "Percentage Useful: 69%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Could working more closely with the USA's allies have consequences that we should take into account? Is it practically possible?\n",
            "    - Could having paid family leave and earned sick days have consequences that we should take into account? Is it practically possible?\n",
            "    - Could Clinton voting against the biggest deal, a multinational one known as CAFTA, have consequences that we should take into account? Is it practically possible?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Could USA sanctioning Iran have consequences that we should take into account? Is it practically possible?\n",
            "    - Are there other consequences of preventing Trump's cavalier attitude toward nuclear weapons that we should take into account?\n",
            "    - Could having smart, fair trade deals have consequences that we should take into account? Is it practically possible?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Could China's entry into North Korea have consequences that we should take into account? Is it practically possible?\n",
            "    - Could providing passengers with air conditioning, restrooms that are functioning properly and beverages and snacks on board have consequences that we should take into account? Is it practically possible?\n",
            "    - Could presenting rather compelling arguments have consequences that we should take into account? Is it practically possible?\n",
            "\n",
            "=== Cluster 9 ===\n",
            "Total Questions: 179\n",
            "Label Distribution: {'Useful': 88, 'Unhelpful': 61, 'Invalid': 30}\n",
            "Percentage Useful: 49%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Is it actually the case that the current political situation requires working closely with NATO and our allies and Trump is desistive of working with our allies? Is there evidence for this claim?\n",
            "    - Are there any events other than Iranians having stocked them with centrifuges that were whirling away that would more reliably account for Iranians having mastered the nuclear fuel cycle?\n",
            "    - Are there any events other than Iranians building covert facilities that would more reliably account for Iranians having mastered the nuclear fuel cycle?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Is it practically possible taking nuclear weapons seriously?\n",
            "    - Is it bad for terrorists to get their hands on nuclear material? Why and to whom is it bad?\n",
            "    - Are there other consequences of taking nuclear weapons seriously?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Why is a nuclear war a danger? To whom is a nuclear war a danger?\n",
            "    - Is there a way of preventing Trump from having a cavalier attitude about nuclear weapons?\n",
            "    - If so, will a subject that sees having new jobs, with rising incomes, investments, and not more tax cuts as not positive agree that retaining (or retracting) commitment to get the economy going again is a good idea?\n",
            "\n",
            "=== Cluster 10 ===\n",
            "Total Questions: 11\n",
            "Label Distribution: {'Useful': 0, 'Unhelpful': 3, 'Invalid': 8}\n",
            "Percentage Useful: 0%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Is Clinton's reliability relevant in the current dialogue?\n",
            "    - Is Clinton's reliability relevant in the current dialogue?\n",
            "    - Is Clinton's reliability relevant in the current dialogue?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Is Clinton's reliability relevant in the current dialogue?\n",
            "    - Is Clinton's reliability relevant in the current dialogue?\n",
            "    - Is Clinton's reliability relevant in the current dialogue?\n",
            "\n",
            "=== Cluster 11 ===\n",
            "Total Questions: 38\n",
            "Label Distribution: {'Useful': 13, 'Unhelpful': 12, 'Invalid': 13}\n",
            "Percentage Useful: 34%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Does the Iran deal imply lead to nuclear problems?\n",
            "    - Does running for President of the USA imply she should have business ability?\n",
            "    - Does running for President of the USA imply she should have a business ability?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Does  running the country imply paying taxes?\n",
            "    - Does All of the things that she 's talking about imply taking care of it?\n",
            "    - Does being the President of the USA imply taking care of the unexpected issues?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Does the matter of agreement on what's best for growing the economy require Trump to take evidence on many sides?\n",
            "    - Does the matter of what's best for the growing economy require Trump to take evidence on many sides?\n",
            "    - Does the matter of the tax proposal require Trump to take evidence on many sides?\n",
            "\n",
            "=== Cluster 12 ===\n",
            "Total Questions: 34\n",
            "Label Distribution: {'Useful': 4, 'Unhelpful': 11, 'Invalid': 19}\n",
            "Percentage Useful: 12%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - If these people's advice is not quoted, does it look like important information or qualifications may have been left out?\n",
            "    - Is paying federal income tax not consistent with other matters Trump stands for? If so, does this inconsistency decrease Trump's credibility?\n",
            "    - Is taking care of USA economy not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - If the study's advice is not quoted, does it look like important information or qualifications may have been left out?\n",
            "    - Is paying taxes not consistent with other matters Trump stands for? If so, does this inconsistency decrease Trump's credibility?\n",
            "    - Is the fact that Clinton did not defeat ISIS when Clinton could inconsistent with the fact that Clinton has written in Clinton's website how to defeat ISIS? If so, does this make Clinton less credible?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Is lead to nuclear problems not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "    - Is the fact that Obama and Clinton did not do anything with respect to Yemen and all theses other places when the Iran deal was taking place inconsistent with the fact that Obama and Clinton are going to do something in the future about Yemen and all these other places? If so, does this make Obama and Clinton less credible?\n",
            "    - Is driving business out not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "\n",
            "=== Cluster 13 ===\n",
            "Total Questions: 40\n",
            "Label Distribution: {'Useful': 15, 'Unhelpful': 19, 'Invalid': 6}\n",
            "Percentage Useful: 38%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - What other consequences should also be taken into account if Clinton voted for every sanction against Iran?\n",
            "    - What other consequences should also be taken into account if we can build a new modern electric grid?\n",
            "    - Are these people pronouncements directly quoted? If not, is a reference to the original source given? Can it be checked?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - What other consequences should also be taken into account if Donald Trump insults Muslims abroad and at home?\n",
            "    - What other consequences should also be taken into account if we are precise in how we talk about what candidates are going to do?\n",
            "    - If so, can the practical inconsistency between Trump's commitments and \"paying federal income tax\" be identified? Can it be shown by evidence?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - What other consequences should also be taken into account if Donald does not insult Muslims?\n",
            "    - What other consequences should also be taken into account if Trump does not forward the biggest tax cuts for the top percent of the people in this country we've ever had?\n",
            "    - What other consequences should also be taken into account if not there is nobody in the USA government to fight them?\n",
            "\n",
            "=== Cluster 14 ===\n",
            "Total Questions: 34\n",
            "Label Distribution: {'Useful': 13, 'Unhelpful': 9, 'Invalid': 12}\n",
            "Percentage Useful: 38%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Is there a proven relation between 'Iranians had stocked them with centrifuges that were whirling away' and 'Iranians had mastered the nuclear fuel cycle?'\n",
            "    - Is there a proven relation between 'Iranians have built covert facilities' and 'Iranians have mastered the nuclear fuel cycle '?\n",
            "    - Is there a proven relation between 'corruption' and 'people that we were going to deport for good reason ended up becoming citizens'?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Is there a proven relation between situations in which people \"don't release their taxes\" and situations in which people \"may have reasons\"?\n",
            "    - Is there a proven relation between situations in which \"does not do what it needs to do\" and situations in which \"it's ineffective\"?\n",
            "    - Is there a proven relation between situations in which \"it has been proved that Trump supported the invasion of Iraq\" and situations in which \"Trump supported the invasion of Iraq\"?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Is there a proven relationship between situations in which \"have different perspectives on what's best for growing the economy\" and situations in which \"have different perspectives on how they make investments that will actually produce jobs and rising incomes\"?\n",
            "    - Is there a proven relationship between situations in which someone \"printed drapery fabrics on long tables, where they pulled out those fabrics and went down with a slikscreen and dumped the paint in and took the squeegee and kept going\" and situations in which someone \"worked really hard\"?\n",
            "    - Is there a proven relation between situations in which \"wants to see private prisons ended in the state system\" and situations in which \"is glad that USA is ending private prisons in the federal system\"?\n",
            "\n",
            "=== Cluster 15 ===\n",
            "Total Questions: 45\n",
            "Label Distribution: {'Useful': 13, 'Unhelpful': 15, 'Invalid': 17}\n",
            "Percentage Useful: 29%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Are there alternative ways of showing that \"Clinton holt the same standards as she looked at differnt trade deals\" than \"Clinton voting against the biggest deal, a multinational one known as CAFTA\"?\n",
            "    - Even if we know that slashing taxes on the wealthy hasn't worked is generally accepted as true, are there any good reasons for doubting that it is true?\n",
            "    - Even if slashing taxes on the wealthy hasn't worked is generally accepted as true, are there any good reasons for doubting that it is true?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - What evidence supports that having to face difficult choices and being under stress is generally accepted as true?\n",
            "    - Is profit-sharing seen as good for most people?\n",
            "    - Even if having to face difficult choices and being under stress is generally accepted as true, are there any good reasons for doubting that it is true in this situation?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Are police officers in a position to know whether reform should happen? Are communities an honest (trustworthy, reliable) source?\n",
            "    - Did communities assert that there are ways to remedy some of the problems we have in the criminal justice system?\n",
            "    - What actions or other indications show that the people of New York accept that working with communities, faith communities, business communities, as well as the police is the right thing to do?\n",
            "\n",
            "=== Cluster 16 ===\n",
            "Total Questions: 42\n",
            "Label Distribution: {'Useful': 20, 'Unhelpful': 20, 'Invalid': 2}\n",
            "Percentage Useful: 48%\n",
            "\n",
            "Sample Questions:\n",
            "\n",
            "  Useful Questions:\n",
            "    - Are there other factors in this particular case that could have interfered with the event of the 'USA driving Iranians to the negotiation table'?\n",
            "    - Are there other factors that could interfere with or counteract the production of the effect 'the USA can have enough clean energy to power every home' in this case?\n",
            "    - Are there other factors that could interfere with or counteract the production of the effect 'we can have enough clean energy to power every home' in this case?\n",
            "\n",
            "  Unhelpful Questions:\n",
            "    - Are there other factors in this particular case that could have interfered with the event of 'race is a significant challenge in our country'?\n",
            "    - Are there other factors in this particular case that could have interfered with the event of 'there being too many military-style weapons on the streets'?\n",
            "    - Are there other factors that could interfere with or counteract the production of the effect 'a situation like the Great Recession' in this case?\n",
            "\n",
            "  Invalid Questions:\n",
            "    - Are there other factors in this particular case that could have interfered with the event of 'TRUMP is very proud of the endorsement of the NRA?'\n",
            "    - Are there other factors in this particular case that could have interfered with the event of 'Trump being proud of the NRA's endorsement'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Theoretical CQs\n",
        "\n",
        "for cluster_id, cluster_data in theoretical_clusters.items():\n",
        "    print(f\"\\n=== Cluster {cluster_id} ===\")\n",
        "    print(f\"Total Questions: {cluster_data['metadata']['tot_questions']}\")\n",
        "    print(f\"Label Distribution: {cluster_data['metadata']['labels']}\")\n",
        "    print(f\"Percentage Useful: {cluster_data['metadata']['percentage_useful'] * 100:.0f}%\")\n",
        "    print(\"\\nSample Questions:\")\n",
        "    for question in cluster_data[\"questions\"][:10]:\n",
        "        print(f\"- {question}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUwgbCnkoRU9",
        "outputId": "5157e82f-6d4b-418a-a348-88d12db00403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cluster 0 ===\n",
            "Total Questions: 6\n",
            "Label Distribution: {'Useful': 0, 'Unhelpful': 2, 'Invalid': 4}\n",
            "Percentage Useful: 0%\n",
            "\n",
            "Sample Questions:\n",
            "- Is what these people said clear? Are there technical terms used that are not explained clearly?\n",
            "- Is what the study said clear? Are there technical terms used that are not explained clearly?\n",
            "- Is what Simon Rose said clear? Are there technical terms used that are not explained clearly?\n",
            "- Is what the report said clear? Are there technical terms used that are not explained clearly?\n",
            "- Is what the police said clear? Are there technical terms used that are not explained clearly?\n",
            "- Is what the Fraternal Order of Police said clear? Are there technical terms used that are not explained clearly?\n",
            "\n",
            "=== Cluster 1 ===\n",
            "Total Questions: 109\n",
            "Label Distribution: {'Useful': 39, 'Unhelpful': 57, 'Invalid': 13}\n",
            "Percentage Useful: 36%\n",
            "\n",
            "Sample Questions:\n",
            "- Is the current political situation actually a typical case of other political situations that require working closely with NATO and our allies? How widely applicable is the generalization?\n",
            "- Is working with our friends in the Mideast actually a typical case of other political moves that are necessary and Trump is very dismissive of? How widely applicable is the generalization?\n",
            "- How strong is the generalization that if Clinton achieved putting together a coalition to impose tough sanctions on Iran, then the USA would drive Iranians to the negotiation table?\n",
            "- Is the USA actually a typical case of other countries that have other problems than Iran? How widely applicable is the generalization?\n",
            "- How strong is the generalization that if we deploy half a billion more solar panels there will be enough clean energy to power every home?\n",
            "- Is China's exports actually a typical case of other countries' exports that Clinton has been working on? How widely applicable is the generalization?\n",
            "- Is American exports actually a typical case of other relevant statistics, such as new jobs, that have imrpoved under Clinton's leadership? How widely applicable is the generalization?\n",
            "- Is Clinton actually a typical case of other people that have been Secreatary of State? How widely applicable is the generalization?\n",
            "- Is Clinton actually a typical case of other people that have been a senator? How widely applicable is the generalization?\n",
            "- Is Clinton's plan actually a typical case of other plans that include helping families balance the responsibilities at home and at business? How widely applicable is the generalization?\n",
            "\n",
            "=== Cluster 2 ===\n",
            "Total Questions: 74\n",
            "Label Distribution: {'Useful': 18, 'Unhelpful': 43, 'Invalid': 13}\n",
            "Percentage Useful: 24%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there special circumstances pertaining to the current political situation that undermine its generalisability to other political situations that require working closely with NATO and our allies?\n",
            "- Are there special circumstances pertaining to working with our friends in the Middel East that undermine its generalisability to other political moves that are necessary?\n",
            "- Are there special circumstances pertaining to the USA that undermine its generalisability to other countries that have other problems than Iran?\n",
            "- Are there special circumstances pertaining to China's exports that undermine its generalisability to other countries' exports that Clinton has been working on?\n",
            "- Are there special circumstances pertaining to American exports that undermine its generalisability to other relevant statistics, such as new jobs, that have improved under Clinton's leadership?\n",
            "- Are there special circumstances pertaining to Clinton that undermine its generalisability to other people that have been Secretary of State?\n",
            "- Are there special circumstances pertaining to Clinton that undermine its generalisability to other people that have been a senator?\n",
            "- Are there special circumstances pertaining to Clinton's plans that undermine its generalisability to other plans that include helping families balance the responsibilities at home and at business?\n",
            "- Are there special circumstances pertaining to the current plans of strong growth, fair growth, and sustained growth that undermine their generalisability to other plans that are led by Clinton?\n",
            "- Are there special circumstances pertaining to Trump that undermine its generalisability to other people that started their business with millions borrowed from their father?\n",
            "\n",
            "=== Cluster 3 ===\n",
            "Total Questions: 35\n",
            "Label Distribution: {'Useful': 12, 'Unhelpful': 13, 'Invalid': 10}\n",
            "Percentage Useful: 34%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there reasons to believe that profit-sharing is not good in this situation?\n",
            "- Will a subject that sees profit-sharing as not good agree with retaining commitment to \"improving labour benefits\"?\n",
            "- Are these people a genuine experts in economics?\n",
            "- Is this study done by genuine experts in peanut allergies? Is the claim that \"peanut allergens can be eluted from ventilation system filters in commercial airliners\" relevant to domain peanut allergies?\n",
            "- Did the study really assert that peanut allergens can be eluted from ventilation system filters in commercial airliners?\n",
            "- Is the claim that \"peanut allergens can be eluted from ventilation system filters in commercial airliners\" consistent with what other experts in peanut allergies say? Is it consistent with known evidence in peanut allergies?\n",
            "- Did Trump support the actions we took in Libya?\n",
            "- How strong is the generalization that if law enforcement responds quickly and professionally, then Clinton will be proud of law enforcement?\n",
            "- Is the fact that there have been 100 billion pounds transfered from savers to borrowers consistent with known evidence in economy?\n",
            "- Is Simon Rose a genuine expert in economy?\n",
            "\n",
            "=== Cluster 4 ===\n",
            "Total Questions: 32\n",
            "Label Distribution: {'Useful': 20, 'Unhelpful': 8, 'Invalid': 4}\n",
            "Percentage Useful: 62%\n",
            "\n",
            "Sample Questions:\n",
            "- Is it actually the case that working with our friends in the Middel East is necessary and Trump has been very dismissive of this? Is there evidence for this claim?\n",
            "- Is it actually the case that the USA has other problems with Iran and that it has to look at the entire global situation? Is there evidence for this claim?\n",
            "- Is it actually the case that Clinton has been a senator and has done a lot? Is there evidence for this claim?\n",
            "- Is it actually the case that Clinton has been a Secreatary of State and has done a lot? Is there evidence for this claim?\n",
            "- Is it the case that they have different perspectives on what's best for growing the economy, or is there room for doubt?\n",
            "- Is it the case that Trump doesn't release their taxes, or is there room for doubt?\n",
            "- Is it the case that stop-and-frisk does not do what it needs to do, or is there room for doubt?\n",
            "- Is it the case that Clinton wants to see private prisons end in the state system, or is there room for doubt?\n",
            "- Is it the case that this time it has been proved that Trump supported the invasion of Iraq, or is there room for doubt?\n",
            "- Is it the case that defeating ISIS is getting tougher and tougher, or is there room for doubt?\n",
            "\n",
            "=== Cluster 5 ===\n",
            "Total Questions: 35\n",
            "Label Distribution: {'Useful': 15, 'Unhelpful': 14, 'Invalid': 6}\n",
            "Percentage Useful: 43%\n",
            "\n",
            "Sample Questions:\n",
            "- If Donald insults Muslims, will they not be on the front lines anymore? What evidence supports this claim? And how likely are the consequences?\n",
            "- If Donald Trump insults Muslims abroad and at home, will they not cooperate with us and provide information that we can't get elsewhere? What evidence supports this claim? How likely are the consequences?\n",
            "- If Clinton voted for every sanction against Iran, will the toughest sanctions on Iran occur? What evidence supports this claim? And how likely are the consequences?\n",
            "- If Trump has a cavalier attitude toward nuclear weapons, might a nuclear war happen? What evidence supports this claim?\n",
            "- If we are not precise in how we talk about what candidates are going to do, will we not be able to be relyable to people around the world? What evidence supports this claim? And how likely are the consequences?\n",
            "- If we can build a new modern electric grid, will creating a lot of new jobs occur? What evidence supports this claim? And how likely are the consequences?\n",
            "- If we can build a new modern electric grid, will a lot of new economic activity occur? What evidence supports this claim? And how likely are the consequences?\n",
            "- How strong is the generalization that if the USA can build a new modern electric grid, the USA will have enough clean energy to power every home?\n",
            "- If Trump puts forward the biggest tax cuts for the top percent of the people in this country we've ever had, will it be trickle-down economics all over again? What evidence supports this claim? And how likely are the consequences?\n",
            "- Is it plausible that Trump does not have proposals to bring back the money that's stranded overseas? What evidence supports this claim?\n",
            "\n",
            "=== Cluster 6 ===\n",
            "Total Questions: 93\n",
            "Label Distribution: {'Useful': 33, 'Unhelpful': 52, 'Invalid': 8}\n",
            "Percentage Useful: 35%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there other relevant goals that conflict with vacuuming up intelligence from Europe and the Middle East?\n",
            "- Are there other relevant goals that conflict with supporting people who are struggling to balance family and work?\n",
            "- Are there other relevant goals that conflict with Iran not having nuclear bombs?\n",
            "- Are there other relevant goals that conflict with the goal of Clinton holding the same standards as she looks at all of these trade deals?\n",
            "- Are there other relevant goals that conflict with making the wealthy and corporations pay their fair share to support this country?\n",
            "- Are there other relevant goals that conflict with trading with the other 95 percent of the world?\n",
            "- Are there other relevant goals that conflict with being better off and growing better?\n",
            "- Are there other relevant goals that conflict with inclusive growth in America?\n",
            "- Are there other relevant goals that conflict with allowing passengers to deplane during gate holds?\n",
            "- Are there other relevant goals that conflict with making banks invest productively?\n",
            "\n",
            "=== Cluster 7 ===\n",
            "Total Questions: 94\n",
            "Label Distribution: {'Useful': 49, 'Unhelpful': 35, 'Invalid': 10}\n",
            "Percentage Useful: 52%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there alternative actions to working more closely with the USA's allies to achieve vacuuming up intelligence from Europe and the Middle East? If so, which is the most efficient action?\n",
            "- Are there alternative actions to having paid family leave and earned sick days to support people who are struggling to balance family and work? If so, which is the most efficient action?\n",
            "- Are there alternative actions to USA sanctioning Iran to achieve Iran not having nuclear bombs? If so, which is the most efficient action?\n",
            "- Are there alternative actions to raising taxes on the wealthy to achieve the goal of making the wealthy and corporations pay their fair share to support this country? If so, which is the most efficient action?\n",
            "- Are there alternative actions to having smart, fair trade deals to achieve trading with the other 95 percent of the world? If so, which is the most efficient action?\n",
            "- Are there alternative actions to doing more for the middle class, investing more in people, their education, their skills, and their future to achieve being better off and growing better? If so, which is the most efficient action?\n",
            "- Are there alternative actions to not giving advantages for people at the very top to achieve inclusive growth? If so, which is the most efficient action?\n",
            "- Are there alternative actions to DOT setting a maximun tarmac delay trigger to achieve allowing passengers to deplane during gate holds? If so, which is the most efficient action?\n",
            "- Are there alternative actions to deplaneing without CBP screening to achieve allowing passengers to deplane during gate holds? If so, which is the most efficient action?\n",
            "- Are there alternative actions to making banks take more risk and give more money out to achieve making banks invest productively? If so, which is the most efficient action?\n",
            "\n",
            "=== Cluster 8 ===\n",
            "Total Questions: 91\n",
            "Label Distribution: {'Useful': 63, 'Unhelpful': 20, 'Invalid': 8}\n",
            "Percentage Useful: 69%\n",
            "\n",
            "Sample Questions:\n",
            "- Could working more closely with the USA's allies have consequences that we should take into account? Is it practically possible?\n",
            "- Could having paid family leave and earned sick days have consequences that we should take into account? Is it practically possible?\n",
            "- Could USA sanctioning Iran have consequences that we should take into account? Is it practically possible?\n",
            "- Are there other consequences of preventing Trump's cavalier attitude toward nuclear weapons that we should take into account?\n",
            "- Could Clinton voting against the biggest deal, a multinational one known as CAFTA, have consequences that we should take into account? Is it practically possible?\n",
            "- Could raising taxes on the wealthy have consequences that we should take into account? Is it practically possible?\n",
            "- Could having smart, fair trade deals have consequences that we should take into account? Is it practically possible?\n",
            "- Could doing more for the middle class, investing more in people, their education, their skills, and their future have consequences that we should take into account? Is it practically possible?\n",
            "- Could not giving advantages for people at the very top have consequences that we should take into account? Is it practically possible?\n",
            "- Could deplaneing without CBP screening have consequences that we should take into account? Is it practically possible?\n",
            "\n",
            "=== Cluster 9 ===\n",
            "Total Questions: 179\n",
            "Label Distribution: {'Useful': 88, 'Unhelpful': 61, 'Invalid': 30}\n",
            "Percentage Useful: 49%\n",
            "\n",
            "Sample Questions:\n",
            "- Is it actually the case that the current political situation requires working closely with NATO and our allies and Trump is desistive of working with our allies? Is there evidence for this claim?\n",
            "- Are there any events other than Iranians having stocked them with centrifuges that were whirling away that would more reliably account for Iranians having mastered the nuclear fuel cycle?\n",
            "- Are there any events other than Iranians building covert facilities that would more reliably account for Iranians having mastered the nuclear fuel cycle?\n",
            "- Is it practically possible taking nuclear weapons seriously?\n",
            "- Is it bad for terrorists to get their hands on nuclear material? Why and to whom is it bad?\n",
            "- Are there other consequences of taking nuclear weapons seriously?\n",
            "- Why is a nuclear war a danger? To whom is a nuclear war a danger?\n",
            "- Is taking nuclear weapons seriously a way to prevent terrorists from getting their hands on nuclear material?\n",
            "- Is there a way of preventing Trump from having a cavalier attitude about nuclear weapons?\n",
            "- Is it actually the case that Clinton has been working on China's exports, and consequently exports have increased 50 percent and helped create new jobs? Is there evidence for this claim?\n",
            "\n",
            "=== Cluster 10 ===\n",
            "Total Questions: 11\n",
            "Label Distribution: {'Useful': 0, 'Unhelpful': 3, 'Invalid': 8}\n",
            "Percentage Useful: 0%\n",
            "\n",
            "Sample Questions:\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is Clinton's reliability relevant in the current dialogue?\n",
            "- Is the reliability of money borrowers relevant in the current dialogue?\n",
            "- How does the allegation made affect the reliability of money borrowers?\n",
            "- How does the allegation made affect the reliability of Clinton?\n",
            "\n",
            "=== Cluster 11 ===\n",
            "Total Questions: 38\n",
            "Label Distribution: {'Useful': 13, 'Unhelpful': 12, 'Invalid': 13}\n",
            "Percentage Useful: 34%\n",
            "\n",
            "Sample Questions:\n",
            "- Does the matter of agreement on what's best for growing the economy require Trump to take evidence on many sides?\n",
            "- Does the matter of what's best for the growing economy require Trump to take evidence on many sides?\n",
            "- Does the matter of the tax proposal require Trump to take evidence on many sides?\n",
            "- Does running the country imply paying taxes?\n",
            "- Does he is rich and charitable imply paying federal income tax?\n",
            "- Does  running the country imply paying taxes?\n",
            "- Does \"he is rich and charitable\" imply paying federal income tax?\n",
            "- Does the Iran deal imply lead to nuclear problems?\n",
            "- Does the Iran deal imply not dealing with nuclear problems?\n",
            "- Does All of the things that she 's talking about imply taking care of it?\n",
            "\n",
            "=== Cluster 12 ===\n",
            "Total Questions: 34\n",
            "Label Distribution: {'Useful': 4, 'Unhelpful': 11, 'Invalid': 19}\n",
            "Percentage Useful: 12%\n",
            "\n",
            "Sample Questions:\n",
            "- If these people's advice is not quoted, does it look like important information or qualifications may have been left out?\n",
            "- If the study's advice is not quoted, does it look like important information or qualifications may have been left out?\n",
            "- Is paying federal income tax not consistent with other matters Trump stands for? If so, does this inconsistency decrease Trump's credibility?\n",
            "- Is paying taxes not consistent with other matters Trump stands for? If so, does this inconsistency decrease Trump's credibility?\n",
            "- Is lead to nuclear problems not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "- Is the fact that Obama and Clinton did not do anything with respect to Yemen and all theses other places when the Iran deal was taking place inconsistent with the fact that Obama and Clinton are going to do something in the future about Yemen and all these other places? If so, does this make Obama and Clinton less credible?\n",
            "- Is the fact that Clinton did not defeat ISIS when Clinton could inconsistent with the fact that Clinton has written in Clinton's website how to defeat ISIS? If so, does this make Clinton less credible?\n",
            "- Is taking care of it not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "- Is she should have a business ability not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "- Is taking care of USA economy not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "\n",
            "=== Cluster 13 ===\n",
            "Total Questions: 40\n",
            "Label Distribution: {'Useful': 15, 'Unhelpful': 19, 'Invalid': 6}\n",
            "Percentage Useful: 38%\n",
            "\n",
            "Sample Questions:\n",
            "- What other consequences should also be taken into account if Donald Trump insults Muslims abroad and at home?\n",
            "- What other consequences should also be taken into account if Donald does not insult Muslims?\n",
            "- What other consequences should also be taken into account if Clinton voted for every sanction against Iran?\n",
            "- What other consequences should also be taken into account if we are precise in how we talk about what candidates are going to do?\n",
            "- What other consequences should also be taken into account if we can build a new modern electric grid?\n",
            "- Are these people pronouncements directly quoted? If not, is a reference to the original source given? Can it be checked?\n",
            "- What other consequences should also be taken into account if Trump does not forward the biggest tax cuts for the top percent of the people in this country we've ever had?\n",
            "- Is the study's pronouncement directly quoted? If not, is a reference to the original source given? Can it be checked?\n",
            "- If so, can the practical inconsistency between Trump's commitments and \"paying federal income tax\" be identified? Can it be shown by evidence?\n",
            "- What other consequences should also be taken into account if the USA police are using the best training, the best techniques, and are well prepared to use force only when necessary?\n",
            "\n",
            "=== Cluster 14 ===\n",
            "Total Questions: 34\n",
            "Label Distribution: {'Useful': 13, 'Unhelpful': 9, 'Invalid': 12}\n",
            "Percentage Useful: 38%\n",
            "\n",
            "Sample Questions:\n",
            "- Is there a proven relation between 'Iranians had stocked them with centrifuges that were whirling away' and 'Iranians had mastered the nuclear fuel cycle?'\n",
            "- Is there a proven relation between 'Iranians have built covert facilities' and 'Iranians have mastered the nuclear fuel cycle '?\n",
            "- Is there a proven relationship between situations in which \"have different perspectives on what's best for growing the economy\" and situations in which \"have different perspectives on how they make investments that will actually produce jobs and rising incomes\"?\n",
            "- Is there a proven relationship between situations in which someone \"printed drapery fabrics on long tables, where they pulled out those fabrics and went down with a slikscreen and dumped the paint in and took the squeegee and kept going\" and situations in which someone \"worked really hard\"?\n",
            "- Is there a proven relation between situations in which people \"don't release their taxes\" and situations in which people \"may have reasons\"?\n",
            "- Is there a proven relation between situations in which \"does not do what it needs to do\" and situations in which \"it's ineffective\"?\n",
            "- Is there a proven relation between situations in which \"wants to see private prisons ended in the state system\" and situations in which \"is glad that USA is ending private prisons in the federal system\"?\n",
            "- Is there a proven relation between situations in which \"it has been proved that Trump supported the invasion of Iraq\" and situations in which \"Trump supported the invasion of Iraq\"?\n",
            "- Is there a proven relation between situations in which \"is getting tougher and tougher\" and situations in which \"it's a big problem\"?\n",
            "- Is there a proven relation between 'corruption' and 'people that we were going to deport for good reason ended up becoming citizens'?\n",
            "\n",
            "=== Cluster 15 ===\n",
            "Total Questions: 45\n",
            "Label Distribution: {'Useful': 13, 'Unhelpful': 15, 'Invalid': 17}\n",
            "Percentage Useful: 29%\n",
            "\n",
            "Sample Questions:\n",
            "- What evidence supports that having to face difficult choices and being under stress is generally accepted as true?\n",
            "- Is profit-sharing seen as good for most people?\n",
            "- Even if having to face difficult choices and being under stress is generally accepted as true, are there any good reasons for doubting that it is true in this situation?\n",
            "- Are there alternative ways of showing that \"Clinton holt the same standards as she looked at differnt trade deals\" than \"Clinton voting against the biggest deal, a multinational one known as CAFTA\"?\n",
            "- Is having new jobs, with rising incomes, investments, and not more tax cuts seen as positive for most people?\n",
            "- Even if we know that slashing taxes on the wealthy hasn't worked is generally accepted as true, are there any good reasons for doubting that it is true?\n",
            "- Even if slashing taxes on the wealthy hasn't worked is generally accepted as true, are there any good reasons for doubting that it is true?\n",
            "- What evidence supports that slashing taxes on the wealthy hasn't worked is generally accepted as true?\n",
            "- Did the Department of Justice assert that an assault weapon ban won't do shit for addressing homicide rates in the US?\n",
            "- Is the Department of Justice in a position to know whether an assault weapon ban won't do shit for addressing homicide rates in the US? Is the Department of Justice an honest (trustworthy, reliable) source?\n",
            "\n",
            "=== Cluster 16 ===\n",
            "Total Questions: 42\n",
            "Label Distribution: {'Useful': 20, 'Unhelpful': 20, 'Invalid': 2}\n",
            "Percentage Useful: 48%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there other factors in this particular case that could have interfered with the event of the 'USA driving Iranians to the negotiation table'?\n",
            "- Are there other factors that could interfere with or counteract the production of the effect 'the USA can have enough clean energy to power every home' in this case?\n",
            "- Are there other factors that could interfere with or counteract the production of the effect 'we can have enough clean energy to power every home' in this case?\n",
            "- Are there other factors in this particular case that could have interfered with the event of 'both sides of the transaction suffer'?\n",
            "- Are there other factors in this particular case that could have interfered with the event of 'they were the beneficiaries (and now the sufferers) of the policy'?\n",
            "- Are there other factors in this particular case that could have interfered with the event of 'race is a significant challenge in our country'?\n",
            "- Are there other factors in this particular case that could have interfered with the result of 'ending up to be arrested and sent to jail for nonviolent offenses'?\n",
            "- Are there other factors in this particular case that could have interfered with the event of 'crime decreases'?\n",
            "- Are there other factors in this particular case that could have interfered with the event of 'there being too many military-style weapons on the streets'?\n",
            "- Are there other factors that could interfere with or counteract the production of the effect 'a situation like the Great Recession' in this case?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM-generated CQs\n",
        "theoretical_clusters = cluster_cqs_with_labels(llm_cqs, num_clusters=10)"
      ],
      "metadata": {
        "id": "75N4MZKPLWSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM-generated CQs\n",
        "\n",
        "for cluster_id, cluster_data in theoretical_clusters.items():\n",
        "    print(f\"\\n=== Cluster {cluster_id} ===\")\n",
        "    print(f\"Total Questions: {cluster_data['metadata']['tot_questions']}\")\n",
        "    print(f\"Label Distribution: {cluster_data['metadata']['labels']}\")\n",
        "    print(f\"Percentage Useful: {cluster_data['metadata']['percentage_useful'] * 100:.0f}%\")\n",
        "    print(\"\\nSample Questions:\")\n",
        "    for question in cluster_data[\"questions\"][:3]:\n",
        "        print(f\"- {question}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV_gA1W0t6-9",
        "outputId": "d76fed29-fe0c-4d23-bbcf-bed432f684ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cluster 0 ===\n",
            "Total Questions: 304\n",
            "Label Distribution: {'Useful': 232, 'Unhelpful': 40, 'Invalid': 32}\n",
            "Percentage Useful: 76%\n",
            "\n",
            "Sample Questions:\n",
            "- What specific intelligence benefits have been gained from working with European and Middle Eastern allies in the past, and how do these benefits justify increased cooperation?\n",
            "- What specific plans do you have to address the root causes of income inequality, and how would you measure the success of these plans?\n",
            "- What is the track record of the speaker in implementing similar policies in the past, and what were the results?\n",
            "\n",
            "=== Cluster 1 ===\n",
            "Total Questions: 461\n",
            "Label Distribution: {'Useful': 299, 'Unhelpful': 100, 'Invalid': 62}\n",
            "Percentage Useful: 65%\n",
            "\n",
            "Sample Questions:\n",
            "- How does Clinton's proposal compare to Trump's proposal in terms of their potential impact on the economy, and what are the key differences between their approaches?\n",
            "- How do Clinton's statements about NATO and Iran relate to the broader topic of discussion, and what is her main point in bringing up these issues?\n",
            "- What is the relevance of mentioning Article 5 of NATO in this context, and how does it support Clinton's argument?\n",
            "\n",
            "=== Cluster 2 ===\n",
            "Total Questions: 359\n",
            "Label Distribution: {'Useful': 286, 'Unhelpful': 45, 'Invalid': 28}\n",
            "Percentage Useful: 80%\n",
            "\n",
            "Sample Questions:\n",
            "- How would the proposed policies address the root causes of income inequality, rather than just providing temporary benefits?\n",
            "- How would you define \"the wealthy\" and what specific tax reforms would you implement to ensure they pay their \"fair share\", and how would you measure the impact of these reforms?\n",
            "- How would you ensure that the benefits of economic growth are shared equitably among all segments of society, and not just concentrated among the wealthy?\n",
            "\n",
            "=== Cluster 3 ===\n",
            "Total Questions: 149\n",
            "Label Distribution: {'Useful': 109, 'Unhelpful': 27, 'Invalid': 13}\n",
            "Percentage Useful: 73%\n",
            "\n",
            "Sample Questions:\n",
            "- What is the definition of \"help create the profits\" and how does one measure it? Is it based on hours worked, productivity, or some other factor?\n",
            "- Is Clinton's criticism of the person's temperament based on a fair and objective assessment, or is it influenced by political bias or personal animosity?\n",
            "- Is Clinton's claim based on a subjective interpretation of the person's behavior, or is it supported by objective metrics or expert analysis?\n",
            "\n",
            "=== Cluster 4 ===\n",
            "Total Questions: 159\n",
            "Label Distribution: {'Useful': 119, 'Unhelpful': 32, 'Invalid': 8}\n",
            "Percentage Useful: 75%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there other factors that might influence a person's behavior in a given situation? Has Clinton taken these factors into account in her assessment of the person's temperament?\n",
            "- Are there other factors that are more important than temperament in determining a person's fitness to be commander-in-chief?\n",
            "- How does Clinton's plan take into account the potential impact of external factors, such as global economic trends or unforeseen events, on its success?\n",
            "\n",
            "=== Cluster 5 ===\n",
            "Total Questions: 268\n",
            "Label Distribution: {'Useful': 225, 'Unhelpful': 29, 'Invalid': 14}\n",
            "Percentage Useful: 84%\n",
            "\n",
            "Sample Questions:\n",
            "- What are the potential drawbacks or risks of increased cooperation with Muslim nations or communities, and how would Clinton mitigate these risks?\n",
            "- What evidence is there that Donald Trump's rhetoric has led to the alienation of Muslim communities, and how would Clinton's approach to working with these communities be more effective?\n",
            "- How does Clinton define \"working more closely\" with allies, and what specific actions or policies would she implement to achieve this goal?\n",
            "\n",
            "=== Cluster 6 ===\n",
            "Total Questions: 285\n",
            "Label Distribution: {'Useful': 192, 'Unhelpful': 48, 'Invalid': 45}\n",
            "Percentage Useful: 67%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there any counterarguments or alternative perspectives that Clinton is not acknowledging or addressing in her statements about NATO and Iran?\n",
            "- How does Clinton's claim that the deal \"put a lid on Iran's nuclear program without firing a single shot\" align with the actual outcome of the deal, and are there any potential long-term risks or consequences associated with the agreement?\n",
            "- Is there any potential bias or conflict of interest that might affect Clinton's judgment or response?\n",
            "\n",
            "=== Cluster 7 ===\n",
            "Total Questions: 223\n",
            "Label Distribution: {'Useful': 169, 'Unhelpful': 38, 'Invalid': 16}\n",
            "Percentage Useful: 76%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there any potential unintended consequences of a full ban on peanut products on airplanes?\n",
            "- What is the evidence that a ban on peanut products on airlines would actually save lives, and are there any other measures that could be taken to reduce the risk of allergic reactions on flights?\n",
            "- How would a ban on peanut products on airlines be balanced against the rights and preferences of passengers who do not have peanut allergies?\n",
            "\n",
            "=== Cluster 8 ===\n",
            "Total Questions: 376\n",
            "Label Distribution: {'Useful': 339, 'Unhelpful': 24, 'Invalid': 13}\n",
            "Percentage Useful: 90%\n",
            "\n",
            "Sample Questions:\n",
            "- What evidence is there that Donald Trump has been \"dismissive\" of working with allies, and how has this alleged dismissiveness impacted national security?\n",
            "- What evidence is there that the American Muslim community is \"on the front lines\" of providing information to combat terrorism, and how can their role be enhanced and supported?\n",
            "- What is the evidence that increasing taxes on the wealthy would lead to economic growth and improved living standards for the majority of citizens?\n",
            "\n",
            "=== Cluster 9 ===\n",
            "Total Questions: 557\n",
            "Label Distribution: {'Useful': 403, 'Unhelpful': 116, 'Invalid': 38}\n",
            "Percentage Useful: 72%\n",
            "\n",
            "Sample Questions:\n",
            "- Is Clinton's characterization of Trump's views on climate change accurate, and is her own stance on the issue supported by scientific evidence?\n",
            "- Is it accurate to say that NATO only invoked Article 5 after 9/11, and are there any other instances where it was invoked?\n",
            "- What does Clinton mean by \"mastered the nuclear fuel cycle\"? Is this a technical term that can be verified, or is it a subjective judgment?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73892706ef784f39b574e334f0b44fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_5dc7fc71698b46269f4e25933385be88"
          }
        },
        "a0d1159ae9704b3797d431d7af5c2a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcda1596f51d48ab965b3e9f8e974994",
            "placeholder": "​",
            "style": "IPY_MODEL_fcde228d75db483e9e6b35393c216adf",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5f50fefcc51047c69fa8bb24acd0c3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d6f73bdfacdf4c12850b72a2f6e76d77",
            "placeholder": "​",
            "style": "IPY_MODEL_40dcba5cdcc248fb93293ca4806600da",
            "value": ""
          }
        },
        "aeeb1e4dfe4c4b1ca8b24897da06e074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_da4dbbf4e51b40368f4a4e2ab78542eb",
            "style": "IPY_MODEL_460c34dbd6ed4ee882ae4c0b1a8951ae",
            "value": true
          }
        },
        "484ac1d1f15b43e2ba45b9243ceab68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6f8f25c186c049d19d1646bf12207882",
            "style": "IPY_MODEL_05892c518c8b4585a9bd03d2339ec1dd",
            "tooltip": ""
          }
        },
        "f74292e4727e42f7b594abea3e6a381a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61fc76eb1a7408db147cba4730d7266",
            "placeholder": "​",
            "style": "IPY_MODEL_02c9183a56bd432a801ace6f100b11f7",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "5dc7fc71698b46269f4e25933385be88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "bcda1596f51d48ab965b3e9f8e974994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcde228d75db483e9e6b35393c216adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f73bdfacdf4c12850b72a2f6e76d77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40dcba5cdcc248fb93293ca4806600da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da4dbbf4e51b40368f4a4e2ab78542eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460c34dbd6ed4ee882ae4c0b1a8951ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f8f25c186c049d19d1646bf12207882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05892c518c8b4585a9bd03d2339ec1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e61fc76eb1a7408db147cba4730d7266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02c9183a56bd432a801ace6f100b11f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "444433aa3dfc45ce84174528fa953dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_329a93c3e6bb4a0f95b8818bd8ffc574",
            "placeholder": "​",
            "style": "IPY_MODEL_be785dc1fa6d4e7c846358f6a28c579b",
            "value": "Connecting..."
          }
        },
        "329a93c3e6bb4a0f95b8818bd8ffc574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be785dc1fa6d4e7c846358f6a28c579b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1a2fd4ec02345e29e530b76e0aed61d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa246fed1db142efadfaa3b10d826746",
              "IPY_MODEL_1037fe4fcb374f3aa767e512630e148e",
              "IPY_MODEL_84ba40d320124ccba5d7a6b2460e9f0d"
            ],
            "layout": "IPY_MODEL_9c48c91d8fac48759d1a490ffdfc3e33"
          }
        },
        "aa246fed1db142efadfaa3b10d826746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d66a3dd348ca4f799544ecfb9f358610",
            "placeholder": "​",
            "style": "IPY_MODEL_b4771d1f5aca4883bfed97e9b50eb40b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1037fe4fcb374f3aa767e512630e148e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d638548739c48258d9481c8d02eb8f4",
            "max": 50977,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e3cb30c39904dd4bb362600cf056013",
            "value": 50977
          }
        },
        "84ba40d320124ccba5d7a6b2460e9f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb172f7e6db4ab599a0d38cf28a32e2",
            "placeholder": "​",
            "style": "IPY_MODEL_e4fec02962534535b06cb622e301a983",
            "value": " 51.0k/51.0k [00:00&lt;00:00, 4.23MB/s]"
          }
        },
        "9c48c91d8fac48759d1a490ffdfc3e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d66a3dd348ca4f799544ecfb9f358610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4771d1f5aca4883bfed97e9b50eb40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d638548739c48258d9481c8d02eb8f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e3cb30c39904dd4bb362600cf056013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cb172f7e6db4ab599a0d38cf28a32e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4fec02962534535b06cb622e301a983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c93f25216cb94e33915cf19ccee77283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a7dfb4f40264dfa96c948d25f79d9b1",
              "IPY_MODEL_dc291016b0a548e4a49daecc9e7175d0",
              "IPY_MODEL_7ad7b718986a422cba5b578074ce8667"
            ],
            "layout": "IPY_MODEL_304ade0b31ee456b848202c0f869defb"
          }
        },
        "6a7dfb4f40264dfa96c948d25f79d9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7def69f300b4754bad50579daf53603",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9b38ae020d4a5c9ee904a91fd5989e",
            "value": "tokenizer.json: 100%"
          }
        },
        "dc291016b0a548e4a49daecc9e7175d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35d79aa9809845e9bacfd93c36dbb95e",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0e2680caf2c4db9b0e0038e2c35cd16",
            "value": 9085698
          }
        },
        "7ad7b718986a422cba5b578074ce8667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5cfad6fb6841a4a093b78564bfc60b",
            "placeholder": "​",
            "style": "IPY_MODEL_794a6342cb94404d8a4e70955aad7920",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 42.7MB/s]"
          }
        },
        "304ade0b31ee456b848202c0f869defb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7def69f300b4754bad50579daf53603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9b38ae020d4a5c9ee904a91fd5989e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35d79aa9809845e9bacfd93c36dbb95e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e2680caf2c4db9b0e0038e2c35cd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a5cfad6fb6841a4a093b78564bfc60b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794a6342cb94404d8a4e70955aad7920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44486a67da1644baa0deb8361b8aeb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_734aa5d9a03e45bca1a14b86f6a5503b",
              "IPY_MODEL_901a1413556a4ef38b1b929682160ce1",
              "IPY_MODEL_caa1d87fe99d4e2c8de3ae9d9b39ffa0"
            ],
            "layout": "IPY_MODEL_60063d3142ff4d10ae31488f82c1417b"
          }
        },
        "734aa5d9a03e45bca1a14b86f6a5503b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02bf55ebb2524a168cc4c4f3d11476c2",
            "placeholder": "​",
            "style": "IPY_MODEL_971d54ab584c4134b439c255e492a6e7",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "901a1413556a4ef38b1b929682160ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eadfa2825719488e89b332fe645efd27",
            "max": 73,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97a5b930284d45c592dc22972cc1b702",
            "value": 73
          }
        },
        "caa1d87fe99d4e2c8de3ae9d9b39ffa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ce5482f40346c99e9a747873988933",
            "placeholder": "​",
            "style": "IPY_MODEL_634dc50c5e3343f4a1feab88513de6d5",
            "value": " 73.0/73.0 [00:00&lt;00:00, 6.90kB/s]"
          }
        },
        "60063d3142ff4d10ae31488f82c1417b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02bf55ebb2524a168cc4c4f3d11476c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971d54ab584c4134b439c255e492a6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eadfa2825719488e89b332fe645efd27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97a5b930284d45c592dc22972cc1b702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70ce5482f40346c99e9a747873988933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "634dc50c5e3343f4a1feab88513de6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a82795213694bc4aec473dd2f06496a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ba07bf044e344459b0b508b37fe3ff9",
              "IPY_MODEL_ae0684f2aaf547f7bf6e6111e0a56808",
              "IPY_MODEL_9403bf5a72fd4875a179b7ed6b293893"
            ],
            "layout": "IPY_MODEL_61d2946e2b2b47dc9af7c2b1d96ef3b6"
          }
        },
        "6ba07bf044e344459b0b508b37fe3ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef297177e544ce78b7366eba2ea89b2",
            "placeholder": "​",
            "style": "IPY_MODEL_c3ca3655567142108b82b9a999816d35",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ae0684f2aaf547f7bf6e6111e0a56808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbccd0c6228146c2976051a08b6a0e2e",
            "max": 3071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34be573fce29468783ad20c453e9b325",
            "value": 3071
          }
        },
        "9403bf5a72fd4875a179b7ed6b293893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e612b42843d54b9b878eb13aa3ef59a7",
            "placeholder": "​",
            "style": "IPY_MODEL_615cf754805f4e828303c42c6cfb611b",
            "value": " 3.07k/3.07k [00:00&lt;00:00, 177kB/s]"
          }
        },
        "61d2946e2b2b47dc9af7c2b1d96ef3b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef297177e544ce78b7366eba2ea89b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ca3655567142108b82b9a999816d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbccd0c6228146c2976051a08b6a0e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34be573fce29468783ad20c453e9b325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e612b42843d54b9b878eb13aa3ef59a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615cf754805f4e828303c42c6cfb611b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69b48d2ac45649ab8cffb257c7b35808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ea4c99287134088914957bf087b9d38",
              "IPY_MODEL_08e8b3973d35479286b91a825e61336c",
              "IPY_MODEL_8023a4d176264e018c99761ebe8e766c"
            ],
            "layout": "IPY_MODEL_e9c079363c67496f9f8c4c99375c443b"
          }
        },
        "1ea4c99287134088914957bf087b9d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ec6f0b4c3bc43dea6502c910a668c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_a6813197451f4ff7b95397aaa56199c3",
            "value": "tokenizer.json: 100%"
          }
        },
        "08e8b3973d35479286b91a825e61336c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fce113bf5042e894cc43d5ec03e1f2",
            "max": 9084480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7dc8539a5d84e29839597bbde224fc4",
            "value": 9084480
          }
        },
        "8023a4d176264e018c99761ebe8e766c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40b4291869f04945aa76e6b59b8bff18",
            "placeholder": "​",
            "style": "IPY_MODEL_6635a82090e145a1b3012950ec2ab268",
            "value": " 9.08M/9.08M [00:00&lt;00:00, 46.3MB/s]"
          }
        },
        "e9c079363c67496f9f8c4c99375c443b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec6f0b4c3bc43dea6502c910a668c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6813197451f4ff7b95397aaa56199c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78fce113bf5042e894cc43d5ec03e1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7dc8539a5d84e29839597bbde224fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40b4291869f04945aa76e6b59b8bff18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6635a82090e145a1b3012950ec2ab268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcc91aaf15f8406fbd570f9d36b59812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fc84112f8914625a7c87299f6ee531d",
              "IPY_MODEL_54afe0e37257406cbc9fe28c22ad18d9",
              "IPY_MODEL_ff661dbf6bdb440f83808f081610da6b"
            ],
            "layout": "IPY_MODEL_0b673d9ce30f4a1dab52919bcbeeea24"
          }
        },
        "1fc84112f8914625a7c87299f6ee531d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d1f9051c6c944519cf3a1ae4ba36775",
            "placeholder": "​",
            "style": "IPY_MODEL_c1b45ba2633d4d29b0274b3d0eec547e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "54afe0e37257406cbc9fe28c22ad18d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_befebc2ec83145c2856bd110bbd097ab",
            "max": 2103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_749bbb2d7b844c2a97932c9b800dc197",
            "value": 2103
          }
        },
        "ff661dbf6bdb440f83808f081610da6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408ad39921454c31ac235ea7ae915121",
            "placeholder": "​",
            "style": "IPY_MODEL_006bfe2a2da34577adc706777184e7e6",
            "value": " 2.10k/2.10k [00:00&lt;00:00, 166kB/s]"
          }
        },
        "0b673d9ce30f4a1dab52919bcbeeea24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d1f9051c6c944519cf3a1ae4ba36775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1b45ba2633d4d29b0274b3d0eec547e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "befebc2ec83145c2856bd110bbd097ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749bbb2d7b844c2a97932c9b800dc197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "408ad39921454c31ac235ea7ae915121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "006bfe2a2da34577adc706777184e7e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c10eb3e14c349e9900a61d16305c3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f7a05d969374c77b6f434fba04f52a6",
              "IPY_MODEL_b0bf57d1653d4f4094efd897430f01ee",
              "IPY_MODEL_5e411a078f084538aa28dfae2a0bfc5b"
            ],
            "layout": "IPY_MODEL_1cce7ad0c8fb40b1a94f7faebf69003e"
          }
        },
        "0f7a05d969374c77b6f434fba04f52a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c578612d1f4de1b3ab794dfc0fdd06",
            "placeholder": "​",
            "style": "IPY_MODEL_2ec22dcbdd4440f6b56ec322e97e8054",
            "value": "tokenizer.model: 100%"
          }
        },
        "b0bf57d1653d4f4094efd897430f01ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8065787ab34b4d45afd19ae73bfa5d66",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47a1c529d38b486bbb702719445f35c7",
            "value": 493443
          }
        },
        "5e411a078f084538aa28dfae2a0bfc5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e216292830d4abfa96acac883368fd7",
            "placeholder": "​",
            "style": "IPY_MODEL_5e3b472bda394ab1af596f48f03d0b9a",
            "value": " 493k/493k [00:00&lt;00:00, 6.61MB/s]"
          }
        },
        "1cce7ad0c8fb40b1a94f7faebf69003e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61c578612d1f4de1b3ab794dfc0fdd06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec22dcbdd4440f6b56ec322e97e8054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8065787ab34b4d45afd19ae73bfa5d66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a1c529d38b486bbb702719445f35c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e216292830d4abfa96acac883368fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3b472bda394ab1af596f48f03d0b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fca878602d8540fe9adcb9a597035101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a2a488b0f69400c9d09409b135d739f",
              "IPY_MODEL_6717b9e99ca04e798fc61efa7ff15955",
              "IPY_MODEL_24aa7f8eafb7490ea9fe372d99925d88"
            ],
            "layout": "IPY_MODEL_a29e1d7014d14033a19c83b04fdb8eb2"
          }
        },
        "7a2a488b0f69400c9d09409b135d739f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dc91eb33b474424b4453931e01ae022",
            "placeholder": "​",
            "style": "IPY_MODEL_64cb66b05845479dadd2d92152ea72a0",
            "value": "tokenizer.json: 100%"
          }
        },
        "6717b9e99ca04e798fc61efa7ff15955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_febe1f95c65448c6b34b2980c26ffa32",
            "max": 1795188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbf82e3328fc49cc9c7cf1c850c353ce",
            "value": 1795188
          }
        },
        "24aa7f8eafb7490ea9fe372d99925d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a749bf1ae743a4a2603ab78322eab6",
            "placeholder": "​",
            "style": "IPY_MODEL_ea1bbae417514f82b709bc76090aed96",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 58.1MB/s]"
          }
        },
        "a29e1d7014d14033a19c83b04fdb8eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc91eb33b474424b4453931e01ae022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64cb66b05845479dadd2d92152ea72a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "febe1f95c65448c6b34b2980c26ffa32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf82e3328fc49cc9c7cf1c850c353ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8a749bf1ae743a4a2603ab78322eab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea1bbae417514f82b709bc76090aed96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32cb44ed4a714cafb626a41c50b5a904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8e195b75d054065b14b4d574ba365ca",
              "IPY_MODEL_775b3affbc8e4b76b25c00e11fced279",
              "IPY_MODEL_15d4a383128a4603853fc1edfbbecd13"
            ],
            "layout": "IPY_MODEL_1f67699aff5e4653964515c91a0cbcae"
          }
        },
        "c8e195b75d054065b14b4d574ba365ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ea844ffc324029ba9e1f9aeb3d13ee",
            "placeholder": "​",
            "style": "IPY_MODEL_39da706f1f7c4d2691c2b96b71de5c11",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "775b3affbc8e4b76b25c00e11fced279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cac6c88d09364597832f570ade2c8699",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37d1871967844fcca438a13e854c317d",
            "value": 414
          }
        },
        "15d4a383128a4603853fc1edfbbecd13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5efea9eb8a9849dba4865877b6d91626",
            "placeholder": "​",
            "style": "IPY_MODEL_af8433da2e5549dd84e3aa74f272f947",
            "value": " 414/414 [00:00&lt;00:00, 30.8kB/s]"
          }
        },
        "1f67699aff5e4653964515c91a0cbcae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58ea844ffc324029ba9e1f9aeb3d13ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39da706f1f7c4d2691c2b96b71de5c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac6c88d09364597832f570ade2c8699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d1871967844fcca438a13e854c317d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5efea9eb8a9849dba4865877b6d91626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8433da2e5549dd84e3aa74f272f947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53df8a0819894845b2b3eb064e01b791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5deab28f41554218bc45525c31885ecc",
              "IPY_MODEL_77dcda9956cb4bd9954dc516da773095",
              "IPY_MODEL_b97d1b411eb24465bd024cc4b5f2e460"
            ],
            "layout": "IPY_MODEL_6ad37350068346e59c0972216e003151"
          }
        },
        "5deab28f41554218bc45525c31885ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e397dd378af4e5f838c929569fe138f",
            "placeholder": "​",
            "style": "IPY_MODEL_b29e1fd1ba4b4d37b5fe8be17218ca8e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "77dcda9956cb4bd9954dc516da773095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2260a200e2484a71b08b8d1c6c2f007b",
            "max": 7305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45fe0b1a61ea44adae8f52984aebe309",
            "value": 7305
          }
        },
        "b97d1b411eb24465bd024cc4b5f2e460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5118c6f77b04829ae1443443e443a38",
            "placeholder": "​",
            "style": "IPY_MODEL_533312f67e604fa1a0b231cc0e418b45",
            "value": " 7.30k/7.30k [00:00&lt;00:00, 444kB/s]"
          }
        },
        "6ad37350068346e59c0972216e003151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e397dd378af4e5f838c929569fe138f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29e1fd1ba4b4d37b5fe8be17218ca8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2260a200e2484a71b08b8d1c6c2f007b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45fe0b1a61ea44adae8f52984aebe309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5118c6f77b04829ae1443443e443a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533312f67e604fa1a0b231cc0e418b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "930b0804f5124eec9402d2be4407139e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df6aab18367e470bb6a40c142089c07c",
              "IPY_MODEL_12ba88a77ec94f5e95e23ba328768236",
              "IPY_MODEL_b7a5c9d658254860a0f112ef8bdee7fd"
            ],
            "layout": "IPY_MODEL_bf889d6aab9943a9a6626d47b3f08b4a"
          }
        },
        "df6aab18367e470bb6a40c142089c07c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9c92cd83d53454b8efcfaf7530df35e",
            "placeholder": "​",
            "style": "IPY_MODEL_6241d47475de49cc88da8e010591cfdb",
            "value": "vocab.json: 100%"
          }
        },
        "12ba88a77ec94f5e95e23ba328768236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d25920e8faac42329a488a2fd3e75ce8",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d5243452c5402685f57a9d3de69f73",
            "value": 2776833
          }
        },
        "b7a5c9d658254860a0f112ef8bdee7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e676823891854656bc56275dad72cf3a",
            "placeholder": "​",
            "style": "IPY_MODEL_09fe71513ded473c95e98499b342dc22",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 38.8MB/s]"
          }
        },
        "bf889d6aab9943a9a6626d47b3f08b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9c92cd83d53454b8efcfaf7530df35e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6241d47475de49cc88da8e010591cfdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d25920e8faac42329a488a2fd3e75ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d5243452c5402685f57a9d3de69f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e676823891854656bc56275dad72cf3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09fe71513ded473c95e98499b342dc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a348584cf0ca4774899d1ace0bfb4610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d59a069e01d64fa3b99c0cb1806885bd",
              "IPY_MODEL_179af4f20a514480bf76ff88cf8ad537",
              "IPY_MODEL_684b8cfea99043f6b7a93bfb9cacb251"
            ],
            "layout": "IPY_MODEL_1a75211c96bb47209e9b99e293965b68"
          }
        },
        "d59a069e01d64fa3b99c0cb1806885bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8e76b2b1784098b4aebf8c2010bb65",
            "placeholder": "​",
            "style": "IPY_MODEL_07cbf3c3388544f6a4b3c96bf079aada",
            "value": "merges.txt: 100%"
          }
        },
        "179af4f20a514480bf76ff88cf8ad537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_980edf69696f4addb51c9ab3a39745d5",
            "max": 1671839,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fbf577b73a7410a946dc8a2d63b70ce",
            "value": 1671839
          }
        },
        "684b8cfea99043f6b7a93bfb9cacb251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c2fa26c22e542b787d98e75a2e646f6",
            "placeholder": "​",
            "style": "IPY_MODEL_fe6ab3ca368f4c87b3b577e05e26165f",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 10.7MB/s]"
          }
        },
        "1a75211c96bb47209e9b99e293965b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8e76b2b1784098b4aebf8c2010bb65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07cbf3c3388544f6a4b3c96bf079aada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "980edf69696f4addb51c9ab3a39745d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fbf577b73a7410a946dc8a2d63b70ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c2fa26c22e542b787d98e75a2e646f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6ab3ca368f4c87b3b577e05e26165f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1c25824fde74eff972c57182acf7f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22a236e9740a4f5c8e8422882a19a5a6",
              "IPY_MODEL_1b8e79007c6d4260b120f2b913d294ad",
              "IPY_MODEL_ee3914f837f94276b0614b4f414912d5"
            ],
            "layout": "IPY_MODEL_708c781044b8415ab730cdc513169dee"
          }
        },
        "22a236e9740a4f5c8e8422882a19a5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4ec039bd8f54f8cbb4d174b057e7789",
            "placeholder": "​",
            "style": "IPY_MODEL_daecae2326a64709ab44b4984bf544a1",
            "value": "tokenizer.json: 100%"
          }
        },
        "1b8e79007c6d4260b120f2b913d294ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6b8d22b46e4046b80facdc6e8bfebc",
            "max": 7031645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25cb59a241824fd0b58fcdbaf6e8d580",
            "value": 7031645
          }
        },
        "ee3914f837f94276b0614b4f414912d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b8bc1d872d1470e81f5461771d128f2",
            "placeholder": "​",
            "style": "IPY_MODEL_518e52f85a0446938c047f1eb6c76ef8",
            "value": " 7.03M/7.03M [00:00&lt;00:00, 67.0MB/s]"
          }
        },
        "708c781044b8415ab730cdc513169dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ec039bd8f54f8cbb4d174b057e7789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daecae2326a64709ab44b4984bf544a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e6b8d22b46e4046b80facdc6e8bfebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25cb59a241824fd0b58fcdbaf6e8d580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b8bc1d872d1470e81f5461771d128f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "518e52f85a0446938c047f1eb6c76ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}