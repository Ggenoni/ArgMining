{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RLF7vk2NVX0b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "7pH1kNHzVX0d"
      },
      "outputs": [],
      "source": [
        "with open(\"validation.json\", \"r\") as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZM87fSbVX0f"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cOxrdAiVX0g",
        "outputId": "011957d5-20b3-44eb-cbb1-0fcb8c1a1c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of datasets: 4\n",
            "\n",
            "\n",
            "+--------------------+---------------------------+\n",
            "| Dataset            |   Number of Interventions |\n",
            "+====================+===========================+\n",
            "| US2016             |                        80 |\n",
            "+--------------------+---------------------------+\n",
            "| rrd                |                        72 |\n",
            "+--------------------+---------------------------+\n",
            "| moral_maze_schemes |                        20 |\n",
            "+--------------------+---------------------------+\n",
            "| us2016reddit       |                        14 |\n",
            "+--------------------+---------------------------+\n"
          ]
        }
      ],
      "source": [
        "datasets = {}\n",
        "for key, value in data.items():\n",
        "    dataset = value[\"dataset\"]\n",
        "    if dataset in datasets:\n",
        "        datasets[dataset] += 1\n",
        "\n",
        "    else:\n",
        "        datasets[dataset] = 1\n",
        "\n",
        "datasets = dict(sorted(datasets.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"Total number of datasets:\", len(datasets))\n",
        "print(\"\\n\")\n",
        "dataset_data = [(key, value) for key, value in datasets.items()]\n",
        "print(tabulate(dataset_data, headers=[\"Dataset\", \"Number of Interventions\"], tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muRacOuFVX0h"
      },
      "source": [
        "### Authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_3USlkVVX0h",
        "outputId": "d6cdc6ff-5a53-49e9-f2fd-4745aa225635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of autors: 69\n",
            "\n",
            "\n",
            "+-----------------+---------------------------+\n",
            "| Author          |   Number of Interventions |\n",
            "+=================+===========================+\n",
            "| TRUMP           |                        43 |\n",
            "+-----------------+---------------------------+\n",
            "| CLINTON         |                        34 |\n",
            "+-----------------+---------------------------+\n",
            "| Antanagoge      |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| JJMurray        |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| MT              |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| howie           |                         4 |\n",
            "+-----------------+---------------------------+\n",
            "| HOLT            |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| CF              |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| JL              |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| Mulder          |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| SofieM          |                         3 |\n",
            "+-----------------+---------------------------+\n",
            "| CL              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| atraveller      |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| cd38            |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| citizen-s       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| darawayne       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| JW              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| JetJock         |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| MP              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| ND              |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| NYCMuscleman18  |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| PracticalJo     |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| SWong           |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| dberger         |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| dlpoole         |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| elizwestley     |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| golff4fun       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| mcliverty       |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| secretcurse     |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| smr             |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| travellots      |                         2 |\n",
            "+-----------------+---------------------------+\n",
            "| Doctor-Mom      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Elmattador      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| FoodAllergyMom  |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Frequent-Flyer  |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Glblwrmingisfak |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Helen           |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| 17th            |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AFCHF           |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AK-traveler     |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AllergyDad      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| AngelComa       |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Bill            |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Tuatho          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Vec             |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Velshtein       |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Zewstain        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| aimwill         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| ambersky        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| annoyed         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| JDwyer          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Javier          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| KHenrickson     |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| MR              |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Melanie         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Mpogoda         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| MrFordization   |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| PeanutAllergy   |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Qubbin          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| Sithsaber       |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| drgreg          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| grayk47         |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| hgranato        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| kateinhawaii    |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| lauraclare      |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| mfball          |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| msrocker        |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| smg             |                         1 |\n",
            "+-----------------+---------------------------+\n",
            "| zuclinator      |                         1 |\n",
            "+-----------------+---------------------------+\n"
          ]
        }
      ],
      "source": [
        "authors = {}\n",
        "for key, value in data.items():\n",
        "    author = key.split(\"_\")[0]\n",
        "    if author in authors:\n",
        "        authors[author] += 1\n",
        "    else:\n",
        "        authors[author] = 1\n",
        "\n",
        "authors = dict(sorted(authors.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"Total number of autors:\", len(authors))\n",
        "print(\"\\n\")\n",
        "authors_data = [(key, value) for key, value in authors.items()]\n",
        "print(tabulate(authors_data, headers=[\"Author\", \"Number of Interventions\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkf7VjX3VX0h"
      },
      "source": [
        "### Argumentation schemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1llQs_EVX0i",
        "outputId": "48f6f745-ccdf-4534-c77d-b85a19a60531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of schemes: 28\n",
            "\n",
            "\n",
            "+-------------------------+-------------------------+\n",
            "| Argumentation scheme    |   Number of Occurrences |\n",
            "+=========================+=========================+\n",
            "| ERPracticalReasoning    |                      97 |\n",
            "+-------------------------+-------------------------+\n",
            "| Example                 |                      91 |\n",
            "+-------------------------+-------------------------+\n",
            "| ERExample               |                      84 |\n",
            "+-------------------------+-------------------------+\n",
            "| CauseToEffect           |                      55 |\n",
            "+-------------------------+-------------------------+\n",
            "| PracticalReasoning      |                      38 |\n",
            "+-------------------------+-------------------------+\n",
            "| Consequences            |                      36 |\n",
            "+-------------------------+-------------------------+\n",
            "| VerbalClassification    |                      25 |\n",
            "+-------------------------+-------------------------+\n",
            "| Sign                    |                      24 |\n",
            "+-------------------------+-------------------------+\n",
            "| CircumstantialAdHominem |                      22 |\n",
            "+-------------------------+-------------------------+\n",
            "| GenericAdHominem        |                      15 |\n",
            "+-------------------------+-------------------------+\n",
            "| Analogy                 |                      11 |\n",
            "+-------------------------+-------------------------+\n",
            "| Values                  |                      10 |\n",
            "+-------------------------+-------------------------+\n",
            "| PositionToKnow          |                      10 |\n",
            "+-------------------------+-------------------------+\n",
            "| PopularOpinion          |                       8 |\n",
            "+-------------------------+-------------------------+\n",
            "| FearAppeal              |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| DangerAppeal            |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| Ad hominem              |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| ERAdHominem             |                       7 |\n",
            "+-------------------------+-------------------------+\n",
            "| ExpertOpinion           |                       6 |\n",
            "+-------------------------+-------------------------+\n",
            "| Alternatives            |                       6 |\n",
            "+-------------------------+-------------------------+\n",
            "| PopularPractice         |                       6 |\n",
            "+-------------------------+-------------------------+\n",
            "| Bias                    |                       4 |\n",
            "+-------------------------+-------------------------+\n",
            "| ERExpertOpinion         |                       4 |\n",
            "+-------------------------+-------------------------+\n",
            "| ArgumentFromAuthority   |                       4 |\n",
            "+-------------------------+-------------------------+\n",
            "| DirectAdHominem         |                       2 |\n",
            "+-------------------------+-------------------------+\n",
            "| PositiveConsequences    |                       1 |\n",
            "+-------------------------+-------------------------+\n",
            "| NegativeConsequences    |                       1 |\n",
            "+-------------------------+-------------------------+\n",
            "| SignFromOtherEvents     |                       1 |\n",
            "+-------------------------+-------------------------+\n"
          ]
        }
      ],
      "source": [
        "schemes = {}\n",
        "for key, value in data.items():\n",
        "    scheme = value[\"schemes\"]\n",
        "    for sch in scheme:\n",
        "        if sch in schemes:\n",
        "            schemes[sch] += 1\n",
        "        else:\n",
        "            schemes[sch] = 1\n",
        "\n",
        "schemes = dict(sorted(schemes.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"Total number of schemes:\", len(schemes))\n",
        "print(\"\\n\")\n",
        "schemes_data = [(key, value) for key, value in schemes.items()]\n",
        "print(tabulate(schemes_data, headers=[\"Argumentation scheme\", \"Number of Occurrences\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eERTtasKVX0i"
      },
      "source": [
        "### Critical questions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def compute_cq_stats(data):\n",
        "    cq_stats = {\n",
        "        \"overall\": {\n",
        "            \"count\": 0,\n",
        "            \"labels\": defaultdict(int),\n",
        "            \"percentage_useful\": 0.0\n",
        "        },\n",
        "        \"theoretical\": {\n",
        "            \"count\": 0,\n",
        "            \"labels\": defaultdict(int),\n",
        "            \"percentage_useful\": 0.0\n",
        "        },\n",
        "        \"llm_generated\": {\n",
        "            \"count\": 0,\n",
        "            \"labels\": defaultdict(int),\n",
        "            \"percentage_useful\": 0.0\n",
        "        }\n",
        "\n",
        "    }\n",
        "\n",
        "    theoretical_cqs = {}\n",
        "    llm_cqs = {}\n",
        "\n",
        "    for value in data.values():\n",
        "        for cq in value.get(\"cqs\", []):\n",
        "            cq_stats[\"overall\"][\"count\"] += 1\n",
        "            cq_id = cq.get(\"id\")\n",
        "            label = cq.get(\"label\")\n",
        "\n",
        "            if cq_id is None:\n",
        "                print(\"Warning: Missing ID in CQ ->\", cq)\n",
        "                continue  # Skip missing IDs\n",
        "\n",
        "            # Update global label counts\n",
        "            cq_stats[\"overall\"][\"labels\"][label] += 1\n",
        "\n",
        "            if \"_T_\" in cq_id:\n",
        "                cq_stats[\"theoretical\"][\"count\"] += 1\n",
        "                cq_stats[\"theoretical\"][\"labels\"][label] += 1\n",
        "\n",
        "                if cq_id in theoretical_cqs:\n",
        "                    print(f\"Duplicate theoretical CQ ID found: {cq_id}\")\n",
        "\n",
        "                theoretical_cqs[cq_id] = {\"cq\": cq[\"cq\"], \"label\": label}\n",
        "\n",
        "            elif \"_LLM_\" in cq_id:\n",
        "                cq_stats[\"llm_generated\"][\"count\"] += 1\n",
        "                cq_stats[\"llm_generated\"][\"labels\"][label] += 1\n",
        "\n",
        "                if cq_id in llm_cqs:\n",
        "                    print(f\"Duplicate LLM CQ ID found: {cq_id}\")\n",
        "\n",
        "                llm_cqs[cq_id] = {\"cq\": cq[\"cq\"], \"label\": label}\n",
        "\n",
        "    # Convert label defaultdicts to normal dicts and sort\n",
        "    for key in [\"overall\", \"theoretical\", \"llm_generated\"]:\n",
        "        cq_stats[key][\"labels\"] = dict(sorted(cq_stats[key][\"labels\"].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    # Compute percentage of useful questions for each category\n",
        "    for key in [\"overall\", \"theoretical\", \"llm_generated\"]:\n",
        "        total = cq_stats[key][\"count\"]\n",
        "        useful = cq_stats[key][\"labels\"].get(\"Useful\", 0)\n",
        "        if total > 0:\n",
        "            cq_stats[key][\"percentage_useful\"] = round(useful / total, 2)\n",
        "\n",
        "    return cq_stats, theoretical_cqs, llm_cqs\n",
        "\n",
        "# Run the function\n",
        "cq_stats_result, theoretical_cqs, llm_cqs = compute_cq_stats(data)\n",
        "\n",
        "# Check consistency\n",
        "print(\"\\n--- Consistency Check ---\")\n",
        "print(\"Expected theoretical CQs:\", cq_stats_result[\"theoretical\"][\"count\"], \"-> Found:\", len(theoretical_cqs))\n",
        "print(\"Expected LLM CQs:\", cq_stats_result[\"llm_generated\"][\"count\"], \"-> Found:\", len(llm_cqs))\n"
      ],
      "metadata": {
        "id": "kQu2ic6Mzoip",
        "outputId": "a094db05-a7c9-4606-dc63-7d4d45304f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate LLM CQ ID found: TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_3_L\n",
            "Duplicate LLM CQ ID found: TRUMP_240_2_LLM_US2016_D_meta-llama_Meta-Llama-3-70B-Instruct_0_L\n",
            "Duplicate theoretical CQ ID found: TRUMP_240_2_T__7\n",
            "\n",
            "--- Consistency Check ---\n",
            "Expected theoretical CQs: 993 -> Found: 992\n",
            "Expected LLM CQs: 3143 -> Found: 3141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_data = [\n",
        "    [\"Total CQs\", cq_stats_result[\"overall\"][\"count\"],\n",
        "     cq_stats_result[\"overall\"][\"labels\"][\"Useful\"],\n",
        "     cq_stats_result[\"overall\"][\"labels\"][\"Unhelpful\"],\n",
        "     cq_stats_result[\"overall\"][\"labels\"][\"Invalid\"],\n",
        "     f'{cq_stats_result[\"overall\"][\"percentage_useful\"] * 100:.0f}%'],\n",
        "    [\"Theoretical\", cq_stats_result[\"theoretical\"][\"count\"],\n",
        "     cq_stats_result[\"theoretical\"][\"labels\"][\"Useful\"],\n",
        "     cq_stats_result[\"theoretical\"][\"labels\"][\"Unhelpful\"],\n",
        "     cq_stats_result[\"theoretical\"][\"labels\"][\"Invalid\"],\n",
        "     f'{cq_stats_result[\"theoretical\"][\"percentage_useful\"] * 100:.0f}%'],\n",
        "    [\"LLM Generated\", cq_stats_result[\"llm_generated\"][\"count\"],\n",
        "     cq_stats_result[\"llm_generated\"][\"labels\"][\"Useful\"],\n",
        "     cq_stats_result[\"llm_generated\"][\"labels\"][\"Unhelpful\"],\n",
        "     cq_stats_result[\"llm_generated\"][\"labels\"][\"Invalid\"],\n",
        "     f'{cq_stats_result[\"llm_generated\"][\"percentage_useful\"] * 100:.0f}%']\n",
        "]\n",
        "\n",
        "print(tabulate(table_data, headers=[\"Category\", \"Total CQs\", \"Useful\", \"Unhelpful\", \"Invalid\", \"% Useful\"], tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "id": "yzuHufovyyP5",
        "outputId": "305b71b7-c6c6-4097-f878-88fa157a6d70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------+----------+-------------+-----------+------------+\n",
            "| Category      |   Total CQs |   Useful |   Unhelpful |   Invalid | % Useful   |\n",
            "+===============+=============+==========+=============+===========+============+\n",
            "| Total CQs     |        4136 |     2790 |         893 |       453 | 67%        |\n",
            "+---------------+-------------+----------+-------------+-----------+------------+\n",
            "| Theoretical   |         993 |      415 |         394 |       184 | 42%        |\n",
            "+---------------+-------------+----------+-------------+-----------+------------+\n",
            "| LLM Generated |        3143 |     2375 |         499 |       269 | 76%        |\n",
            "+---------------+-------------+----------+-------------+-----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FLzpsF8VX0j",
        "outputId": "14af8d8b-e2b5-485a-e192-c2108a54cd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats on critical questions:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_cqs': 4136,\n",
              " 'theoretical': {'count': 993,\n",
              "  'labels': {'Useful': 415, 'Unhelpful': 394, 'Invalid': 184},\n",
              "  'percentage_useful': 0.42},\n",
              " 'llm_generated': {'count': 3143,\n",
              "  'labels': {'Useful': 2375, 'Unhelpful': 499, 'Invalid': 269},\n",
              "  'percentage_useful': 0.76}}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "print(\"Stats on critical questions:\\n\")\n",
        "cq_stats_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qADj-G3VX0k"
      },
      "source": [
        "### Overall stats per dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imGDKGGPVX0k",
        "outputId": "9581cf0c-471c-46bf-ba58-2623886787b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total stats on authors, argumentation schemes, and critical questions per dataset:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'US2016': {'num_interventions': 80,\n",
              "  'authors': {'num_authors': 3,\n",
              "   'authors_count': {'TRUMP': 43, 'CLINTON': 34, 'HOLT': 3},\n",
              "   'avg_interventions_per_author': 26.666666666666668},\n",
              "  'schemes': {'unique_schemes': 18,\n",
              "   'num_schemes': 342,\n",
              "   'schemes_count': {'Example': 74,\n",
              "    'CauseToEffect': 45,\n",
              "    'Consequences': 36,\n",
              "    'PracticalReasoning': 34,\n",
              "    'VerbalClassification': 25,\n",
              "    'Sign': 24,\n",
              "    'CircumstantialAdHominem': 22,\n",
              "    'GenericAdHominem': 15,\n",
              "    'Values': 10,\n",
              "    'PositionToKnow': 10,\n",
              "    'Analogy': 7,\n",
              "    'FearAppeal': 7,\n",
              "    'DangerAppeal': 7,\n",
              "    'PopularOpinion': 6,\n",
              "    'Alternatives': 6,\n",
              "    'PopularPractice': 6,\n",
              "    'ExpertOpinion': 4,\n",
              "    'Bias': 4}},\n",
              "  'cqs': {'total_cqs': 2121,\n",
              "   'theoretical': {'count': 722,\n",
              "    'labels_count': {'Unhelpful': 283, 'Useful': 270, 'Invalid': 169},\n",
              "    'percentage_useful': 0.37},\n",
              "   'llm_generated': {'count': 1399,\n",
              "    'labels_count': {'Useful': 1117, 'Unhelpful': 166, 'Invalid': 116},\n",
              "    'percentage_useful': 0.8}}},\n",
              " 'rrd': {'num_interventions': 72,\n",
              "  'authors': {'num_authors': 44,\n",
              "   'authors_count': {'Antanagoge': 4,\n",
              "    'JJMurray': 4,\n",
              "    'howie': 4,\n",
              "    'Mulder': 3,\n",
              "    'SofieM': 3,\n",
              "    'atraveller': 2,\n",
              "    'cd38': 2,\n",
              "    'citizen-s': 2,\n",
              "    'darawayne': 2,\n",
              "    'JetJock': 2,\n",
              "    'NYCMuscleman18': 2,\n",
              "    'PracticalJo': 2,\n",
              "    'SWong': 2,\n",
              "    'dberger': 2,\n",
              "    'dlpoole': 2,\n",
              "    'elizwestley': 2,\n",
              "    'golff4fun': 2,\n",
              "    'mcliverty': 2,\n",
              "    'smr': 2,\n",
              "    'travellots': 2,\n",
              "    'Doctor-Mom': 1,\n",
              "    'FoodAllergyMom': 1,\n",
              "    'Frequent-Flyer': 1,\n",
              "    'AFCHF': 1,\n",
              "    'AK-traveler': 1,\n",
              "    'AllergyDad': 1,\n",
              "    'Bill': 1,\n",
              "    'Vec': 1,\n",
              "    'aimwill': 1,\n",
              "    'ambersky': 1,\n",
              "    'annoyed': 1,\n",
              "    'JDwyer': 1,\n",
              "    'Javier': 1,\n",
              "    'KHenrickson': 1,\n",
              "    'Mpogoda': 1,\n",
              "    'PeanutAllergy': 1,\n",
              "    'Qubbin': 1,\n",
              "    'drgreg': 1,\n",
              "    'hgranato': 1,\n",
              "    'kateinhawaii': 1,\n",
              "    'lauraclare': 1,\n",
              "    'msrocker': 1,\n",
              "    'smg': 1,\n",
              "    'zuclinator': 1},\n",
              "   'avg_interventions_per_author': 1.6363636363636365},\n",
              "  'schemes': {'unique_schemes': 5,\n",
              "   'num_schemes': 195,\n",
              "   'schemes_count': {'ERPracticalReasoning': 97,\n",
              "    'ERExample': 84,\n",
              "    'ERAdHominem': 7,\n",
              "    'ERExpertOpinion': 4,\n",
              "    'Example': 3}},\n",
              "  'cqs': {'total_cqs': 1403,\n",
              "   'theoretical': {'count': 203,\n",
              "    'labels_count': {'Useful': 110, 'Unhelpful': 84, 'Invalid': 9},\n",
              "    'percentage_useful': 0.54},\n",
              "   'llm_generated': {'count': 1200,\n",
              "    'labels_count': {'Useful': 912, 'Unhelpful': 217, 'Invalid': 71},\n",
              "    'percentage_useful': 0.76}}},\n",
              " 'moral_maze_schemes': {'num_interventions': 20,\n",
              "  'authors': {'num_authors': 9,\n",
              "   'authors_count': {'MT': 4,\n",
              "    'CF': 3,\n",
              "    'JL': 3,\n",
              "    'CL': 2,\n",
              "    'JW': 2,\n",
              "    'MP': 2,\n",
              "    'ND': 2,\n",
              "    'Helen': 1,\n",
              "    'Melanie': 1},\n",
              "   'avg_interventions_per_author': 2.2222222222222223},\n",
              "  'schemes': {'unique_schemes': 10,\n",
              "   'num_schemes': 34,\n",
              "   'schemes_count': {'CauseToEffect': 10,\n",
              "    'Example': 8,\n",
              "    'PracticalReasoning': 4,\n",
              "    'Analogy': 3,\n",
              "    'PopularOpinion': 2,\n",
              "    'ExpertOpinion': 2,\n",
              "    'DirectAdHominem': 2,\n",
              "    'PositiveConsequences': 1,\n",
              "    'NegativeConsequences': 1,\n",
              "    'SignFromOtherEvents': 1}},\n",
              "  'cqs': {'total_cqs': 372,\n",
              "   'theoretical': {'count': 43,\n",
              "    'labels_count': {'Useful': 24, 'Unhelpful': 15, 'Invalid': 4},\n",
              "    'percentage_useful': 0.56},\n",
              "   'llm_generated': {'count': 329,\n",
              "    'labels_count': {'Useful': 224, 'Unhelpful': 56, 'Invalid': 49},\n",
              "    'percentage_useful': 0.68}}},\n",
              " 'us2016reddit': {'num_interventions': 14,\n",
              "  'authors': {'num_authors': 13,\n",
              "   'authors_count': {'secretcurse': 2,\n",
              "    'Elmattador': 1,\n",
              "    'Glblwrmingisfak': 1,\n",
              "    '17th': 1,\n",
              "    'AngelComa': 1,\n",
              "    'Tuatho': 1,\n",
              "    'Velshtein': 1,\n",
              "    'Zewstain': 1,\n",
              "    'MR': 1,\n",
              "    'MrFordization': 1,\n",
              "    'Sithsaber': 1,\n",
              "    'grayk47': 1,\n",
              "    'mfball': 1},\n",
              "   'avg_interventions_per_author': 1.0769230769230769},\n",
              "  'schemes': {'unique_schemes': 4,\n",
              "   'num_schemes': 18,\n",
              "   'schemes_count': {'Ad hominem': 7,\n",
              "    'Example': 6,\n",
              "    'ArgumentFromAuthority': 4,\n",
              "    'Analogy': 1}},\n",
              "  'cqs': {'total_cqs': 240,\n",
              "   'theoretical': {'count': 25,\n",
              "    'labels_count': {'Unhelpful': 12, 'Useful': 11, 'Invalid': 2},\n",
              "    'percentage_useful': 0.44},\n",
              "   'llm_generated': {'count': 215,\n",
              "    'labels_count': {'Useful': 122, 'Unhelpful': 60, 'Invalid': 33},\n",
              "    'percentage_useful': 0.57}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def compute_stats(data):\n",
        "    stats = defaultdict(lambda: {\n",
        "        \"num_interventions\": 0,\n",
        "        \"authors_count\": defaultdict(int),\n",
        "        \"schemes_count\": defaultdict(int),\n",
        "        \"authors\": {},\n",
        "        \"schemes\": {},\n",
        "        \"cqs\": {\n",
        "            \"total_cqs\": 0,\n",
        "            \"theoretical\": {\"count\": 0, \"labels_count\": defaultdict(int), \"percentage_useful\": 0.0},\n",
        "            \"llm_generated\": {\"count\": 0, \"labels_count\": defaultdict(int), \"percentage_useful\": 0.0}\n",
        "        }\n",
        "    })\n",
        "\n",
        "    for key, value in data.items():\n",
        "        dataset = value[\"dataset\"]\n",
        "        author = key.split(\"_\")[0]  # Extract author name\n",
        "\n",
        "        stats[dataset][\"num_interventions\"] += 1\n",
        "        stats[dataset][\"authors_count\"][author] += 1\n",
        "\n",
        "        for scheme in value.get(\"schemes\", []):\n",
        "            stats[dataset][\"schemes_count\"][scheme] += 1\n",
        "\n",
        "        # Process Critical Questions (CQs)\n",
        "        for cq in value.get(\"cqs\", []):\n",
        "            stats[dataset][\"cqs\"][\"total_cqs\"] += 1\n",
        "            cq_id = cq[\"id\"]\n",
        "            label = cq[\"label\"]\n",
        "\n",
        "            if \"_T_\" in cq_id:\n",
        "                stats[dataset][\"cqs\"][\"theoretical\"][\"count\"] += 1\n",
        "                stats[dataset][\"cqs\"][\"theoretical\"][\"labels_count\"][label] += 1\n",
        "            elif \"_LLM_\" in cq_id:\n",
        "                stats[dataset][\"cqs\"][\"llm_generated\"][\"count\"] += 1\n",
        "                stats[dataset][\"cqs\"][\"llm_generated\"][\"labels_count\"][label] += 1\n",
        "\n",
        "    # Convert defaultdicts to regular dicts and add final stats\n",
        "    result = {}\n",
        "    for ds, ds_data in stats.items():\n",
        "        num_authors = len(ds_data[\"authors_count\"])\n",
        "        ds_data[\"authors_count\"] = dict(sorted(ds_data[\"authors_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "        avg_interventions = ds_data[\"num_interventions\"] / num_authors if num_authors > 0 else 0\n",
        "\n",
        "        unique_schemes = len(ds_data[\"schemes_count\"])\n",
        "        num_schemes = sum(ds_data[\"schemes_count\"].values())\n",
        "        ds_data[\"schemes_count\"] = dict(sorted(ds_data[\"schemes_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        ds_data[\"cqs\"][\"theoretical\"][\"labels_count\"] = dict(sorted(ds_data[\"cqs\"][\"theoretical\"][\"labels_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "        ds_data[\"cqs\"][\"llm_generated\"][\"labels_count\"] = dict(sorted(ds_data[\"cqs\"][\"llm_generated\"][\"labels_count\"].items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "        ds_data[\"cqs\"][\"theoretical\"][\"percentage_useful\"] = round(ds_data[\"cqs\"][\"theoretical\"][\"labels_count\"][\"Useful\"] / ds_data[\"cqs\"][\"theoretical\"][\"count\"], 2) if ds_data[\"cqs\"][\"theoretical\"][\"count\"] > 0 else 0\n",
        "        ds_data[\"cqs\"][\"llm_generated\"][\"percentage_useful\"] = round(ds_data[\"cqs\"][\"llm_generated\"][\"labels_count\"][\"Useful\"] / ds_data[\"cqs\"][\"llm_generated\"][\"count\"], 2) if ds_data[\"cqs\"][\"theoretical\"][\"count\"] > 0 else 0\n",
        "\n",
        "        result[ds] = {\n",
        "            \"num_interventions\": ds_data[\"num_interventions\"],\n",
        "            \"authors\": {\n",
        "                \"num_authors\": num_authors,\n",
        "                \"authors_count\": ds_data[\"authors_count\"],\n",
        "                \"avg_interventions_per_author\": avg_interventions\n",
        "            },\n",
        "            \"schemes\": {\n",
        "                \"unique_schemes\": unique_schemes,\n",
        "                \"num_schemes\": num_schemes,\n",
        "                \"schemes_count\": ds_data[\"schemes_count\"]\n",
        "            },\n",
        "            \"cqs\": ds_data[\"cqs\"]\n",
        "        }\n",
        "\n",
        "    return dict(sorted(result.items(), key=lambda item: item[1][\"num_interventions\"], reverse=True))\n",
        "\n",
        "\n",
        "stats_result = compute_stats(data)\n",
        "print(\"Total stats on authors, argumentation schemes, and critical questions per dataset:\\n\")\n",
        "stats_result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save datasets stats\n",
        "filename = \"datasets_stats.json\"\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(stats_result, f, indent=4, ensure_ascii=False)\n",
        "print(f\"Stats saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxoFVhVtu_Hl",
        "outputId": "e914944f-d19a-460d-d251-162504a448db"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats saved to datasets_stats.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-3iINQ-VX0l"
      },
      "source": [
        "### Critical questions in depth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "login()  # 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"
      ],
      "metadata": {
        "id": "_EmldYjetUTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models\n",
        "models_list = [\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "]\n",
        "\n",
        "def compute_token_stats_for_models(cq_dict, models_list):\n",
        "    stats_by_model = {}\n",
        "\n",
        "    for model_name in models_list:\n",
        "        print(f\"Processing {model_name}...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        token_lengths = [len(tokenizer(cq[\"cq\"])[\"input_ids\"]) for cq in cq_dict.values()]\n",
        "\n",
        "        if not token_lengths:\n",
        "            stats_by_model[model_name] = {\"max\": 0, \"min\": 0, \"avg\": 0.0}\n",
        "            continue\n",
        "\n",
        "        stats_by_model[model_name] = {\n",
        "            \"max\": max(token_lengths),\n",
        "            \"min\": min(token_lengths),\n",
        "            \"avg\": round(sum(token_lengths) / len(token_lengths), 2)\n",
        "        }\n",
        "\n",
        "    return stats_by_model\n",
        "\n",
        "# Compute stats for each model\n",
        "theoretical_token_stats = compute_token_stats_for_models(theoretical_cqs, models_list)\n",
        "llm_token_stats = compute_token_stats_for_models(llm_cqs, models_list)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\\n\")\n",
        "print(\"Theoretical CQ Token Stats:\")\n",
        "for model, stats in theoretical_token_stats.items():\n",
        "    print(f\"{model}: {stats}\")\n",
        "\n",
        "print(\"\\nLLM-Generated CQ Token Stats:\")\n",
        "for model, stats in llm_token_stats.items():\n",
        "    print(f\"{model}: {stats}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucgzf2Hrrx2W",
        "outputId": "4801ac81-f348-44ab-db76-9def51b92f9e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "Processing deepseek-ai/DeepSeek-R1-Distill-Llama-8B...\n",
            "Processing mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
            "Processing Qwen/Qwen2.5-7B-Instruct...\n",
            "Processing meta-llama/Meta-Llama-3-8B-Instruct...\n",
            "Processing deepseek-ai/DeepSeek-R1-Distill-Llama-8B...\n",
            "Processing mistralai/Mixtral-8x7B-Instruct-v0.1...\n",
            "Processing Qwen/Qwen2.5-7B-Instruct...\n",
            "\n",
            "\n",
            "\n",
            "Theoretical CQ Token Stats:\n",
            "meta-llama/Meta-Llama-3-8B-Instruct: {'max': 65, 'min': 9, 'avg': 27.01}\n",
            "deepseek-ai/DeepSeek-R1-Distill-Llama-8B: {'max': 65, 'min': 9, 'avg': 27.01}\n",
            "mistralai/Mixtral-8x7B-Instruct-v0.1: {'max': 73, 'min': 9, 'avg': 28.8}\n",
            "Qwen/Qwen2.5-7B-Instruct: {'max': 64, 'min': 8, 'avg': 26.11}\n",
            "\n",
            "LLM-Generated CQ Token Stats:\n",
            "meta-llama/Meta-Llama-3-8B-Instruct: {'max': 72, 'min': 9, 'avg': 28.9}\n",
            "deepseek-ai/DeepSeek-R1-Distill-Llama-8B: {'max': 72, 'min': 9, 'avg': 28.9}\n",
            "mistralai/Mixtral-8x7B-Instruct-v0.1: {'max': 79, 'min': 9, 'avg': 31.55}\n",
            "Qwen/Qwen2.5-7B-Instruct: {'max': 71, 'min': 8, 'avg': 27.96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_cqs_with_labels(cq_dict, num_clusters=5):\n",
        "    cqs = [cq[\"cq\"] for cq in cq_dict.values()]\n",
        "    labels = [cq[\"label\"] for cq in cq_dict.values()]\n",
        "\n",
        "    # Convert to numerical representation\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(cqs)  # TF-IDF matrix\n",
        "\n",
        "    # Apply K-Means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "\n",
        "    # Initialize cluster dictionary\n",
        "    clustered_cqs = {\n",
        "        i: {\"questions\": [], \"metadata\": {\"tot_questions\": 0, \"labels\": {'Useful': 0, 'Unhelpful': 0, \"Invalid\": 0}, \"percentage_useful\": 0.0}}\n",
        "        for i in range(num_clusters)\n",
        "    }\n",
        "\n",
        "    for i, cq in enumerate(cqs):\n",
        "        cluster_id = clusters[i]\n",
        "        clustered_cqs[cluster_id][\"questions\"].append(cq)\n",
        "        clustered_cqs[cluster_id][\"metadata\"][\"tot_questions\"] += 1\n",
        "        clustered_cqs[cluster_id][\"metadata\"][\"labels\"][labels[i]] += 1\n",
        "\n",
        "    for cluster_id, data in clustered_cqs.items():\n",
        "        useful_count = data[\"metadata\"][\"labels\"][\"Useful\"]\n",
        "        total_count = data[\"metadata\"][\"tot_questions\"]\n",
        "        if total_count > 0:\n",
        "            data[\"metadata\"][\"percentage_useful\"] = round(useful_count / total_count, 2)\n",
        "\n",
        "    return clustered_cqs\n"
      ],
      "metadata": {
        "id": "o_GgQAoLoKQu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Theoretical CQs\n",
        "theoretical_clusters = cluster_cqs_with_labels(theoretical_cqs, num_clusters=10)\n",
        "\n",
        "for cluster_id, cluster_data in theoretical_clusters.items():\n",
        "    print(f\"\\n=== Cluster {cluster_id} ===\")\n",
        "    print(f\"Total Questions: {cluster_data['metadata']['tot_questions']}\")\n",
        "    print(f\"Label Distribution: {cluster_data['metadata']['labels']}\")\n",
        "    print(f\"Percentage Useful: {cluster_data['metadata']['percentage_useful'] * 100:.0f}%\")\n",
        "    print(\"\\nSample Questions:\")\n",
        "    for question in cluster_data[\"questions\"][:3]:\n",
        "        print(f\"- {question}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUwgbCnkoRU9",
        "outputId": "aba3818d-5d06-44d4-d889-377ecd54fbad"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cluster 0 ===\n",
            "Total Questions: 173\n",
            "Label Distribution: {'Useful': 72, 'Unhelpful': 62, 'Invalid': 39}\n",
            "Percentage Useful: 42%\n",
            "\n",
            "Sample Questions:\n",
            "- If Donald insults Muslims, will they not be on the front lines anymore? What evidence supports this claim? And how likely are the consequences?\n",
            "- If Donald Trump insults Muslims abroad and at home, will they not cooperate with us and provide information that we can't get elsewhere? What evidence supports this claim? How likely are the consequences?\n",
            "- How strong is the generalization that if Clinton achieved putting together a coalition to impose tough sanctions on Iran, then the USA would drive Iranians to the negotiation table?\n",
            "\n",
            "=== Cluster 1 ===\n",
            "Total Questions: 110\n",
            "Label Distribution: {'Useful': 71, 'Unhelpful': 29, 'Invalid': 10}\n",
            "Percentage Useful: 65%\n",
            "\n",
            "Sample Questions:\n",
            "- Could working more closely with the USA's allies have consequences that we should take into account? Is it practically possible?\n",
            "- What other consequences should also be taken into account if Donald Trump insults Muslims abroad and at home?\n",
            "- What other consequences should also be taken into account if Donald does not insult Muslims?\n",
            "\n",
            "=== Cluster 2 ===\n",
            "Total Questions: 42\n",
            "Label Distribution: {'Useful': 20, 'Unhelpful': 20, 'Invalid': 2}\n",
            "Percentage Useful: 48%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there other factors in this particular case that could have interfered with the event of the 'USA driving Iranians to the negotiation table'?\n",
            "- Are there other factors that could interfere with or counteract the production of the effect 'the USA can have enough clean energy to power every home' in this case?\n",
            "- Are there other factors that could interfere with or counteract the production of the effect 'we can have enough clean energy to power every home' in this case?\n",
            "\n",
            "=== Cluster 3 ===\n",
            "Total Questions: 44\n",
            "Label Distribution: {'Useful': 2, 'Unhelpful': 13, 'Invalid': 29}\n",
            "Percentage Useful: 5%\n",
            "\n",
            "Sample Questions:\n",
            "- Is paying federal income tax not consistent with other matters Trump stands for? If so, does this inconsistency decrease Trump's credibility?\n",
            "- Is paying taxes not consistent with other matters Trump stands for? If so, does this inconsistency decrease Trump's credibility?\n",
            "- Is lead to nuclear problems not consistent with other matters Clinton stands for? If so, does this inconsistency decrease Clinton's credibility?\n",
            "\n",
            "=== Cluster 4 ===\n",
            "Total Questions: 73\n",
            "Label Distribution: {'Useful': 21, 'Unhelpful': 42, 'Invalid': 10}\n",
            "Percentage Useful: 29%\n",
            "\n",
            "Sample Questions:\n",
            "- Is the current political situation actually a typical case of other political situations that require working closely with NATO and our allies? How widely applicable is the generalization?\n",
            "- Is working with our friends in the Mideast actually a typical case of other political moves that are necessary and Trump is very dismissive of? How widely applicable is the generalization?\n",
            "- Is the USA actually a typical case of other countries that have other problems than Iran? How widely applicable is the generalization?\n",
            "\n",
            "=== Cluster 5 ===\n",
            "Total Questions: 92\n",
            "Label Distribution: {'Useful': 27, 'Unhelpful': 40, 'Invalid': 25}\n",
            "Percentage Useful: 29%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there reasons to believe that profit-sharing is not good in this situation?\n",
            "- What evidence supports that having to face difficult choices and being under stress is generally accepted as true?\n",
            "- Is profit-sharing seen as good for most people?\n",
            "\n",
            "=== Cluster 6 ===\n",
            "Total Questions: 74\n",
            "Label Distribution: {'Useful': 18, 'Unhelpful': 43, 'Invalid': 13}\n",
            "Percentage Useful: 24%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there special circumstances pertaining to the current political situation that undermine its generalisability to other political situations that require working closely with NATO and our allies?\n",
            "- Are there special circumstances pertaining to working with our friends in the Middel East that undermine its generalisability to other political moves that are necessary?\n",
            "- Are there special circumstances pertaining to the USA that undermine its generalisability to other countries that have other problems than Iran?\n",
            "\n",
            "=== Cluster 7 ===\n",
            "Total Questions: 197\n",
            "Label Distribution: {'Useful': 102, 'Unhelpful': 58, 'Invalid': 37}\n",
            "Percentage Useful: 52%\n",
            "\n",
            "Sample Questions:\n",
            "- Is it actually the case that working with our friends in the Middel East is necessary and Trump has been very dismissive of this? Is there evidence for this claim?\n",
            "- Is it actually the case that the current political situation requires working closely with NATO and our allies and Trump is desistive of working with our allies? Is there evidence for this claim?\n",
            "- Is there a proven relation between 'Iranians had stocked them with centrifuges that were whirling away' and 'Iranians had mastered the nuclear fuel cycle?'\n",
            "\n",
            "=== Cluster 8 ===\n",
            "Total Questions: 93\n",
            "Label Distribution: {'Useful': 33, 'Unhelpful': 52, 'Invalid': 8}\n",
            "Percentage Useful: 35%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there other relevant goals that conflict with vacuuming up intelligence from Europe and the Middle East?\n",
            "- Are there other relevant goals that conflict with supporting people who are struggling to balance family and work?\n",
            "- Are there other relevant goals that conflict with Iran not having nuclear bombs?\n",
            "\n",
            "=== Cluster 9 ===\n",
            "Total Questions: 94\n",
            "Label Distribution: {'Useful': 49, 'Unhelpful': 35, 'Invalid': 10}\n",
            "Percentage Useful: 52%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there alternative actions to working more closely with the USA's allies to achieve vacuuming up intelligence from Europe and the Middle East? If so, which is the most efficient action?\n",
            "- Are there alternative actions to having paid family leave and earned sick days to support people who are struggling to balance family and work? If so, which is the most efficient action?\n",
            "- Are there alternative actions to USA sanctioning Iran to achieve Iran not having nuclear bombs? If so, which is the most efficient action?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM-generated CQs\n",
        "theoretical_clusters = cluster_cqs_with_labels(llm_cqs, num_clusters=10)\n",
        "\n",
        "for cluster_id, cluster_data in theoretical_clusters.items():\n",
        "    print(f\"\\n=== Cluster {cluster_id} ===\")\n",
        "    print(f\"Total Questions: {cluster_data['metadata']['tot_questions']}\")\n",
        "    print(f\"Label Distribution: {cluster_data['metadata']['labels']}\")\n",
        "    print(f\"Percentage Useful: {cluster_data['metadata']['percentage_useful'] * 100:.0f}%\")\n",
        "    print(\"\\nSample Questions:\")\n",
        "    for question in cluster_data[\"questions\"][:3]:\n",
        "        print(f\"- {question}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV_gA1W0t6-9",
        "outputId": "3f41694a-d8bb-4a7f-fea1-5046450b354d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Cluster 0 ===\n",
            "Total Questions: 304\n",
            "Label Distribution: {'Useful': 232, 'Unhelpful': 40, 'Invalid': 32}\n",
            "Percentage Useful: 76%\n",
            "\n",
            "Sample Questions:\n",
            "- What specific intelligence benefits have been gained from working with European and Middle Eastern allies in the past, and how do these benefits justify increased cooperation?\n",
            "- What specific plans do you have to address the root causes of income inequality, and how would you measure the success of these plans?\n",
            "- What is the track record of the speaker in implementing similar policies in the past, and what were the results?\n",
            "\n",
            "=== Cluster 1 ===\n",
            "Total Questions: 461\n",
            "Label Distribution: {'Useful': 299, 'Unhelpful': 100, 'Invalid': 62}\n",
            "Percentage Useful: 65%\n",
            "\n",
            "Sample Questions:\n",
            "- How does Clinton's proposal compare to Trump's proposal in terms of their potential impact on the economy, and what are the key differences between their approaches?\n",
            "- How do Clinton's statements about NATO and Iran relate to the broader topic of discussion, and what is her main point in bringing up these issues?\n",
            "- What is the relevance of mentioning Article 5 of NATO in this context, and how does it support Clinton's argument?\n",
            "\n",
            "=== Cluster 2 ===\n",
            "Total Questions: 359\n",
            "Label Distribution: {'Useful': 286, 'Unhelpful': 45, 'Invalid': 28}\n",
            "Percentage Useful: 80%\n",
            "\n",
            "Sample Questions:\n",
            "- How would the proposed policies address the root causes of income inequality, rather than just providing temporary benefits?\n",
            "- How would you define \"the wealthy\" and what specific tax reforms would you implement to ensure they pay their \"fair share\", and how would you measure the impact of these reforms?\n",
            "- How would you ensure that the benefits of economic growth are shared equitably among all segments of society, and not just concentrated among the wealthy?\n",
            "\n",
            "=== Cluster 3 ===\n",
            "Total Questions: 149\n",
            "Label Distribution: {'Useful': 109, 'Unhelpful': 27, 'Invalid': 13}\n",
            "Percentage Useful: 73%\n",
            "\n",
            "Sample Questions:\n",
            "- What is the definition of \"help create the profits\" and how does one measure it? Is it based on hours worked, productivity, or some other factor?\n",
            "- Is Clinton's criticism of the person's temperament based on a fair and objective assessment, or is it influenced by political bias or personal animosity?\n",
            "- Is Clinton's claim based on a subjective interpretation of the person's behavior, or is it supported by objective metrics or expert analysis?\n",
            "\n",
            "=== Cluster 4 ===\n",
            "Total Questions: 159\n",
            "Label Distribution: {'Useful': 119, 'Unhelpful': 32, 'Invalid': 8}\n",
            "Percentage Useful: 75%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there other factors that might influence a person's behavior in a given situation? Has Clinton taken these factors into account in her assessment of the person's temperament?\n",
            "- Are there other factors that are more important than temperament in determining a person's fitness to be commander-in-chief?\n",
            "- How does Clinton's plan take into account the potential impact of external factors, such as global economic trends or unforeseen events, on its success?\n",
            "\n",
            "=== Cluster 5 ===\n",
            "Total Questions: 268\n",
            "Label Distribution: {'Useful': 225, 'Unhelpful': 29, 'Invalid': 14}\n",
            "Percentage Useful: 84%\n",
            "\n",
            "Sample Questions:\n",
            "- What are the potential drawbacks or risks of increased cooperation with Muslim nations or communities, and how would Clinton mitigate these risks?\n",
            "- What evidence is there that Donald Trump's rhetoric has led to the alienation of Muslim communities, and how would Clinton's approach to working with these communities be more effective?\n",
            "- How does Clinton define \"working more closely\" with allies, and what specific actions or policies would she implement to achieve this goal?\n",
            "\n",
            "=== Cluster 6 ===\n",
            "Total Questions: 285\n",
            "Label Distribution: {'Useful': 192, 'Unhelpful': 48, 'Invalid': 45}\n",
            "Percentage Useful: 67%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there any counterarguments or alternative perspectives that Clinton is not acknowledging or addressing in her statements about NATO and Iran?\n",
            "- How does Clinton's claim that the deal \"put a lid on Iran's nuclear program without firing a single shot\" align with the actual outcome of the deal, and are there any potential long-term risks or consequences associated with the agreement?\n",
            "- Is there any potential bias or conflict of interest that might affect Clinton's judgment or response?\n",
            "\n",
            "=== Cluster 7 ===\n",
            "Total Questions: 223\n",
            "Label Distribution: {'Useful': 169, 'Unhelpful': 38, 'Invalid': 16}\n",
            "Percentage Useful: 76%\n",
            "\n",
            "Sample Questions:\n",
            "- Are there any potential unintended consequences of a full ban on peanut products on airplanes?\n",
            "- What is the evidence that a ban on peanut products on airlines would actually save lives, and are there any other measures that could be taken to reduce the risk of allergic reactions on flights?\n",
            "- How would a ban on peanut products on airlines be balanced against the rights and preferences of passengers who do not have peanut allergies?\n",
            "\n",
            "=== Cluster 8 ===\n",
            "Total Questions: 376\n",
            "Label Distribution: {'Useful': 339, 'Unhelpful': 24, 'Invalid': 13}\n",
            "Percentage Useful: 90%\n",
            "\n",
            "Sample Questions:\n",
            "- What evidence is there that Donald Trump has been \"dismissive\" of working with allies, and how has this alleged dismissiveness impacted national security?\n",
            "- What evidence is there that the American Muslim community is \"on the front lines\" of providing information to combat terrorism, and how can their role be enhanced and supported?\n",
            "- What is the evidence that increasing taxes on the wealthy would lead to economic growth and improved living standards for the majority of citizens?\n",
            "\n",
            "=== Cluster 9 ===\n",
            "Total Questions: 557\n",
            "Label Distribution: {'Useful': 403, 'Unhelpful': 116, 'Invalid': 38}\n",
            "Percentage Useful: 72%\n",
            "\n",
            "Sample Questions:\n",
            "- Is Clinton's characterization of Trump's views on climate change accurate, and is her own stance on the issue supported by scientific evidence?\n",
            "- Is it accurate to say that NATO only invoked Article 5 after 9/11, and are there any other instances where it was invoked?\n",
            "- What does Clinton mean by \"mastered the nuclear fuel cycle\"? Is this a technical term that can be verified, or is it a subjective judgment?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}